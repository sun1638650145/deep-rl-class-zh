{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第1单元: 训练你的第一个深度强化学习智能体🚀\n",
    "在这份笔记中, 你将训练你的第一个着陆器智能体**正确登陆月球🌕并将其分享到社区, 并记得实验不同的配置**\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit1频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境: [LunarLander-v2](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)\n",
    "\n",
    "📚 强化学习库: [Stable-Baselines3](https://stable-baselines3.readthedocs.io/en/master/)\n",
    "\n",
    "⬇️ 这是你将在几分钟内实现的目标的示例([原始视频下载链接](https://huggingface.co/ThomasSimonini/ppo-LunarLander-v2/resolve/main/replay.mp4)). ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src=\"./assets/replay.mp4\" type=\"video/mp4\"></video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<video autoplay controls><source src=\"./assets/replay.mp4\" type=\"video/mp4\"></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记的目标🏆\n",
    "在这份笔记学习结束后, 你将:\n",
    "* 能够使用环境库**Gym**.\n",
    "* 能够使用深度强化学习库**Stable-Baselines3**.\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![深度强化学习课程.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在这个免费课程中, 你将:\n",
    "* 📖 研究深度强化学习的**理论和实践**.\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ 在独特的环境中训练**智能体**.\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 先决条件 🏗\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第1单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit1/README.md)\n",
    "\n",
    "🔲 📚 先学习**强化学习基础**(蒙特卡洛MC, 时序差分TD和奖励假设...) 👉 https://huggingface.co/blog/deep-rl-intro\n",
    "\n",
    "🔲 📢 注册[我们的Discord服务器](https://discord.gg/aYka4Yhff9)**并在#introduce-yourself频道介绍自己** 🥳\n",
    "\n",
    "🔲 🐕 你是Discord新手吗? 请查看我们的**discord 101以获得最佳实践** 👉 https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一个深度强化学习的小回顾📚\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "有**两种**方法可以找到你的最优策略:\n",
    "* 通过**训练直接训练你的策略**: 基于策略的方法.\n",
    "* 通过**训练预期回报的价值函数**, 智能体将在每个状态下使用我们的函数定义我们的策略: 基于价值的方法."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* 最后, 我们谈到深度强化学习是因为**我们引入了深度神经网络去评估采取的动作(基于策略)或估算状态的价值(基于价值), 所以得名“深度”**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 让我们训练一个深度强化学习着陆器智能体来正确的着陆月球🌕并且上传到 Hugging Face Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第0步: 设置GPU 💪\n",
    "* 为了**更快的训练智能体, 我们将使用GPU**, 选择`修改 > 笔记本设置`\n",
    "\n",
    "![image.png](./assets/image1.png)\n",
    "\n",
    "* `硬件加速器 > GPU`\n",
    "\n",
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境**(记录视频帧).\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg\n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "第一步是安装多个依赖项:\n",
    "* `gym`: 包含LunarLander-v2环境🌛\n",
    "* `stable-baselines3`: 深度强化学习库.\n",
    "* `huggingface_sb3`: Stable-baselines3的插件, 用于从Hugging Face Hub 🤗 上下载或者上传模型."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym pygame box2d-py  # 如果使用Apple M1 conda install box2d-py\n",
    "!pip install stable-baselines3\n",
    "!pip install huggingface_sb3\n",
    "!pip install pyglet  # 如果你使用IDE, 则不需要这些步骤."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 导入包 📦\n",
    "我们导入的另一个库是 huggingface_hub, **它能从Hub上下载或者上传预训练模型**.\n",
    "\n",
    "Hugging Face Hub 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning&sort=downloads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from huggingface_sb3 import load_from_hub, package_to_hub, push_to_hub\n",
    "from huggingface_hub import notebook_login  # 需要登陆Hugging Face账户才能将模型上传到Hub.\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第3步: 了解什么是Gym以及它是如何工作的? 🤖️\n",
    "🏋️ 这个包含我们环境的库叫做Gym. 你将在**深度强化学习中使用很多Gym的环境**. \n",
    "\n",
    "Gym库提供了两个东西:\n",
    "* 允许你**创建强化学习环境**的接口.\n",
    "* **一组环境**(gym-control, atrai, box2D...).\n",
    "\n",
    "让我们看一个例子, 但首先让我们记住什么是强化学习循环."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在每一步:\n",
    "* 我们的智能体从**环境**接收**状态S0** -- 我们接收我们游戏(环境)的第一帧画面.\n",
    "* 基于**状态S0**, 智能体采取**行动A0** -- 我们的智能体将向右移动.\n",
    "* 从环境产生中**新状态S1** -- 新的一帧画面.\n",
    "* 环境给智能体一些**奖罚R1** -- 我们没有死(奖励+1).\n",
    "\n",
    "在Gym中:\n",
    "\n",
    "1⃣️ 我们使用`gym.make()`创建环境\n",
    "\n",
    "2⃣️ 我们使用`observation = env.reset()`重置环境到初始状态\n",
    "\n",
    "对于每一步:\n",
    "\n",
    "3⃣️ 我们使用模型获取一个动作(在我们的例子中, 我们获取一个随机的动作)\n",
    "\n",
    "4⃣️ 我们使用`env.step(action)`在环境中执行这个动作, 然后得到:\n",
    "* `obsevation`: 新状态(st+1)\n",
    "* `reward`: 我们执行动作后得到的奖罚\n",
    "* `done`: 提示这局游戏是否结束\n",
    "* `info`: 额外信息的字典(取决于具体的环境).\n",
    "\n",
    "如果这局游戏结束:\n",
    "* 我们再次使用`observation = env.reset()`重置环境到初始状态\n",
    "\n",
    "**让我们看看这个例子!** 确保读懂代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "# 首先, 我们创建LunarLander-v2的环境.\n",
    "env = gym.make('LunarLander-v2')  \n",
    "\n",
    "# 重置环境到初始状态.\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(20):\n",
    "    # 如果你使用IDE, 可以取消渲染环境的注释.\n",
    "    # env.render()\n",
    "    # 获取一个随机动作.\n",
    "    action = env.action_space.sample()  \n",
    "    print('动作: ', action)\n",
    "    \n",
    "    # 执行动作, 获取新状态, 奖罚, 这局游戏是否结束和额外信息.\n",
    "    observation, reward, done, info = env.step(action)\n",
    "    \n",
    "    # 如果这局游戏结束(这个例子中结束的标志是着陆, 坠毁或超时).\n",
    "    if done:\n",
    "        # 重置环境到初始状态.\n",
    "        print('环境被重置.')\n",
    "        observation = env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第4步: 创建LunarLander环境🌛并且了解它如何工作\n",
    "#### [环境 🎮](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)\n",
    "在第一个教程中, 我们将训练一个[登月着陆器]((https://www.gymlibrary.ml/environments/box2d/lunar_lander/)智能体, 它将正确的登陆月球. 为此, 智能体需要**学习去调整它的速度和位置(水平, 垂直和角度)来正确着陆**.\n",
    "\n",
    "---\n",
    "💡 当你开始使用一个环境的好习惯是查看文档.\n",
    "\n",
    "👉 https://www.gymlibrary.ml/environments/box2d/lunar_lander/\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "让我们一起看看环境是什么样的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 我们使用 gym.make('<环境的名称>') 创建环境.\n",
    "env = gym.make('LunarLander-v2')\n",
    "env.reset()\n",
    "print('_' * 5 + '可观察的环境' + '_' * 5)\n",
    "print()\n",
    "print('可观察的环境向量的形状', env.observation_space.shape)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们看到`可观察的环境空间的形状 (8,)`, 这指出可观察空间是一个大小为8的向量, 每个值是着陆器的不同信息:\n",
    "* 着陆器坐标(x)\n",
    "* 着陆器坐标(y)\n",
    "* 水平速度(x)\n",
    "* 垂直速度(y)\n",
    "* 角度\n",
    "* 角速度\n",
    "* 左侧起落架是否着陆\n",
    "* 右侧起落架是否着陆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5)\n",
    "print()\n",
    "print('动作的总数', env.action_space.n)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "动作空间(智能体可用的可能动作集)是有4个可用动作的离散值 🎮:\n",
    "* 不行动,\n",
    "* 启动左方向的引擎,\n",
    "* 启动主引擎,\n",
    "* 启动右方向的引擎.\n",
    "\n",
    "奖励函数(在每个时间步给予的奖励)💰:\n",
    "* 从屏幕顶部移动到着陆台, 零速度是100~140分.\n",
    "* 启动主引擎每帧画面 -0.3分.\n",
    "* 每侧起落架着地+10分.\n",
    "* 如果着陆器坠毁(额外-100分)或停止(+100分), 则本轮游戏结束.\n",
    "* 如果你的智能体得到200分, 你的任务就结束了."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 向量化环境\n",
    "* 我们创建一个由16个环境组成的向量化环境(堆叠多个独立环境在一个环境的方法), 这样**我们将有更多样化的体验在训练过程**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建环境.\n",
    "env = make_vec_env('LunarLander-v2', n_envs=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第5步: 创建一个模型 🤖️\n",
    "* 现在我们研究了我们的环境并且理解了我们的问题: **通过控制左, 右方向和主引擎能够正确的将着陆器降落到着陆台上**. 让我们构建一个算法来解决这个问题. 🚀\n",
    "* 为此, 我们将使用我们的第一个深度强化学习库, [Stable Baselines3(SB3)](https://stable-baselines3.readthedocs.io/en/master/).\n",
    "* SB3是**一组PyTorch强化学习算法的可靠实现集合**.\n",
    "---\n",
    "💡 使用新的库的好习惯是先阅读文档: https://stable-baselines3.readthedocs.io/en/master/ 然后尝试一些教程.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "为了解决这个问题, 我们将使用SB3 **PPO**. [PPO(又名近端策略优化(Proximal Policy Optimization))是一种在本课程中你将学到的最先进的深度强化学习算法之一.](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#example%5D)\n",
    "\n",
    "PPO是以下各项的组合:\n",
    "* 基于价值的强化学习方法: 通过学习一个动作-价值函数来告诉我们在**给定的状态和动作下的最有价值的动作**.\n",
    "* 基于策略的强化学习方法: 通过学习一个策略来给我们**动作的概率分布**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Stable-Baselines3 易于设置:\n",
    "\n",
    "1⃣️ 你**创建一个环境**(我们的例子中已经在上面完成)\n",
    "\n",
    "2⃣️ 你**定义一个你想使用的模型并实例化它**`model = PPO('MlpPolicy')`\n",
    "\n",
    "3⃣️ 你通过`model.learn()`**训练智能体**, 并定义训练的时间步\n",
    "\n",
    "```\n",
    "# 创建环境\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "# 实例化智能体\n",
    "model = PPO('MlpPolicy', env, verbose=1)\n",
    "# 训练智能体\n",
    "model.learn(total_timesteps=int(2e5))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 定义MlpPolicy PPO架构\n",
    "# 我们使用多层感知机(MlpPolicy)是因为输入是一个向量, \n",
    "# 如果我们输入是帧, 那我们将使用CnnPolicy.\n",
    "model = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 答案\n",
    "# 我们添加了一些参数来更快的训练\n",
    "model = PPO(policy='MlpPolicy',\n",
    "            env=env,\n",
    "            n_steps=1024,\n",
    "            batch_size=64,\n",
    "            n_epochs=4,\n",
    "            gamma=0.99,\n",
    "            gae_lambda=0.98,\n",
    "            ent_coef=0.01,\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第6步: 训练PPO智能体 🏃\n",
    "* 让我们训练智能体 500000 步, 不要忘记使用Colab的GPU. 这大概需要10分钟, 但如果你仅想尝试一下, 你可使用更少的时间步.\n",
    "* 在训练期间, ☕️好好休息一下吧 🤗"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 训练500000步\n",
    "\n",
    "# TODO: 指定模型的文件名并保存模型.\n",
    "model_name = ''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 答案\n",
    "# 训练500000步\n",
    "model.learn(total_timesteps=500000)\n",
    "# 保存模型\n",
    "model_name = 'ppo-LunarLander-v2'\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第7步: 评估智能体 📈\n",
    "* 现在我们的登月着陆器智能体已经训练好了🚀, 我们需要**检查它的性能**.\n",
    "* Stable-Baselines3 提供了一个方法`evaluate_policy`来进行评估.\n",
    "* 要填写哪些部分, 你需要[查看文档](https://stable-baselines3.readthedocs.io/en/master/guide/examples.html#basic-usage-training-saving-loading)\n",
    "* 下一步, 我们将了解**如何自动评估和分享你的智能体到排行榜竞争**, 现在让我们自己做\n",
    "\n",
    "💡 当你评估你的智能体时, 你不应该使用训练环境, 而是创建一个评估环境."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# TODO: 评估智能体\n",
    "# 创建一个新的评估环境\n",
    "eval_env = \n",
    "\n",
    "# 使用10个评估周期和确定性动作评估模型\n",
    "mean_reward, std_reward = \n",
    "\n",
    "# 打印结果\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "eval_env = gym.make('LunarLander-v2')\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f'mean_reward={mean_reward:.2f} +/- {std_reward}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* 就我而言, 在训练1百万步后我得到的平均奖励是`200.20 +/- 20.80`, 这意味着我们的登月着陆器智能体已经准备好登陆月球了.🌛🥳"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第8步(不涉及核心内容, 可选): 发布我们训练好的模型到 Hub 上 🔥\n",
    "现在我们看到经过训练之后得到了很棒的结果, 我们可以通过一行代码发布我们训练的模型到hub🤗上.\n",
    "\n",
    "📚 库文档 👉 https://github.com/huggingface/huggingface_sb3/tree/main#hugging-face--x-stable-baselines3-v20\n",
    "\n",
    "这有一个模型卡的例子(使用Space Invaders):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![ModelCard.gif](./assets/ModelCard.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "通过使用`package_to_hub`, **你可以评估, 记录回放视频, 生成智能体的模型卡并把它发布到hub**.\n",
    "\n",
    "---\n",
    "package_to_hub 函数\n",
    "---\n",
    "\n",
    "看这边:\n",
    "* 你可以**展示我们的作品** 🔥\n",
    "* 你可以**可视化智能体的活动** 👀\n",
    "* 你可以**与社区分享其他人也可以使用的智能体** 💾\n",
    "* 你可以**访问排行榜🏆以查看你的智能体和你同学的智能体相比如何** 👉 https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "为了能分享你的模型到社区, 有以下三个步骤需要做:\n",
    "\n",
    "1⃣️ (如果没有完成)创建一个Hugging Face账户 ➡ https://huggingface.co/join\n",
    "\n",
    "2⃣️ 登陆账户, 然后你需要保存一个Hugging Face的身份验证令牌(token).\n",
    "\n",
    "* 创建一个新的具有**写入规则**的令牌(https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* 复制令牌\n",
    "* 运行下面的脚本并输入令牌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "notebook_login()\n",
    "!git config --global credential.helper store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "如果你使用IDE, 也可在终端中使用以下命令:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3⃣️ 我们现在准备好使用`package_to_hub()`发布我们训练的智能体到🤗 hub 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "让我们填写`package_to_hub`函数:\n",
    "* `model`: 我们训练的模型.\n",
    "* `model_name`: 我们在`model.save()`中定义的模型的名称.\n",
    "* `model_architectrue`: 我们使用的模型架构: 在我们的例子中是PPO.\n",
    "* `env_id`: 环境的名称, 在我们的例子中是`LunarLander-v2`.\n",
    "* `eval_env`: 定义一个评估环境.\n",
    "* `repo_id`: 将创建/更新的Hugging Face Hub仓库的名称(repo_id={你的用户名/仓库名}).\n",
    "\n",
    "💡 **一个好的名字是{用户名}/{模型架构}-{环境名称}**\n",
    "\n",
    "* `commit_message`: 提交时的信息."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# 创建评估环境.\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "\n",
    "# TODO: 填写环境的名称.\n",
    "env_id = ''\n",
    "\n",
    "# TODO: 填写我们使用的模型结构.\n",
    "model_architecture = ''\n",
    "\n",
    "# TODO: 填写仓库id.\n",
    "# `repo_id`是Hugging Face Hub中模型存储库的id\n",
    "# (repo_id = {organization}/{repo_name} 例子 ThomasSimonini/ppo-LunarLander-v2)\n",
    "repo_id = ''\n",
    "\n",
    "# TODO: 填写提交信息.\n",
    "commit_message = ''\n",
    "\n",
    "# 在将仓库推送到hub之前, 需要保存评估模型, 生成模型卡以及录制智能体的回放视频.\n",
    "package_to_hub(model=model,  # 我们训练的模型.\n",
    "               model_name=model_name,  # 我们训练模型的名称.\n",
    "               model_architecture=model_architecture,  # 我们使用的模型架构: 在我们的例子中是PPO.\n",
    "               env_id=env_id,  # 环境的名称.\n",
    "               eval_env=eval_env,  # 评估环境.\n",
    "               repo_id=repo_id,  # Hugging Face Hub仓库的名称(repo_id={你的用户名/仓库名} 例子 ThomasSimonini/ppo-LunarLander-v2).\n",
    "               commit_message=commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "# 创建评估环境.\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id)])\n",
    "\n",
    "# 填写环境的名称.\n",
    "env_id = 'LunarLander-v2'\n",
    "\n",
    "# 填写我们使用的模型结构.\n",
    "model_architecture = 'PPO'\n",
    "\n",
    "# 填写仓库id.\n",
    "# 请更改成你自己的id!\n",
    "repo_id = 'ThomasSimonini/TEST2ppo-LunarLander-v2'\n",
    "\n",
    "# 填写提交信息.\n",
    "commit_message = 'Upload PPO LunarLander-v2 trained agent.'\n",
    "\n",
    "package_to_hub(model=model,  # 我们训练的模型.\n",
    "               model_name=model_name,  # 我们训练模型的名称.\n",
    "               model_architecture=model_architecture,\n",
    "               env_id=env_id,\n",
    "               eval_env=eval_env,\n",
    "               repo_id=repo_id,\n",
    "               commit_message=commit_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "恭喜🥳你刚刚训练并上传了你的第一个深度强化学习智能体. 上面的脚步应该显示了模型仓库的链接, 例如: https://huggingface.co/osanseviero/test_sb3. 当你访问这个链接时, 你可以:\n",
    "* 在右侧你可以看到智能体的回放视频.\n",
    "* 点击\"Files and versions\"以查看仓库中的全部文件.\n",
    "* 点击\"Use in stable-baselines3\"以获取如何加载模型的代码.\n",
    "* 得到描述模型的模型卡(文件`README.md`).\n",
    "\n",
    "在底层, Hub使用基于git的仓库(即使你不知道什么是git也不用担心), 这意味着你可以在实验和提高你的智能体以后更新新版本的模型.\n",
    "\n",
    "使用排行榜🏆比较你和同学的LunarLander-v2结果 👉 https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 额外的挑战(可选) 🏆\n",
    "最好的学习方式就是**自己进行尝试**! 如你所见, 当前的智能体还有做到最好. 作为第一个建议, 你可以训练更多的时间步. 比如1000000步, 我们可以看到更好的结果!\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard)中, 你将找到你的智能体的位置. 你想要获得第一吗?\n",
    "\n",
    "以下是一些实现这个目标的想法:\n",
    "* 训练更多的时间步\n",
    "* 尝试不同的`PPO`超参数. 你可以在 https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html#parameters 看到它们\n",
    "* 翻阅[Stable-Baselines3的文档](https://stable-baselines3.readthedocs.io/en/master/modules/dqn.html)并尝试其他的模型, 比如DQN.\n",
    "* **发布你训练的新模型**到Hub上 🔥\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard)上**比较你和同学的LunarLander-v2结果** 🏆 👉 https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard\n",
    "\n",
    "登月对你来说太无聊了? 尝试**其他环境**, 为什么不试试CartPole-v1, MountainCar-v0 或者 CarRacing-v0? [使用gym文档](https://www.gymlibrary.ml/)查询它们如何工作. 玩得开心🎉."
   ]
  },
  {
   "cell_type": "raw",
   "source": [
    "---\n",
    "祝贺你完成本章! 这才是最重要的, 这还有**一些额外的信息**.\n",
    "\n",
    "如果你仍然对这些感到困惑...这是完全正常的! **这对我和所有学习强化学习的人都是一样的**.\n",
    "\n",
    "在继续尝试其他挑战之前, **花一点时间真正的掌握这些内容**. 理解这些内容并打下基础是非常重要的.\n",
    "\n",
    "当然, 在后续课程中, 我们将会继续使用并再次解释这些内容, 但**最好是在开始下一章之前完全掌握这些**."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 这是专门为你打造的课程 👷🏿‍♀️\n",
    "\n",
    "我们希望根据你的反馈提高和改进课程. 如果你有一些建议, 请打开GitHub仓库的issue: https://github.com/huggingface/deep-rl-class/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第二单元见! 🔥\n",
    "## 不断学习, 不断精彩! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}