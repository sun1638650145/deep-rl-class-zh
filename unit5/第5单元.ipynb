{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c747ef",
   "metadata": {},
   "source": [
    "# 第5单元: 使用PyTorch编写你的第一个深度强化学习算法: Reinforce. 并测试它的鲁棒性 💪\n",
    "\n",
    "在这份笔记中, 你将从头编写你的第一个深度强化学习算法: Reinforce(也称为蒙特卡洛策略梯度).\n",
    "\n",
    "Reinforce是一种**基于策略的方法**: 一种**尝试直接优化策略而不是使用动作价值函数**的深度强化学习算法. 更准确的说, Reinforce是策略梯度方法, 是基于策略方法的子类, 旨在**通过使用梯度提升(Gradient Ascent)估计最优策略的权重来直接优化策略.**\n",
    "\n",
    "为了测试鲁棒性, 我们将在3个不同的简单环境进行训练:\n",
    "* Cartpole-v1\n",
    "* PixelcopterEnv\n",
    "* PongEnv\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit5频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境:\n",
    "\n",
    "* [CartPole-v1](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)\n",
    "* [Pixelcopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "* [Pong](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pong.html)\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870bdfa",
   "metadata": {},
   "source": [
    "![Sans titre.gif](./assets/Sans%20titre.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a640e4e",
   "metadata": {},
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用**PyTorch从头编写Reinforce算法.**\n",
    "* 能够使用**简单的环境测试你的智能体的鲁棒性.**\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77934d83",
   "metadata": {},
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![Deep Reinforcement Learning Course.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c62e4",
   "metadata": {},
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践.**\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ **在独特的环境中训练智能体.**\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddb776",
   "metadata": {},
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第5单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit5/README.md)\n",
    "\n",
    "🔲 📚 通过阅读 👉 https://huggingface.co/blog/deep-rl-pg **学习策略梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2882fa6",
   "metadata": {},
   "source": [
    "### 第0步: 设置GPU 💪\n",
    "\n",
    "* 为了**更快的训练智能体, 我们将使用GPU,** 选择`修改 > 笔记本设置`\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26f4cb",
   "metadata": {},
   "source": [
    "* `硬件加速器 > GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80291f9",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6945d",
   "metadata": {},
   "source": [
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境**(记录视频帧).\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg\n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=False, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c26a71",
   "metadata": {},
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "第一步是安装多个依赖项:\n",
    "* `gym`: 包含Cartpole-v1, Pixelcopter和Pong环境.\n",
    "* `gym-games`: 使用PyGame制作的gym环境.\n",
    "* `huggingface_hub`: 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这里看到全部可用的Reinforce模型. 👉 https://huggingface.co/models?other=reinforce\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "!pip install git+https://github.com/qlan3/gym-games.git\n",
    "!pip install huggingface_hub\n",
    "!pip install pyglet  # 如果你使用IDE, 则不需要这些步骤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d6fb9a",
   "metadata": {},
   "source": [
    "### 第2步: 导入包 📦\n",
    "\n",
    "除了安装的库, 我们还使用:\n",
    "* `imageio`: 生成回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31943094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0acf",
   "metadata": {},
   "source": [
    "* 让我们检查我们是否有GPU.\n",
    "* 如果有, 你应该看到`cuda:0`或者`mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19504f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d526c9",
   "metadata": {},
   "source": [
    "我们现在已经准备好实现我们的Reinforce算法 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f166ac",
   "metadata": {},
   "source": [
    "## 第一个智能体: 玩CartPole-v1 🤖️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8653a",
   "metadata": {},
   "source": [
    "### 第3步: 创建并理解CartPole环境\n",
    "#### [环境 🎮 ](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01ee36",
   "metadata": {},
   "source": [
    "![cartpole.jpg](./assets/cartpole.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553d754",
   "metadata": {},
   "source": [
    "### 为什么我们使用像CartPole-v1这样的简单环境?\n",
    "正如[强化学习技巧](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)所解释的, 当你从头开始实现智能体时, 你需要**确保它可以正确工作, 并在深入之前找到简单环境中的bug.** 因为在简单环境中更容易找到bug.\n",
    "> 尝试在玩具问题上有一些\"生命迹象\".\n",
    "\n",
    "> 通过在越来越复杂的环境上运行来验证实现(你可以和RL zoo的结果进行比较). 你通常需要为该步骤运行超参数优化.\n",
    "---\n",
    "#### CartPole-v1环境\n",
    "> 一根杆通过一个未驱动的接头连接到推车上, 该推车沿着无摩擦的轨道移动. 摆锤直立放置在推车上, 目标是通过在推车上施加左右方向的力, 使杆保持平衡.\n",
    "\n",
    "所以, 我们从CartPole-v1开始. 目标是向左或者向右推动车辆, **使杆保持平衡.**\n",
    "\n",
    "如果出现以下的情况, 则该局游戏结束:\n",
    "* 杆的倾角超过±12˚\n",
    "* 推车的位置变化超过±2.4\n",
    "* 每轮游戏超过500步\n",
    "\n",
    "杆保持平衡的每个时间步, 我们都会得到+1的奖励分数 💰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "# 创建环境.\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# 创建评估环境.\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# 获取状态空间和动作空间的大小.\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f03892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '可观察的环境' + '_' * 5, end='\\n\\n')\n",
    "print('可观察的环境向量的形状', s_size)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a62583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5, end='\\n\\n')\n",
    "print('动作的总数', a_size)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9110",
   "metadata": {},
   "source": [
    "### 第4步: 让我们构建Reinforce架构\n",
    "此实现基于两个实现:\n",
    "* [PyTorch官方强化学习示例](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "* [Udacity的Reinforce算法](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37237d",
   "metadata": {},
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd938180",
   "metadata": {},
   "source": [
    "所以我们需要:\n",
    "* 两个全连接层(fc1和fc2).\n",
    "* 使用ReLU作为全连接层fc1的激活函数.\n",
    "* 使用Softmax输出动作的概率分布."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ccb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        \n",
    "        # fc1输出到fc2.\n",
    "        \n",
    "        # 最后使用softmax激活输出.\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64797a2",
   "metadata": {},
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # fc1输出到fc2.\n",
    "        x = self.fc2(x)\n",
    "        # 最后使用softmax激活输出.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
