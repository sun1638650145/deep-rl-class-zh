{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c747ef",
   "metadata": {},
   "source": [
    "# 第5单元: 使用PyTorch编写你的第一个深度强化学习算法: Reinforce. 并测试它的鲁棒性 💪\n",
    "\n",
    "在这份笔记中, 你将从头编写你的第一个深度强化学习算法: Reinforce(也称为蒙特卡洛策略梯度).\n",
    "\n",
    "Reinforce是一种**基于策略的方法**: 一种**尝试直接优化策略而不是使用动作价值函数**的深度强化学习算法. 更准确的说, Reinforce是策略梯度方法, 是基于策略方法的子类, 旨在**通过使用梯度提升(Gradient Ascent)估计最优策略的权重来直接优化策略.**\n",
    "\n",
    "为了测试鲁棒性, 我们将在3个不同的简单环境进行训练:\n",
    "* Cartpole-v1\n",
    "* PixelcopterEnv\n",
    "* PongEnv\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit5频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境:\n",
    "\n",
    "* [CartPole-v1](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)\n",
    "* [Pixelcopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "* [Pong](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pong.html)\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870bdfa",
   "metadata": {},
   "source": [
    "![Sans titre.gif](./assets/Sans%20titre.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a640e4e",
   "metadata": {},
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用**PyTorch从头编写Reinforce算法.**\n",
    "* 能够使用**简单的环境测试你的智能体的鲁棒性.**\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77934d83",
   "metadata": {},
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![Deep Reinforcement Learning Course.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c62e4",
   "metadata": {},
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践.**\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ **在独特的环境中训练智能体.**\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddb776",
   "metadata": {},
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第5单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit5/README.md)\n",
    "\n",
    "🔲 📚 通过阅读 👉 https://huggingface.co/blog/deep-rl-pg **学习策略梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2882fa6",
   "metadata": {},
   "source": [
    "### 第0步: 设置GPU 💪\n",
    "\n",
    "* 为了**更快的训练智能体, 我们将使用GPU,** 选择`修改 > 笔记本设置`\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26f4cb",
   "metadata": {},
   "source": [
    "* `硬件加速器 > GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80291f9",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6945d",
   "metadata": {},
   "source": [
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境**(记录视频帧).\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573b000",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg\n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823ea2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=False, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c26a71",
   "metadata": {},
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "第一步是安装多个依赖项:\n",
    "* `gym`: 包含Cartpole-v1, Pixelcopter和Pong环境.\n",
    "* `gym-games`: 使用PyGame制作的gym环境.\n",
    "* `huggingface_hub`: 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这里看到全部可用的Reinforce模型. 👉 https://huggingface.co/models?other=reinforce\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e0050",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "!pip install git+https://github.com/qlan3/gym-games.git\n",
    "!pip install huggingface_hub\n",
    "!pip install pyglet  # 如果你使用IDE, 则不需要这些步骤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d6fb9a",
   "metadata": {},
   "source": [
    "### 第2步: 导入包 📦\n",
    "\n",
    "除了安装的库, 我们还使用:\n",
    "* `imageio`: 生成回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31943094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0acf",
   "metadata": {},
   "source": [
    "* 让我们检查我们是否有GPU.\n",
    "* 如果有, 你应该看到`cuda:0`或者`mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e0aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19504f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d526c9",
   "metadata": {},
   "source": [
    "我们现在已经准备好实现我们的Reinforce算法 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f166ac",
   "metadata": {},
   "source": [
    "## 第一个智能体: 玩CartPole-v1 🤖️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8653a",
   "metadata": {},
   "source": [
    "### 第3步: 创建并理解CartPole环境\n",
    "#### [环境 🎮 ](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01ee36",
   "metadata": {},
   "source": [
    "![cartpole.jpg](./assets/cartpole.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553d754",
   "metadata": {},
   "source": [
    "### 为什么我们使用像CartPole-v1这样的简单环境?\n",
    "正如[强化学习技巧](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)所解释的, 当你从头开始实现智能体时, 你需要**确保它可以正确工作, 并在深入之前找到简单环境中的bug.** 因为在简单环境中更容易找到bug.\n",
    "> 尝试在玩具问题上有一些\"生命迹象\".\n",
    "\n",
    "> 通过在越来越复杂的环境上运行来验证实现(你可以和RL zoo的结果进行比较). 你通常需要为该步骤运行超参数优化.\n",
    "---\n",
    "#### CartPole-v1环境\n",
    "> 一根杆通过一个未驱动的接头连接到推车上, 该推车沿着无摩擦的轨道移动. 摆锤直立放置在推车上, 目标是通过在推车上施加左右方向的力, 使杆保持平衡.\n",
    "\n",
    "所以, 我们从CartPole-v1开始. 目标是向左或者向右推动车辆, **使杆保持平衡.**\n",
    "\n",
    "如果出现以下的情况, 则该局游戏结束:\n",
    "* 杆的倾角超过±12˚\n",
    "* 推车的位置变化超过±2.4\n",
    "* 每轮游戏超过500步\n",
    "\n",
    "杆保持平衡的每个时间步, 我们都会得到+1的奖励分数 💰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f28ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "# 创建环境.\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# 创建评估环境.\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# 获取状态空间和动作空间的大小.\n",
    "s_size = env.observation_space.shape\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f03892",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '可观察的环境' + '_' * 5, end='\\n\\n')\n",
    "print('可观察的环境向量的形状', s_size)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a62583",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5, end='\\n\\n')\n",
    "print('动作的总数', a_size)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9110",
   "metadata": {},
   "source": [
    "### 第4步: 让我们构建Reinforce架构\n",
    "此实现基于两个实现:\n",
    "* [PyTorch官方强化学习示例](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "* [Udacity的Reinforce算法](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37237d",
   "metadata": {},
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd938180",
   "metadata": {},
   "source": [
    "所以我们需要:\n",
    "* 两个全连接层(fc1和fc2).\n",
    "* 使用ReLU作为全连接层fc1的激活函数.\n",
    "* 使用Softmax输出动作的概率分布."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ccb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        \n",
    "        # fc1输出到fc2.\n",
    "        \n",
    "        # 最后使用softmax激活输出.\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64797a2",
   "metadata": {},
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c57e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # fc1输出到fc2.\n",
    "        x = self.fc2(x)\n",
    "        # 最后使用softmax激活输出.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15479e8e",
   "metadata": {},
   "source": [
    "我这里有一个错误, 你能猜到在哪里吗?\n",
    "* 为了找到答案, 让我们前向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81803e",
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_policy = Policy(s_size, a_size 64).to(device)\n",
    "debug_policy.act(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a489633",
   "metadata": {},
   "source": [
    "* 这里我们看到错误说 `ValueError: The value argument to log_prob must be a Tensor`\n",
    "* 这表明`m.log_prob(action)`中的`action`必须是一个张量, **但现在不是.**\n",
    "* 你知道这是为什么吗? 检查函数`act`并尝试查看它为什么不正常工作.\n",
    "\n",
    "建议 💡: 这个实现中有问题. 记住我们的函数`act`是**我们想从动作的概率分布中采样一个动作.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefea48a",
   "metadata": {},
   "source": [
    "#### 答案\n",
    "* 由于**我们想从动作的概率分布中采样一个动作**, 但我们不能使用`action = np.argmax(m)`, 因为它总是输出具有最高概率的动作.\n",
    "* 我们需要替换成`action = m.sample()`, 它将从概率分布$P(.|s)$中采样一个动作出来.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f9333",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # fc1输出到fc2.\n",
    "        x = self.fc2(x)\n",
    "        # 最后使用softmax激活输出.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102e7e8",
   "metadata": {},
   "source": [
    "通过使用CartPole, 调试起来更加容易, 因为**我们知道bug来自我们的代码而不是我们的环境.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fe649",
   "metadata": {},
   "source": [
    "### 第5步: 构建Reinforce训练算法\n",
    "* 与伪代码相反, 我们在每轮(episode)之后更新策略, **而不是使用一个批次的轮(batch of episodes).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70a514",
   "metadata": {},
   "source": [
    "![image.png](./assets/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18fd15",
   "metadata": {},
   "source": [
    "为什么我们要最小化损失? 你说的是梯度上升(Gradient Ascent)而不是梯度下降(Gradient Descent)?\n",
    "* 我们想最大化我们的目标函数$J(\\theta)$, 但是在PyTorch中就像在TensorFlow中一样, **最好是最小化目标函数.**\n",
    "    * 因此, 假设我们想在某个时间步加强动作$a_3$. 在训练这个动作之前, 它的概率是0.25.\n",
    "    * 所以我们需要修改$\\theta$, 使得$\\pi_\\theta(a_3|s;\\theta)>0.25$\n",
    "    * 因为概率和必须为1, 所以最大化$\\pi_\\theta(a_3|s;\\theta)$将**最小化其他动作的概率.**\n",
    "    * 所以我们应该告诉PyTorch去**最小化$1-\\pi_\\theta(a_3|s;\\theta)$.**\n",
    "    * 当$\\pi_\\theta(a_3|s;\\theta)$趋近1时, 损失函数趋近0.\n",
    "    * 所以这等同于最大化梯度$\\pi_\\theta(a_3|s;\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb5941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # 帮助我们计算训练过程中的分数.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # 伪代码第3行.\n",
    "    for i_episode in range(1, n_training_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state =  # TODO: 重置环境.\n",
    "        # 伪代码第4行.\n",
    "        for t in range(max_t):\n",
    "            action, log_prob =  # TODO: 获取一个动作.\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, info =  # TODO: 在环境上执行动作.\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        score_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # 伪代码第6行: 计算奖罚值.\n",
    "        # 这里我们计算衰减, 例如: [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        # 我们计算奖罚值的总和sum(gamma[t] * reward[t])\n",
    "        R = sum([a * b for a, b in zip( , )])  # TODO: 我们需要在函数zip()中填入什么, 请记住我们计算gamma[t] * reward[t]\n",
    "\n",
    "        # 伪代码第7行.\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # 伪代码第8行: PyTorch执行计算.\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('第{}轮\\t 平均得分{:.2f}'.format(i_episode, np.mean(score_deque)))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7fe31",
   "metadata": {},
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048229b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # 帮助我们计算训练过程中的分数.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # 伪代码第3行.\n",
    "    for i_episode in range(1, n_training_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # 伪代码第4行.\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        score_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # 伪代码第6行: 计算奖罚值.\n",
    "        # 这里我们计算衰减, 例如: [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        # 我们计算奖罚值的总和sum(gamma[t] * reward[t])\n",
    "        R = sum([a * b for a, b in zip(discounts, rewards)])\n",
    "\n",
    "        # 伪代码第7行.\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # 伪代码第8行: PyTorch执行计算.\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('第{}轮\\t 平均得分{:.2f}'.format(i_episode, np.mean(score_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9713c",
   "metadata": {},
   "source": [
    "### 训练\n",
    "* 我们已经准备好训练我们的智能体.\n",
    "* 首先, 我们需要定义一个包含所有训练超参数的变量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa90a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    'h_size': 16,\n",
    "    'n_training_episodes': 1000,\n",
    "    'n_evaluation_episodes': 10,\n",
    "    'max_t': 1000,\n",
    "    'gamma': 1.0,\n",
    "    'lr': 1e-2,\n",
    "    'env_id': env_id,\n",
    "    'state_space': s_size,\n",
    "    'action_space': a_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建模型并将它放到硬件设备上.\n",
    "cartpole_policy = Policy(cartpole_hyperparameters['state_space'],\n",
    "                         cartpole_hyperparameters['action_space'],\n",
    "                         cartpole_hyperparameters['h_size']).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78258a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters['n_training_episodes'],\n",
    "                   cartpole_hyperparameters['max_t'],\n",
    "                   cartpole_hyperparameters['gamma'],\n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a89a9",
   "metadata": {},
   "source": [
    "### 第6步:  定义评估函数 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d252f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"用`n_eval_episodes`轮评估智能体, 并返回奖励的均值和标准差.\n",
    "\n",
    "    Args:\n",
    "        env: 评估环境.\n",
    "        max_steps: 每轮的最大步数.\n",
    "        n_eval_episodes: 测试的总轮数.\n",
    "        policy: Reinforce智能体.\n",
    "\n",
    "    Returns:\n",
    "        奖励的均值和标准差.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        \n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    \n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dd7f5",
   "metadata": {},
   "source": [
    "### 第7步: 评估我们的智能体 📈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50db69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env,\n",
    "               cartpole_hyperparameters['max_t'],\n",
    "               cartpole_hyperparameters['n_evaluation_episodes'],\n",
    "               cartpole_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
