{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a3c747ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第5单元: 使用PyTorch编写你的第一个深度强化学习算法: Reinforce. 并测试它的鲁棒性 💪\n",
    "\n",
    "在这份笔记中, 你将从头编写你的第一个深度强化学习算法: Reinforce(也称为蒙特卡洛策略梯度).\n",
    "\n",
    "Reinforce是一种**基于策略的方法**: 一种**尝试直接优化策略而不是使用动作价值函数**的深度强化学习算法. 更准确的说, Reinforce是策略梯度方法, 是基于策略方法的子类, 旨在**通过使用梯度提升(Gradient Ascent)估计最优策略的权重来直接优化策略.**\n",
    "\n",
    "为了测试鲁棒性, 我们将在3个不同的简单环境进行训练:\n",
    "* Cartpole-v1\n",
    "* PixelCopterEnv\n",
    "* PongEnv\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit5频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境:\n",
    "\n",
    "* [CartPole-v1](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)\n",
    "* [PixelCopter](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "* [Pong](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pong.html)\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e870bdfa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Sans titre.gif](./assets/Sans%20titre.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a640e4e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用**PyTorch从头编写Reinforce算法.**\n",
    "* 能够使用**简单的环境测试你的智能体的鲁棒性.**\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77934d83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![Deep Reinforcement Learning Course.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e5c62e4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践.**\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ **在独特的环境中训练智能体.**\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ddb776",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第5单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit5/README.md)\n",
    "\n",
    "🔲 📚 通过阅读 👉 https://huggingface.co/blog/deep-rl-pg **学习策略梯度**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2882fa6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第0步: 设置GPU 💪\n",
    "\n",
    "* 为了**更快的训练智能体, 我们将使用GPU,** 选择`修改 > 笔记本设置`\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e26f4cb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* `硬件加速器 > GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80291f9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be6945d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境**(记录视频帧).\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b573b000",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg\n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823ea2c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=False, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69c26a71",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "第一步是安装多个依赖项:\n",
    "* `gym`: 包含Cartpole-v1, PixelCopter和Pong环境.\n",
    "* `gym-games`: 使用PyGame制作的gym环境.\n",
    "* `huggingface_hub`: 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这里看到全部可用的Reinforce模型. 👉 https://huggingface.co/models?other=reinforce\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?pipeline_tag=reinforcement-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b8e0050",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym\n",
    "!pip install git+https://github.com/ntasfi/PyGame-Learning-Environment.git\n",
    "!pip install git+https://github.com/qlan3/gym-games.git\n",
    "!pip install huggingface_hub\n",
    "!pip install pyglet  # 如果你使用IDE, 则不需要这些步骤."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d6fb9a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 导入包 📦\n",
    "\n",
    "除了安装的库, 我们还使用:\n",
    "* `imageio`: 生成回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31943094",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "\n",
    "import gym\n",
    "import gym_pygame\n",
    "\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb0acf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* 让我们检查我们是否有GPU.\n",
    "* 如果有, 你应该看到`cuda:0`或者`mps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97e0aa5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda:0')\n",
    "elif torch.backends.mps.is_available() and torch.backends.mps.is_built():\n",
    "    device = torch.device('mps')\n",
    "else:\n",
    "    device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19504f29",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d526c9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们现在已经准备好实现我们的Reinforce算法 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f166ac",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 第1个智能体: 玩CartPole-v1 🤖️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b8653a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第3步: 创建并理解CartPole环境\n",
    "#### [环境 🎮 ](https://www.gymlibrary.ml/environments/classic_control/cart_pole/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa01ee36",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![cartpole.jpg](./assets/cartpole.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1553d754",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 为什么我们使用像CartPole-v1这样的简单环境?\n",
    "正如[强化学习技巧](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html)所解释的, 当你从头开始实现智能体时, 你需要**确保它可以正确工作, 并在深入之前找到简单环境中的bug.** 因为在简单环境中更容易找到bug.\n",
    "> 尝试在玩具问题上有一些\"生命迹象\".\n",
    "\n",
    "> 通过在越来越复杂的环境上运行来验证实现(你可以和RL zoo的结果进行比较). 你通常需要为该步骤运行超参数优化.\n",
    "\n",
    "---\n",
    "\n",
    "#### CartPole-v1环境\n",
    "\n",
    "> 一根杆通过一个未驱动的接头连接到推车上, 该推车沿着无摩擦的轨道移动. 摆锤直立放置在推车上, 目标是通过在推车上施加左右方向的力, 使杆保持平衡.\n",
    "\n",
    "所以, 我们从CartPole-v1开始. 目标是向左或者向右推动车辆, **使杆保持平衡.**\n",
    "\n",
    "如果出现以下的情况, 则该局游戏结束:\n",
    "* 杆的倾角超过±12˚\n",
    "* 推车的位置变化超过±2.4\n",
    "* 每轮游戏超过500步\n",
    "\n",
    "杆保持平衡的每个时间步, 我们都会得到+1的奖励分数 💰."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3f28ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_id = 'CartPole-v1'\n",
    "# 创建环境.\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# 创建评估环境.\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "# 获取状态空间和动作空间的大小.\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f03892",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('_' * 5 + '可观察的环境' + '_' * 5, end='\\n\\n')\n",
    "print('可观察的环境向量的形状', s_size)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a62583",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5, end='\\n\\n')\n",
    "print('动作的总数', a_size)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ce9110",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第4步: 让我们构建Reinforce架构\n",
    "此实现基于两个实现:\n",
    "* [PyTorch官方强化学习示例](https://github.com/pytorch/examples/blob/main/reinforcement_learning/reinforce.py)\n",
    "* [Udacity的Reinforce算法](https://github.com/udacity/deep-reinforcement-learning/blob/master/reinforce/REINFORCE.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c37237d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd938180",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "所以我们需要:\n",
    "* 两个全连接层(fc1和fc2).\n",
    "* 使用ReLU作为全连接层fc1的激活函数.\n",
    "* 使用Softmax输出动作的概率分布."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71ccb0d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        \n",
    "        # fc1输出到fc2.\n",
    "        \n",
    "        # 最后使用softmax激活输出.\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64797a2",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8c57e5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # fc1输出到fc2.\n",
    "        x = self.fc2(x)\n",
    "        # 最后使用softmax激活输出.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state)\n",
    "        m = Categorical(probs)\n",
    "        action = np.argmax(m)\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15479e8e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我这里有一个错误, 你能猜到在哪里吗?\n",
    "* 为了找到答案, 让我们前向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e81803e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "debug_policy = Policy(s_size, a_size, 64).to(device)\n",
    "debug_policy.act(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a489633",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* 这里我们看到错误说 `ValueError: The value argument to log_prob must be a Tensor`\n",
    "* 这表明`m.log_prob(action)`中的`action`必须是一个张量, **但现在不是.**\n",
    "* 你知道这是为什么吗? 检查函数`act`并尝试查看它为什么不正常工作.\n",
    "\n",
    "建议 💡: 这个实现中有问题. 记住我们的函数`act`是**我们想从动作的概率分布中采样一个动作.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aefea48a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案\n",
    "* 由于**我们想从动作的概率分布中采样一个动作**, 但我们不能使用`action = np.argmax(m)`, 因为它总是输出具有最高概率的动作.\n",
    "* 我们需要替换成`action = m.sample()`, 它将从概率分布$P(.|s)$中采样一个动作出来."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d5f9333",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        # 创建两个全连接层.\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"定义前向传播.\"\"\"\n",
    "        # 状态输入到fc1, 然后使用ReLU激活.\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # fc1输出到fc2.\n",
    "        x = self.fc2(x)\n",
    "        # 最后使用softmax激活输出.\n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        \"\"\"给定一个状态获得一个动作.\"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0102e7e8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "通过使用CartPole, 调试起来更加容易, 因为**我们知道bug来自我们的代码而不是我们的环境.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2fe649",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第5步: 构建Reinforce训练算法\n",
    "* 与伪代码相反, 我们在每轮(episode)之后更新策略, **而不是使用一个批次的轮(batch of episodes).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70a514",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed18fd15",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "为什么我们要最小化损失? 你说的是梯度上升(Gradient Ascent)而不是梯度下降(Gradient Descent)?\n",
    "* 我们想最大化我们的目标函数$J(\\theta)$, 但是在PyTorch中就像在TensorFlow中一样, **最好是最小化目标函数.**\n",
    "    * 因此, 假设我们想在某个时间步加强动作$a_3$. 在训练这个动作之前, 它的概率是0.25.\n",
    "    * 所以我们需要修改$\\theta$, 使得$\\pi_\\theta(a_3|s;\\theta)>0.25$\n",
    "    * 因为概率和必须为1, 所以最大化$\\pi_\\theta(a_3|s;\\theta)$将**最小化其他动作的概率.**\n",
    "    * 所以我们应该告诉PyTorch去**最小化$1-\\pi_\\theta(a_3|s;\\theta)$.**\n",
    "    * 当$\\pi_\\theta(a_3|s;\\theta)$趋近1时, 损失函数趋近0.\n",
    "    * 所以这等同于最大化梯度$\\pi_\\theta(a_3|s;\\theta)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedb5941",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # 帮助我们计算训练过程中的分数.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # 伪代码第3行.\n",
    "    for i_episode in range(1, n_training_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state =  # TODO: 重置环境.\n",
    "        # 伪代码第4行.\n",
    "        for t in range(max_t):\n",
    "            action, log_prob =  # TODO: 获取一个动作.\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, info =  # TODO: 在环境上执行动作.\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        score_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # 伪代码第6行: 计算奖罚值.\n",
    "        # 这里我们计算衰减, 例如: [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        # 我们计算奖罚值的总和sum(gamma[t] * reward[t])\n",
    "        R = sum([a * b for a, b in zip( , )])  # TODO: 我们需要在函数zip()中填入什么, 请记住我们计算gamma[t] * reward[t]\n",
    "\n",
    "        # 伪代码第7行.\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # 伪代码第8行: PyTorch执行计算.\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('第{}轮\\t 平均得分{:.2f}'.format(i_episode, np.mean(score_deque)))\n",
    "            \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc7fe31",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048229b8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # 帮助我们计算训练过程中的分数.\n",
    "    score_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # 伪代码第3行.\n",
    "    for i_episode in range(1, n_training_episodes + 1):\n",
    "        saved_log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        # 伪代码第4行.\n",
    "        for t in range(max_t):\n",
    "            action, log_prob = policy.act(state)\n",
    "            saved_log_probs.append(log_prob)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break\n",
    "        score_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        # 伪代码第6行: 计算奖罚值.\n",
    "        # 这里我们计算衰减, 例如: [0.99^1, 0.99^2, 0.99^3, ..., 0.99^len(rewards)]\n",
    "        discounts = [gamma ** i for i in range(len(rewards) + 1)]\n",
    "        # 我们计算奖罚值的总和sum(gamma[t] * reward[t])\n",
    "        R = sum([a * b for a, b in zip(discounts, rewards)])\n",
    "\n",
    "        # 伪代码第7行.\n",
    "        policy_loss = []\n",
    "        for log_prob in saved_log_probs:\n",
    "            policy_loss.append(-log_prob * R)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "\n",
    "        # 伪代码第8行: PyTorch执行计算.\n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if i_episode % print_every == 0:\n",
    "            print('第{}轮\\t 平均得分{:.2f}'.format(i_episode, np.mean(score_deque)))\n",
    "\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d9713c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 训练\n",
    "* 我们已经准备好训练我们的智能体.\n",
    "* 首先, 我们需要定义一个包含所有训练超参数的变量."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa90a3c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cartpole_hyperparameters = {\n",
    "    'h_size': 16,\n",
    "    'n_training_episodes': 1000,\n",
    "    'n_evaluation_episodes': 10,\n",
    "    'max_t': 1000,\n",
    "    'gamma': 1.0,\n",
    "    'lr': 1e-2,\n",
    "    'env_id': env_id,\n",
    "    'state_space': s_size,\n",
    "    'action_space': a_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a78c85b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建模型并将它放到硬件设备上.\n",
    "cartpole_policy = Policy(cartpole_hyperparameters['state_space'],\n",
    "                         cartpole_hyperparameters['action_space'],\n",
    "                         cartpole_hyperparameters['h_size']).to(device)\n",
    "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78258a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = reinforce(cartpole_policy,\n",
    "                   cartpole_optimizer,\n",
    "                   cartpole_hyperparameters['n_training_episodes'],\n",
    "                   cartpole_hyperparameters['max_t'],\n",
    "                   cartpole_hyperparameters['gamma'],\n",
    "                   100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a89a9",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第6步:  定义评估函数 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2165264e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
    "    \"\"\"用`n_eval_episodes`轮评估智能体, 并返回奖励的均值和标准差.\n",
    "\n",
    "    Args:\n",
    "        env: 评估环境.\n",
    "        max_steps: 每轮的最大步数.\n",
    "        n_eval_episodes: 测试的总轮数.\n",
    "        policy: Reinforce智能体.\n",
    "\n",
    "    Returns:\n",
    "        奖励的均值和标准差.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            action, _ = policy.act(state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "        \n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    \n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606dd7f5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第7步: 评估我们的智能体 📈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50db69b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_agent(eval_env,\n",
    "               cartpole_hyperparameters['max_t'],\n",
    "               cartpole_hyperparameters['n_evaluation_episodes'],\n",
    "               cartpole_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f85ce5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第8步(不涉及核心内容, 可选): 发布我们训练好的模型到Hub上 🚀\n",
    "现在我们看到经过训练之后得到了很棒的结果, 我们可以通过一行代码发布我们训练的模型到hub 🤗 上.\n",
    "\n",
    "这有一个模型卡的例子:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdcfb1",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120778aa",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在底层, Hub使用基于git的仓库(即使你不知道什么是git也不用担心), 这意味着你可以在实验和提高你的智能体以后更新新版本的模型."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c80bdb6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 发布到Hugging Face Hub\n",
    "#### 请勿修改下面的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a4631d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, Repository\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fadf3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def record_video(env, policy, out_directory, fps=30):\n",
    "    images = []  \n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "    \n",
    "    while not done:\n",
    "        # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "        action, _ = policy.act(state)\n",
    "        state, reward, done, info = env.step(action) # 我们直接使用next_state = state来记录顺序(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        \n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f806349d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   hyperparameters,\n",
    "                   eval_env,\n",
    "                   video_fps=30,\n",
    "                   local_repo_path='hub',\n",
    "                   commit_message='Push Reinforce agent to Hub',\n",
    "                   token=None):\n",
    "    _, repo_name = repo_id.split('/')\n",
    "\n",
    "    # 第0步: 克隆或创建仓库.\n",
    "    # 创建仓库(如果内容不为空, 则克隆).\n",
    "    api = HfApi()\n",
    "\n",
    "    repo_url = api.create_repo(repo_id=repo_id,\n",
    "                               token=token,\n",
    "                               private=False,\n",
    "                               exist_ok=True)\n",
    "\n",
    "    # git pull\n",
    "    repo_local_path = Path(local_repo_path) / repo_name\n",
    "    repo = Repository(repo_local_path, clone_from=repo_url, use_auth_token=True)\n",
    "    repo.git_pull()\n",
    "\n",
    "    repo.lfs_track(['*.mp4'])\n",
    "\n",
    "    # 第1步: 保存模型.\n",
    "    torch.save(model, os.path.join(repo_local_path, 'model.pt'))\n",
    "\n",
    "    # 第2步: 保存超参数到JSON.\n",
    "    with open(Path(repo_local_path) / 'hyperparameters.json', 'w') as outfile:\n",
    "        json.dump(hyperparameters, outfile)\n",
    "\n",
    "    # 第3步: 评估模型并构建JSON.\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env,\n",
    "                                             hyperparameters['max_t'],\n",
    "                                             hyperparameters['n_evaluation_episodes'],\n",
    "                                             model)\n",
    "\n",
    "    # 首先, 获取当前时间.\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "        'env_id': hyperparameters['env_id'],\n",
    "        'mean_reward': mean_reward,\n",
    "        'n_evaluation_episodes': hyperparameters['n_evaluation_episodes'],\n",
    "        'eval_datetime': eval_form_datetime,\n",
    "    }\n",
    "    # 写入JSON文件.\n",
    "    with open(Path(repo_local_path) / 'results.json', 'w') as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # 第4步: 创建模型卡.\n",
    "    env_name = hyperparameters['env_id']\n",
    "\n",
    "    metadata = {\n",
    "        'tags': [\n",
    "            env_name,\n",
    "            'reinforce',\n",
    "            'reinforcement-learning',\n",
    "            'custom-implementation',\n",
    "            'deep-rl-class'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 添加评估.\n",
    "    eval = metadata_eval_result(model_pretty_name=repo_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_name,\n",
    "                                dataset_id=env_name)\n",
    "\n",
    "    # 合并所有的字典{metadata, eval}.\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f'''\n",
    "      # 使用**Reinforce**智能体来玩**{env_id}**\n",
    "      这是一个使用**Reinforce**训练有素的模型玩**{env_id}**.\n",
    "      要学习使用这个模型并训练你的模型, 请查阅深度强化学习课程第5单元: https://github.com/huggingface/deep-rl-class/tree/main/unit5\n",
    "      '''\n",
    "\n",
    "    readme_path = repo_local_path / 'README.md'\n",
    "\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # 保存我们的评估信息到README的元数据.\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # 第5步: 录制回放视频.\n",
    "    video_path = repo_local_path / 'replay.mp4'\n",
    "    record_video(env, model, video_path, video_fps)\n",
    "\n",
    "    # 发布到Hub.\n",
    "    print(f'发布 {repo_name} 到你的Hugging Face Hub')\n",
    "    repo.push_to_hub(commit_message=commit_message)\n",
    "\n",
    "    print(f'你的模型已经发布到Hub. 你可以点击链接查看的你的模型: {repo_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5ab08c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "通过使用`package_to_hub`, **你可以评估, 记录回放视频, 生成智能体的模型卡并把它发布到hub.**\n",
    "\n",
    "看这边:\n",
    "\n",
    "* 你可以**展示我们的作品** 🔥\n",
    "* 你可以**可视化智能体的活动** 👀\n",
    "* 你可以**与社区分享其他人也可以使用的智能体** 💾\n",
    "* 你可以**访问排行榜🏆以查看你的智能体和你同学的智能体相比如何** 👉 https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a5b4e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "为了能分享你的模型到社区, 有以下三个步骤需要做:\n",
    "\n",
    "1⃣️ (如果没有完成)创建一个Hugging Face账户 ➡ https://huggingface.co/join\n",
    "\n",
    "2⃣️ 登陆账户, 然后你需要保存一个Hugging Face的身份验证令牌(token).\n",
    "\n",
    "* 创建一个新的具有**写入规则**的令牌(https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6a39d19",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image5.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6b3ace",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04eb7896",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "如果你使用IDE, 也可在终端中使用以下命令:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa1f47d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f0321b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "3⃣️ 我们现在准备好使用`package_to_hub()`发布我们训练的智能体到🤗 Hub 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9707b350",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo_id = ''  # TODO: 定义你的仓库id:{你的用户名/Reinforce-{model-id}}.\n",
    "package_to_hub(repo_id,\n",
    "               cartpole_policy,  # 我们想保存的模型.\n",
    "               cartpole_hyperparameters,  # 超参数.\n",
    "               eval_env,  # 评估环境.\n",
    "               video_fps=30,\n",
    "               local_repo_path='hub',\n",
    "               commit_message='Push Reinforce agent to the Hub')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a35993",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "现在我们尝试我们实现的智能体的鲁棒性, 让我们实验一下更复杂的环境例如:\n",
    "* PixelCopter\n",
    "* Pong"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d9b4a",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 第2个智能体: PixelCopter 🚁\n",
    "### 第1步: 研究PixelCopter环境 👀\n",
    "* [环境的文档](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pixelcopter.html)\n",
    "\n",
    "可观察的环境空间的形状(7,) 👀:\n",
    "* 玩家y坐标\n",
    "* 玩家的速度\n",
    "* 玩家到地面的距离\n",
    "* 玩家到顶部的距离\n",
    "* 下一个方块x坐标到玩家的距离\n",
    "* 下一个方块顶部的y坐标\n",
    "* 下一个方块底部的y坐标\n",
    "\n",
    "动作空间有2个可用动作 🎮:\n",
    "* 向上移动,\n",
    "* 向下移动.\n",
    "\n",
    "奖励函数(在每个时间步给予的奖励)💰:\n",
    "* 通过每个垂直方块, 它都会得到+1的奖励. 每次都到结束状态都得到-1的惩罚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd5b43b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_id = 'Pixelcopter-PLE-v0'\n",
    "\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc085d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 定义超参数 ⚙️\n",
    "* 因为这个环境比较复杂, 所以我们需要调整超参数.\n",
    "* 特别是隐藏层的大小, 我们需要更多的神经元."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2ca5c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pixelcopter_hyperparameters = {\n",
    "    'h_size': 64,\n",
    "    'n_training_episodes': 50000,\n",
    "    'n_evaluation_episodes': 10,\n",
    "    'max_t': 10000,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 1e-4,\n",
    "    'env_id': env_id,\n",
    "    'state_space': s_size,\n",
    "    'action_space': a_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e98ddc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6cb2298",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建模型并将它放到硬件设备上.\n",
    "# torch.manual_seed(50)\n",
    "pixelcopter_policy = Policy(pixelcopter_hyperparameters['state_space'],\n",
    "                            pixelcopter_hyperparameters['action_space'],\n",
    "                            pixelcopter_hyperparameters['h_size']).to(device)\n",
    "pixelcopter_optimizer = optim.Adam(pixelcopter_policy.parameters(), lr=pixelcopter_hyperparameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8b4f8b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pixelcopter_policy,\n",
    "                   pixelcopter_optimizer,\n",
    "                   pixelcopter_hyperparameters['n_training_episodes'],\n",
    "                   pixelcopter_hyperparameters['max_t'],\n",
    "                   pixelcopter_hyperparameters['gamma'],\n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb0019",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo_id = ''  # TODO: 定义你的仓库id:{你的用户名/Reinforce-{model-id}}.\n",
    "package_to_hub(repo_id,\n",
    "               pixelcopter_policy,  # 我们想保存的模型.\n",
    "               pixelcopter_hyperparameters,  # 超参数.\n",
    "               eval_env,  # 评估环境.\n",
    "               video_fps=30,\n",
    "               local_repo_path='hub',\n",
    "               commit_message='Push Reinforce agent to the Hub')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002ff61c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 第3个智能体: Pong 🎾\n",
    "### 第1步: 研究Pong环境 👀\n",
    "* [环境的文档](https://pygame-learning-environment.readthedocs.io/en/latest/user/games/pong.html)\n",
    "\n",
    "可观察的环境空间的形状(7,) 👀:\n",
    "* 玩家y坐标\n",
    "* 玩家的速度\n",
    "* 电脑的位置\n",
    "* 球的x坐标\n",
    "* 球的y坐标\n",
    "* 球的水平速度\n",
    "* 球的垂直速度\n",
    "\n",
    "动作空间有3个可用动作 🎮:\n",
    "* 向上移动,\n",
    "* 向下移动,\n",
    "* 不移动.\n",
    "\n",
    "奖励函数(在每个时间步给予的奖励)💰:\n",
    "* 每个对手未接到的球(在球拍后), 它都会得到+1的奖励. 每个未接到的球都得到-1的惩罚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd685f7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env_id = 'Pong-PLE-v0'\n",
    "\n",
    "env = gym.make(env_id)\n",
    "eval_env = gym.make(env_id)\n",
    "\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec329f35",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 定义超参数 ⚙️\n",
    "* 因为这个环境比较复杂, 所以我们需要调整超参数.\n",
    "* 特别是隐藏层的大小, 我们需要更多的神经元."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a77c6a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pong_hyperparameters = {\n",
    "    'h_size': 64,\n",
    "    'n_training_episodes': 20000,\n",
    "    'n_evaluation_episodes': 10,\n",
    "    'max_t': 5000,\n",
    "    'gamma': 0.99,\n",
    "    'lr': 1e-2,\n",
    "    'env_id': env_id,\n",
    "    'state_space': s_size,\n",
    "    'action_space': a_size\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1ec611",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self, s_size, a_size, h_size):\n",
    "        super(Policy, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "        self.fc2 = nn.Linear(h_size, h_size * 2)\n",
    "        self.fc3 = nn.Linear(h_size * 2, a_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return F.softmax(x, dim=1)\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        probs = self.forward(state).cpu()\n",
    "        m = Categorical(probs)\n",
    "        action = m.sample()\n",
    "\n",
    "        return action.item(), m.log_prob(action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d1a7512",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第3步: 训练智能体 🏃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3b58aa",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建模型并将它放到硬件设备上.\n",
    "# torch.manual_seed(50)\n",
    "pong_policy = Policy(pong_hyperparameters['state_space'],\n",
    "                     pong_hyperparameters['action_space'],\n",
    "                     pong_hyperparameters['h_size']).to(device)\n",
    "pong_optimizer = optim.Adam(pong_policy.parameters(), lr=pong_hyperparameters['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245ecfc9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "scores = reinforce(pong_policy,\n",
    "                   pong_optimizer,\n",
    "                   pong_hyperparameters['n_training_episodes'],\n",
    "                   pong_hyperparameters['max_t'],\n",
    "                   pong_hyperparameters['gamma'],\n",
    "                   1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5bd9a54",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第4步: 发布我们训练好的模型到Hugging Face Hub 🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f13afc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "repo_id = ''  # TODO: 定义你的仓库id:{你的用户名/Reinforce-{model-id}}.\n",
    "package_to_hub(repo_id,\n",
    "               pong_policy,  # 我们想保存的模型.\n",
    "               pong_hyperparameters,  # 超参数.\n",
    "               eval_env,  # 评估环境.\n",
    "               video_fps=30,\n",
    "               local_repo_path='hub',\n",
    "               commit_message='Push Reinforce agent to the Hub')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a65aae",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 额外的挑战(可选) 🏆\n",
    "最好的学习方式就是**自己进行尝试**! 如你所见, 当前的智能体还有做到最好. 作为第一个建议, 你可以训练更多的时间步. 也可以尝试找到更好的参数.\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard)中, 你将找到你的智能体的位置. 你想要获得第一吗?\n",
    "\n",
    "以下是一些实现这个目标的想法:\n",
    "* 训练更多的时间步\n",
    "* 尝试不同的超参数. 你可以在 👉 https://huggingface.co/models?other=reinforce 看到其他同学的超参数它们\n",
    "* **发布你训练的新模型**到Hub上 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095e1cbe",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "祝贺你完成本章! 这才是最重要的, 这还有**一些额外的信息**.\n",
    "\n",
    "如果你仍然对这些感到困惑...这是完全正常的! **这对我和所有学习强化学习的人都是一样的**.\n",
    "\n",
    "在继续尝试其他挑战之前, **花一点时间真正的掌握这些内容**. 理解这些内容并打下基础是非常重要的.\n",
    "\n",
    "当然, 在后续课程中, 我们将会继续使用并再次解释这些内容, 但**最好是在开始下一章之前完全掌握这些**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6357ab15",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 这是专门为你打造的课程 👷🏿‍♀️\n",
    "\n",
    "我们希望根据你的反馈提高和改进课程. 如果你有一些建议, 请打开GitHub仓库的issue: https://github.com/huggingface/deep-rl-class/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed8cb13f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "第6单元见! 🔥\n",
    "## 不断学习, 不断精彩! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}