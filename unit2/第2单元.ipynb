{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e129ae5b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第2单元: 使用FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 进行 Q-Learning\n",
    "在这份笔记中, **你将从头开始编写你的第一个强化学习智能体**使用Q-Learning来玩冰冻湖 ❄️ 并将其分享到社区, 请记得实验不同的配置\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit2频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境: \n",
    "* [FrozenLake-v1](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "* [Taxi-v3](https://www.gymlibrary.ml/environments/toy_text/taxi/)\n",
    "\n",
    "📚 强化学习库: Python和NumPy\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b785d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![envs.gif](./assets/envs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a9749",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用环境库**Gym**.\n",
    "* 能够从头开始编写一个Q-Learning智能体.\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cca1b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![深度强化学习课程.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b511db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践**.\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ 在独特的环境中训练**智能体**.\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef533af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第2单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit2/README.md)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第1部分**](https://huggingface.co/blog/deep-rl-q-part1)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第2部分**](https://huggingface.co/blog/deep-rl-q-part2)\n",
    "\n",
    "🔲 📢 注册[我们的Discord服务器](https://discord.gg/aYka4Yhff9)并**在#introduce-yourself频道介绍自己** 🥳\n",
    "\n",
    "🔲 🐕 你是Discord新手吗? 请查看我们的**discord 101以获得最佳实践** 👉 https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441b3cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一个Q-Learning的小回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668d36e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Q-Learning是一种**强化学习算法**\n",
    "    * 通过训练一个包含**动作(action)-价值(value)的Q-函数(Q-Function)**, Q-表(Q-Table)作为内部存储器, 它包含了**所有的状态-动作对.**\n",
    "    * 给定一个状态和动作, 我们的Q-函数将**搜索Q-表对应的值.**\n",
    "\n",
    "![Q-函数-2.jpg](./assets/Q-function-2.jpg)\n",
    "\n",
    "* 当训练完成时, **我们得到一个最优的Q-函数, 因此得到一个最优的Q-表.**\n",
    "* 如果我们有**最优的Q-函数**, 那我们就知道**对于每个状态最好的行动是什么**, 所以我们就有一个最优策略.\n",
    "\n",
    "![link-value-policy.jpg](./assets/link-value-policy.jpg)\n",
    "\n",
    "但是一开始, 我们的**Q-表是无效的, 因为它的每个状态-动作对都是任意值(大多数时候我们将Q-表初始化为0).** 当我们探索环境并更新我们的Q-表时, 它将给我们越来越好的近似值.\n",
    "\n",
    "![Q-learning-1.jpg](./assets/Q-learning-1.jpg)\n",
    "\n",
    "这是Q-Learning的伪代码:\n",
    "\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bef6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第0步: 设置虚拟屏幕 🖥\n",
    "\n",
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境(记录视频帧).**\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fafa06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg \n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1cad2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa149d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "\n",
    "第一步是安装多个依赖项:\n",
    "\n",
    "* `gym`: 包含FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 环境.\n",
    "* `pygame`: 用于FrozenLake-v1和Taxi-v3的UI.\n",
    "* `numpy`: 用于处理我们的Q-表.\n",
    "\n",
    "Hugging Face Hub 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b047e48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym==0.24.0 pygame\n",
    "!pip install huggingface_hub\n",
    "!pip install imageio imageio-ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1550b30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 导入包 📦\n",
    "\n",
    "除了安装的库, 我们还使用:\n",
    "* `imageio`: 生成回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1b6ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61dbef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们现在已经准备好编写我们的Q-Learning算法了 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56942fdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第1部分: 冰冻湖 ⛄️ (不打滑版)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea830f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第1步: 创建并理解[冰冻湖环境 ⛄️ ](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "💡 当你开始使用一个环境的好习惯是查看它的文档.\n",
    "\n",
    "👉 https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "我们将训练我们的Q-Learning智能体从起始状态(S)导航到目标状态(G), 方法是仅在冰面(F)上行走并避开冰窟窿(H).\n",
    "\n",
    "我们有两个尺寸的环境:\n",
    "* `map_name='4x4'`: 4x4地图版本.\n",
    "* `map_name='8x8'`: 8x8地图版本.\n",
    "\n",
    "环境有两种模式:\n",
    "* `is_slippery=False`: 由于冰冻湖不打滑的特性, 智能体总能沿预期方向移动.\n",
    "* `is_slippery=True`: 由于冰冻湖打滑(随机)的特性, 智能体可能不总沿预期方向移动."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e90d83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "现在让我们使用4x4地图和不打滑的版本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73daa2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建FrozenLake-v1环境并使用4x4地图和不打滑的版本.\n",
    "env = gym.make()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703ea55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0dc1a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(id='FrozenLake-v1',\n",
    "               map_name='4x4',\n",
    "               is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7e02c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "你也可以自定义地图来使用:\n",
    "```python\n",
    "desc=['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "但是我们现在将使用默认的环境."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01f278",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 让我们一起看看环境是什么样的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ce7cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 我们使用 gym.make('<环境的名称>') 创建环境.\n",
    "env.reset()\n",
    "print('_' * 5 + '可观察的环境' + '_' * 5)\n",
    "print()\n",
    "print('可观察的环境', env.observation_space)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8e0ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们通过`可观察的环境 Discrete(16)`看到, 可观察的环境是一个表示**智能体当前位置current_row * nrows + current_col(行和列均从0开始)的值.**\n",
    "\n",
    "例如, 在4x4的地图上目标位置可以被计算成: 3 * 4 + 3 = 15. 可能的环境值取决于地图的大小. **例如, 4x4的地图有16个可能的环境值.**\n",
    "\n",
    "对于当前, 初始值 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d48284",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9df15b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5)\n",
    "print()\n",
    "print('动作的总数', env.action_space.n)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f9e30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "动作空间(智能体可用的动作集)是有4个可用动作的离散值 🎮:\n",
    "\n",
    "* 0: 向左走\n",
    "* 1: 向下走\n",
    "* 2: 向右走\n",
    "* 3: 向上走\n",
    "\n",
    "奖励函数 💰:\n",
    "\n",
    "* 到达目标: +1\n",
    "* 到达冰窟窿: 0\n",
    "* 到达冰面: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1231d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 创建并初始化Q-表 🗄\n",
    "(👀 伪代码的第1步)\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)\n",
    "是时候初始化我们的Q-表了! 要知道我们需要使用多少行(states)和列(actions), 我们需要知道可观察和动作空间. OpenAI Gym给我们提供了一个方法: `env.action_space.n`和`env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c4683",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_space = \n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = \n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feab293",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 让我们创建大小为(state_space, action_space)的Q-表, 并使用np.zeros将它们的值初始化为0.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = \n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8b4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34ad24",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5ec6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb424dd5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 让我们创建大小为(state_space, action_space)的Q-表, 并使用np.zeros将它们的值初始化为0.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros([state_space, action_space])\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca6b80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26a614",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第3步: 定义epsilon-greedy策略 🤖️\n",
    "epsilon-greedy是一个平衡探索/经验的训练策略.\n",
    "\n",
    "epsilon-greedy的思路是:\n",
    "* 当概率为$1-\\epsilon$: **我们使用经验**(也就是我们的智能体选择一个最高的状态-动作对值的动作).\n",
    "* 当概率为$\\epsilon$: **我们进行探索**(尝试随机动作).\n",
    "\n",
    "然后随着训练的进行, 我们逐渐**降低epsilon的值, 因为我们需要越来越少的探索和使用越来越多的经验.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5b68d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Q-learning-3.jpg](./assets/Q-learning-3.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fddf6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # 随机生成一个在0和1之前的数字.\n",
    "    random_num =\n",
    "    # 如果random_num > epsilon --> 使用经验.\n",
    "    if random_num > epsilon:\n",
    "        # 从给定状态中采取最大值的动作;\n",
    "        # 在这np.argmax()非常有用.\n",
    "        action =\n",
    "    # 否则 --> 探索\n",
    "    else:\n",
    "        action = # 进行随机动作.\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca932559",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4ab83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # 随机生成一个在0和1之前的数字.\n",
    "    random_num = np.random.uniform(0, 1)\n",
    "    # 如果random_num > epsilon --> 使用经验.\n",
    "    if random_num > epsilon:\n",
    "        # 从给定状态中采取最大值的动作;\n",
    "        # 在这np.argmax()非常有用.\n",
    "        action = np.argmax(Qtable[state])\n",
    "    # 否则 --> 探索\n",
    "    else:\n",
    "        action = env.action_space.sample() # 进行随机动作.\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f710c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第4步: 定义贪心策略 🤖️\n",
    "请记住我们有两个策略, 因为Q-Learning是一种**off-policy**算法. 这意味着我们使用**不同的策略来执行动作和更新价值函数.**\n",
    "* epsilon-greedy策略(动作策略)\n",
    "* 贪心策略(更新策略)\n",
    "\n",
    "贪心策略也将是我们训练Q-learning智能体后最终的策略. 贪心策略用来从Q-表中选择一个动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347f0f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![off-on-4.jpg](./assets/off-on-4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaf2c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # 经验: 从给定状态中采取最大值的动作.\n",
    "    action =\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9707be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa8a8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # 经验: 从给定状态中采取最大值的动作.\n",
    "    action = np.argmax(Qtable[state])\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37db1bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第5步: 定义超参数 ⚙️\n",
    "与探索相关的参数是一些最重要的超参数.\n",
    "* 我们需要确保我们的智能体**探索足够的状态空间**来学习一个好的值近似(value approximation), 为此我们需要逐渐降低epsilon.\n",
    "* 如果你epsilon减小得过快(太高的衰减率), **你将承担智能体卡住的风险**, 因为你的智能体没有探索足够的状态空间, 因此无法解决问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803d2a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 训练参数.\n",
    "n_training_episodes = 10000  # 训练的总轮数.\n",
    "learning_rate = 0.7  # 学习率.\n",
    "\n",
    "# 评估参数.\n",
    "n_eval_episodes = 100  # 测试的总轮数.\n",
    "\n",
    "# 环境参数.\n",
    "env_id = 'FrozenLake-v1'  # 环境的名称.\n",
    "max_steps = 99  # 每轮的最大步数.\n",
    "gamma = 0.95  # 衰减系数.\n",
    "eval_seed = []  # 环境的评估种子.\n",
    "\n",
    "# 探索参数.\n",
    "epsilon = 1.0  # 探索率.\n",
    "max_epsilon = 1.0  # 初始探索率.\n",
    "min_epsilon = 0.05  # 最小探索率.\n",
    "decay_rate = 0.0005  # 指数探索衰减率."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d589f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第6步: 创建训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2cc59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in range(n_training_episodes):\n",
    "        # 减小epsilon(因为我们需要越来越少的探索).\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # 重置环境.\n",
    "        state = env.reset()\n",
    "\n",
    "        # 循环.\n",
    "        for step in range(max_steps):\n",
    "            # 使用epsilon-greedy策略选择动作At.\n",
    "            action =\n",
    "\n",
    "            # 采取动作At并观察Rt+1和St+1,\n",
    "            # 采取动作(a)并观察结果状态(s')和奖罚(r).\n",
    "            new_state, reward, done, info =\n",
    "\n",
    "            # 更新Q(s, a) = Q(s, a) + lr[R(s, a) + gamma * max Q(s', a') - Q(s, a)]\n",
    "            Qtable[state][action] =\n",
    "\n",
    "            # 如果done=True, 退出循环.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # 更新状态.\n",
    "            state = new_state\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b60fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c9cd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in range(n_training_episodes):\n",
    "        # 减小epsilon(因为我们需要越来越少的探索).\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # 重置环境.\n",
    "        state = env.reset()\n",
    "\n",
    "        # 循环.\n",
    "        for step in range(max_steps):\n",
    "            # 使用epsilon-greedy策略选择动作At.\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # 采取动作At并观察Rt+1和St+1,\n",
    "            # 采取动作(a)并观察结果状态(s')和奖罚(r).\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # 更新Q(s, a) = Q(s, a) + lr[R(s, a) + gamma * max Q(s', a') - Q(s, a)]\n",
    "            Qtable[state][action] += learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            # 如果done=True, 退出循环.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # 更新状态.\n",
    "            state = new_state\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06172632",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第7步: 训练Q-Learning智能体 🏃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2191ced",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77aabf4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第8步: 现在让我们看看Q-表是什么样子 👀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88e97a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf9a46",
   "metadata": {},
   "source": [
    "### 第9步: 定义评估函数 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517015da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \"\"\"用`n_eval_episodes`轮评估智能体, 并返回奖励的均值和标准差.\n",
    "\n",
    "    Args:\n",
    "        env: 评估环境.\n",
    "        max_steps: 每轮的最大步数.\n",
    "        n_eval_episodes: 测试的总轮数.\n",
    "        Q: Q-表.\n",
    "        seed: 评估随机种子数组(用于Taxi-v3).\n",
    "\n",
    "    Returns:\n",
    "        奖励的均值和标准差.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        if seed:\n",
    "            state = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state = env.reset()\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "            action = greedy_policy(Q, state)\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f834b",
   "metadata": {},
   "source": [
    "### 第10步: 评估我们的Q-Learning智能体 📈\n",
    "* 正常情况你的奖励均值应该是1.0\n",
    "* 这相对容易, 因为状态空间确实非常小(16). 你可以尝试[使用打滑版替换.](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估我们的智能体.\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f'平均奖励={mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c1d9f",
   "metadata": {},
   "source": [
    "### 第11步(不涉及核心内容, 可选): 发布我们训练好的模型到 Hub 上 🔥\n",
    "\n",
    "现在我们看到经过训练之后得到了很棒的结果, 我们可以通过一行代码发布我们训练的模型到hub🤗上.\n",
    "\n",
    "这有一个模型卡的例子:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57b64b",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17605326",
   "metadata": {},
   "source": [
    "在底层, Hub使用基于git的仓库(即使你不知道什么是git也不用担心), 这意味着你可以在实验和提高你的智能体以后更新新版本的模型."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de30536",
   "metadata": {},
   "source": [
    "#### 请勿修改下面的代码(用于发布到Hugging Face Hub, 不需要理解原理)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4422442",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, Repository\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457841e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "    images = []  \n",
    "    done = False\n",
    "    state = env.reset(seed=np.random.randint(0, 500))\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "    \n",
    "    while not done:\n",
    "        # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "        action = np.argmax(Qtable[state])\n",
    "        state, reward, done, info = env.step(action) # 我们直接使用next_state = state来记录顺序(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        \n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339969cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   env,\n",
    "                   video_fps=1,\n",
    "                   local_repo_path='hub',\n",
    "                   commit_message='Push Q-Learning agent to Hub',\n",
    "                   token=None):\n",
    "    _, repo_name = repo_id.split('/')\n",
    "\n",
    "    eval_env = env\n",
    "\n",
    "    # 第0步: 克隆或创建仓库.\n",
    "    # 创建仓库(如果内容不为空, 则克隆).\n",
    "    api = HfApi()\n",
    "\n",
    "    repo_url = api.create_repo(repo_id=repo_id,\n",
    "                               token=token,\n",
    "                               private=False,\n",
    "                               exist_ok=True)\n",
    "\n",
    "    # git pull\n",
    "    repo_local_path = Path(local_repo_path) / repo_name\n",
    "    repo = Repository(repo_local_path, clone_from=repo_url, use_auth_token=True)\n",
    "    repo.git_pull()\n",
    "\n",
    "    repo.lfs_track(['*.mp4'])\n",
    "\n",
    "    # 第1步: 保存模型.\n",
    "    if env.spec.kwargs.get('map_name'):\n",
    "        model['map_name'] = env.spec.kwargs.get('map_name')\n",
    "        if not env.spec.kwargs.get('is_slippery', ''):\n",
    "            model['slippery'] = False\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # 序列化模型.\n",
    "    with open(Path(repo_local_path) / 'q-learning.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # 第2步: 评估模型并构建JSON.\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env, model['max_steps'], model['n_eval_episodes'], model['qtable'],\n",
    "                                             model['eval_seed'])\n",
    "\n",
    "    # 首先, 获取当前时间.\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "        'env_id': model['env_id'],\n",
    "        'mean_reward': mean_reward,\n",
    "        'n_eval_episodes': model['n_eval_episodes'],\n",
    "        'eval_datetime': eval_form_datetime,\n",
    "    }\n",
    "    # 写入JSON文件.\n",
    "    with open(Path(repo_local_path) / 'results.json', 'w') as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # 第3步: 创建模型卡.\n",
    "    env_name = model['env_id']\n",
    "    if env.spec.kwargs.get('map_name'):\n",
    "        env_name += '-' + env.spec.kwargs.get('map_name')\n",
    "\n",
    "    if not env.spec.kwargs.get('is_slippery', ''):\n",
    "        env_name += '-' + 'no_slippery'\n",
    "\n",
    "    metadata = {\n",
    "        'tags': [\n",
    "            env_name,\n",
    "            'q-learning',\n",
    "            'reinforcement-learning',\n",
    "            'custom-implementation'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 添加评估.\n",
    "    eval = metadata_eval_result(model_pretty_name=repo_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_name,\n",
    "                                dataset_id=env_name)\n",
    "\n",
    "    # 合并所有的字典{metadata, eval}.\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f'''\n",
    "      # 使用**Q-Learning**智能体来玩**{env_id}**\n",
    "      这是一个使用**Q-Learning**训练有素的模型玩**{env_id}**.\n",
    "      '''\n",
    "\n",
    "    model_card += '''\n",
    "      ## 用法\n",
    "      ```python\n",
    "      '''\n",
    "\n",
    "    model_card += f'''model = load_from_hub(repo_id='{repo_id}', filename='q-learning.pkl')\n",
    "\n",
    "      # 不要忘记检查是否需要添加额外的参数(例如is_slippery=False)\n",
    "      env = gym.make(model['env_id'])\n",
    "\n",
    "      evaluate_agent(env, model['max_steps'], model['n_eval_episodes'], model['qtable'], model['eval_seed'])\n",
    "      '''\n",
    "\n",
    "    model_card += '''\n",
    "      ```\n",
    "      '''\n",
    "\n",
    "    readme_path = repo_local_path / 'README.md'\n",
    "\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # 保存我们的评估信息到README的元数据.\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # 第4步: 录制回放视频.\n",
    "    video_path = repo_local_path / 'replay.mp4'\n",
    "    record_video(env, model['qtable'], video_path, video_fps)\n",
    "\n",
    "    # 发布到Hub.\n",
    "    print(f'发布 {repo_name} 到你的Hugging Face Hub')\n",
    "    repo.push_to_hub(commit_message=commit_message)\n",
    "\n",
    "    print(f'你的模型已经发布到Hub. 你可以点击链接查看的你的模型: {repo_url}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c72e58d",
   "metadata": {},
   "source": [
    "通过使用`package_to_hub`, **你可以评估, 记录回放视频, 生成智能体的模型卡并把它发布到hub.**\n",
    "\n",
    "看这边:\n",
    "\n",
    "* 你可以**展示我们的作品** 🔥\n",
    "* 你可以**可视化智能体的活动** 👀\n",
    "* 你可以**与社区分享其他人也可以使用的智能体** 💾\n",
    "* 你可以**访问排行榜🏆以查看你的智能体和你同学的智能体相比如何** 👉 https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a79f562",
   "metadata": {},
   "source": [
    "为了能分享你的模型到社区, 有以下三个步骤需要做:\n",
    "\n",
    "1⃣️ (如果没有完成)创建一个Hugging Face账户 ➡ https://huggingface.co/join\n",
    "\n",
    "2⃣️ 登陆账户, 然后你需要保存一个Hugging Face的身份验证令牌(token).\n",
    "\n",
    "* 创建一个新的具有**写入规则**的令牌(https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ce9f36",
   "metadata": {},
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a247bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba3f727f",
   "metadata": {},
   "source": [
    "如果你使用IDE, 也可在终端中使用以下命令:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb86263",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73cd5b44",
   "metadata": {},
   "source": [
    "3⃣️ 我们现在准备好使用`package_to_hub()`发布我们训练的智能体到🤗 Hub 🔥.\n",
    "* 让我们创建**包含超参数和Q-表的模型字典.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352b06f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'n_training_episodes': n_training_episodes,\n",
    "    'learning_rate': learning_rate,\n",
    "\n",
    "    'n_eval_episodes': n_eval_episodes,\n",
    "\n",
    "    'env_id': env_id,\n",
    "    'max_steps': max_steps,\n",
    "    'gamma': gamma,\n",
    "    'eval_seed': eval_seed,\n",
    "\n",
    "    'epsilon': epsilon,\n",
    "    'max_epsilon': max_epsilon,\n",
    "    'min_epsilon': min_epsilon,\n",
    "    'decay_rate': decay_rate,\n",
    "\n",
    "    'qtable': Qtable_frozenlake\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d3dee2",
   "metadata": {},
   "source": [
    "让我们填写`package_to_hub`函数:\n",
    "* `repo_id`: 将创建/更新的Hugging Face Hub仓库的名称`(repo_id={你的用户名/仓库名})`. 💡 一个好的名字是**{用户名}/q-{环境名称}**\n",
    "* `model`: 包含超参数和Q-表的模型字典.\n",
    "* `env`: 环境.\n",
    "* `commit_message`: 提交时的信息."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b042cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb307558",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = ''  # 记得填写用户名.\n",
    "repo_name = 'q-FrozenLake-v1-4x4-noSlippery'\n",
    "package_to_hub(repo_id=f'{username}/{repo_name}',\n",
    "               model=model,\n",
    "               env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf9f954c",
   "metadata": {},
   "source": [
    "恭喜🥳你刚刚从头实现了训练并上传了你的第一个深度强化学习智能体. 冰冻湖(不打滑版)是一个非常简单的环境, 让我们尝试更难的. 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d723ff96",
   "metadata": {},
   "source": [
    "# 第2部分: 出租车-v3 🚖"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e5c023",
   "metadata": {},
   "source": [
    "### 第1步: 创建并理解[出租车-v3环境 🚕 ](https://www.gymlibrary.ml/environments/toy_text/taxi/)\n",
    "---\n",
    "💡 当你开始使用一个环境的好习惯是查看它的文档.\n",
    "\n",
    "👉 https://www.gymlibrary.ml/environments/toy_text/taxi/\n",
    "\n",
    "---\n",
    "在出租车-v3环境 🚕 的网格世界中, 有4个指定的位置, 分别由R(红), G(绿), Y(黄)和B(蓝)指示. 当新一轮开始时, 出租车从随机的方格出现并且乘客也在一个随机的位置出现. 出租车开到乘客的位置接上乘客, 将乘客拉到目的地(4个指定位置中的另一个), 然后下车. 一旦乘客下车, 这一轮就结束了."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a3afe0",
   "metadata": {},
   "source": [
    "![image.png](./assets/image3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459d096a",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccd119e",
   "metadata": {},
   "source": [
    "因为出租车**有25个位置, 乘客有5个可能的位置**(包括乘客就在出租车上)以及**4个目的地位置**, 所以一共有**500个离散状态**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a0b1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b97a97a1",
   "metadata": {},
   "source": [
    "动作空间(智能体可用的动作集)是有**6个可用动作**的离散值 🎮:\n",
    "\n",
    "* 0: 向南走\n",
    "* 1: 向北走\n",
    "* 2: 向东走\n",
    "* 3: 向西走\n",
    "* 4: 接乘客\n",
    "* 5: 乘客下车\n",
    "\n",
    "奖励函数 💰:\n",
    "\n",
    "* -1/每步, 除非触发其他奖励.\n",
    "* +20 接送乘客.\n",
    "* -10 非法执行'接乘客'或'乘客下车'的动作."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554787b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建我们的Q-表, 其中包含500个状态行和6个动作列(500x6).\n",
    "Qtable_taxi = initialize_q_table(state_space, action_space)\n",
    "print(Qtable_taxi)\n",
    "print('Q-表的大小: ', Qtable_taxi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9763ffca",
   "metadata": {},
   "source": [
    "### 第2步: 定义超参数 ⚙️\n",
    "⚠️ 请不要修改`eval_seed`: `eval_seed`数组**允许我们为每个同学使用相同的出租车初始位置来评估你的智能体.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07ef50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练参数.\n",
    "n_training_episodes = 25000  # 训练的总轮数.\n",
    "learning_rate = 0.7  # 学习率.\n",
    "\n",
    "# 评估参数.\n",
    "n_eval_episodes = 100  # 测试的总轮数.\n",
    "\n",
    "# 请不要修改eval_seed\n",
    "eval_seed = [16, 54, 165, 177, 191, 191, 120, 80, 149, 178, 48, 38, 6, 125, 174,\n",
    "             73, 50, 172, 100, 148, 146, 6, 25, 40, 68, 148, 49, 167, 9, 97,\n",
    "             164, 176, 61, 7, 54, 55, 161, 131, 184, 51, 170, 12, 120, 113, 95,\n",
    "             126, 51, 98, 36, 135, 54, 82, 45, 95, 89, 59, 95, 124, 9, 113,\n",
    "             58, 85, 51, 134, 121, 169, 105, 21, 30, 11, 50, 65, 12, 43, 82,\n",
    "             145, 152, 97, 106, 55, 31, 85, 38, 112, 102, 168, 123, 97, 21, 83,\n",
    "             158, 26, 80, 63, 5, 81, 32, 11, 28, 148]  # 评估种子, 确保所有同学的智能体在相同的出租车初始位置上训练.\n",
    "                                                       # 每个种子有指定的初始状态.\n",
    "                                                       \n",
    "# 环境参数.\n",
    "env_id = 'Taxi-v3'  # 环境的名称.\n",
    "max_steps = 99  # 每轮的最大步数.\n",
    "gamma = 0.95  # 衰减系数.\n",
    "\n",
    "# 探索参数.\n",
    "epsilon = 1.0  # 探索率.\n",
    "max_epsilon = 1.0  # 初始探索率.\n",
    "min_epsilon = 0.05  # 最小探索率.\n",
    "decay_rate = 0.0005  # 指数探索衰减率."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657df40c",
   "metadata": {},
   "source": [
    "### 第3步: 训练Q-Learning智能体 🏃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503010bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable_taxi = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_taxi)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da99049",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable_taxi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d473b",
   "metadata": {},
   "source": [
    "### 第4步(不涉及核心内容, 可选): 创建模型字典 💾 并发布我们训练好的模型到 Hub 上 🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55455398",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = {\n",
    "    'n_training_episodes': n_training_episodes,\n",
    "    'learning_rate': learning_rate,\n",
    "\n",
    "    'n_eval_episodes': n_eval_episodes,\n",
    "\n",
    "    'env_id': env_id,\n",
    "    'max_steps': max_steps,\n",
    "    'gamma': gamma,\n",
    "    'eval_seed': eval_seed,\n",
    "\n",
    "    'epsilon': epsilon,\n",
    "    'max_epsilon': max_epsilon,\n",
    "    'min_epsilon': min_epsilon,\n",
    "    'decay_rate': decay_rate,\n",
    "\n",
    "    'qtable': Qtable_taxi\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e524344",
   "metadata": {},
   "outputs": [],
   "source": [
    "username = ''  # 记得填写用户名.\n",
    "repo_name = 'q-Taxi-v3'\n",
    "package_to_hub(repo_id=f'{username}/{repo_name}',\n",
    "               model=model,\n",
    "               env=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e3f6d4",
   "metadata": {},
   "source": [
    "使用排行榜比较你的Taxi-v3智能体和你同学的. 🏆 👉 https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14c2210",
   "metadata": {},
   "source": [
    "![unnamed.png](./assets/unnamed.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c902de9",
   "metadata": {},
   "source": [
    "# 第3部分: 从Hub加载 🔽\n",
    "Hugging Face Hub 🤗 的惊人之处在于, 你可以轻松地从社区下载强大的模型.\n",
    "\n",
    "从Hub下载模型非常简单.\n",
    "1. 你可以访问https://huggingface.co/models?other=q-learning查看所有的Q-Learning模型列表.\n",
    "2. 你可以选择一个并且复制它的`repo_id`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90931688",
   "metadata": {},
   "source": [
    "![image.png](./assets/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03b6a50",
   "metadata": {},
   "source": [
    "3. 然后我们只需要使用函数`load_from_hub`: \n",
    "* `repo_id`: Hugging Face Hub中模型仓库id.\n",
    "* `filename`: 仓库中的zip文件名."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe514854",
   "metadata": {},
   "source": [
    "#### 请勿修改代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c077f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "\n",
    "def load_from_hub(repo_id: str,\n",
    "                  filename: str) -> str:\n",
    "    \"\"\"从Hugging Face Hub下载模型.\n",
    "\n",
    "    Args:\n",
    "        repo_id: Hugging Face Hub中模型仓库id.\n",
    "        filename: 仓库中的zip文件名.\n",
    "        \n",
    "    Return: \n",
    "        从Hugging Face Hub下载的模型.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from huggingface_hub import cached_download, hf_hub_url\n",
    "    except ImportError:\n",
    "        raise ImportError(\n",
    "            '你需要安装huggingface_hub来使用`load_from_hub`.'\n",
    "            '访问 https://pypi.org/project/huggingface-hub/ 下载.'\n",
    "        )\n",
    "\n",
    "    # 从Hub上获取模型, 下载并缓存模型到你的本地磁盘.\n",
    "    pickle_model = hf_hub_download(repo_id=repo_id,\n",
    "                                   filename=filename)\n",
    "\n",
    "    with open(pickle_model, 'rb') as fp:\n",
    "        downloaded_model_file = pickle.load(fp)\n",
    "\n",
    "    return downloaded_model_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b98c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id='ThomasSimonini/q-Taxi-v3',\n",
    "                      filename='q-learning.pkl')\n",
    "print(model)\n",
    "    \n",
    "env = gym.make(model['env_id'])\n",
    "evaluate_agent(env, model['max_steps'], model['n_eval_episodes'], model['qtable'], model['eval_seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf18652",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_from_hub(repo_id='ThomasSimonini/q-FrozenLake-v1-no-slippery',\n",
    "                      filename='q-learning.pkl')\n",
    "\n",
    "env = gym.make(model['env_id'], is_slippery=False)\n",
    "evaluate_agent(env, model['max_steps'], model['n_eval_episodes'], model['qtable'], model['eval_seed'])    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ec4f5e0",
   "metadata": {},
   "source": [
    "恭喜🥳, 你刚刚编写, 训练并上传了你的第一个深度强化学习智能体.\n",
    "\n",
    "理解Q-Learning是**理解基于价值的方法的重要一步.**\n",
    "\n",
    "在下个深度Q-Learning单元中, 我们将看到创建和更新Q-表是一个很棒的策略--但是, **这没有拓展性.**\n",
    "\n",
    "例如, 想象你创建一个智能体学习玩Doom. Doom是一个有海量状态空间(数百万不同的状态)的大型环境. 为该环境创建和更新一个Q-表效率不高. 这就是我们要研究深度Q-Learning的原因, **这是一种使用神经网络的算法, 在给定的状态, 每个动作都有不同的Q-值近似.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a39171",
   "metadata": {},
   "source": [
    "## 额外的挑战(可选) 🏆\n",
    "\n",
    "最好的学习方式就是**自己进行尝试!** 如你所见, 当前的智能体还有做到最好. 作为第一个建议, 你可以训练更多的时间步. 比如1000000步, 我们可以看到更好的结果!\n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard)中, 你将找到你的智能体的位置. 你想要获得第一吗?\n",
    "\n",
    "以下是一些实现这个目标的想法:\n",
    "\n",
    "* 训练更多的时间步\n",
    "* 通过查看你的同学们的模型尝试不同的超参数.\n",
    "* **发布你训练的新模型**到Hub上 🔥\n",
    "\n",
    "在冰上行走和开出租车对你来说太无聊了? **尝试其他环境**, 为什么不试试FrozenLake-v1打滑版呢? [使用gym文档](https://www.gymlibrary.ml/)查询它们如何工作. 玩得开心🎉."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c9c5d7",
   "metadata": {},
   "source": [
    "---\n",
    "祝贺你完成本章! 这才是最重要的, **这还有一些额外的信息.**\n",
    "\n",
    "如果你仍然对这些感到困惑...这是完全正常的! **这对我和所有学习强化学习的人都是一样的.**\n",
    "\n",
    "**在继续尝试其他挑战之前, 花一点时间真正的掌握这些内容.** 理解这些内容并打下基础是非常重要的.\n",
    "\n",
    "当然, 在后续课程中, 我们将会继续使用并再次解释这些内容, 但**最好是在开始下一章之前完全掌握这些.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86142de9",
   "metadata": {},
   "source": [
    "### 这是专门为你打造的课程 👷🏿‍♀️\n",
    "\n",
    "我们希望根据你的反馈提高和改进课程. 如果你有一些建议, 请打开GitHub仓库的issue: https://github.com/huggingface/deep-rl-class/issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aab7b0d",
   "metadata": {},
   "source": [
    "第3单元见! 🔥\n",
    "## 不断学习, 不断精彩!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
