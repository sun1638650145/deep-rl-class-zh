{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e129ae5b",
   "metadata": {},
   "source": [
    "# 第2单元: 使用FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 进行 Q-Learning\n",
    "在这份笔记中, **你将从头开始编写你的第一个强化学习智能体**使用Q-Learning来玩FrozenLake ❄️ 并将其分享到社区, 并记得实验不同的配置\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit2频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境: \n",
    "* [FrozenLake-v1](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "* [Taxi-v3](https://www.gymlibrary.ml/environments/toy_text/taxi/)\n",
    "\n",
    "📚 强化学习库: Python和NumPy\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b785d",
   "metadata": {},
   "source": [
    "![envs.gif](./assets/envs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a9749",
   "metadata": {},
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用环境库**Gym**.\n",
    "* 能够从头开始编写一个Q-Learning智能体.\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cca1b",
   "metadata": {},
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![深度强化学习课程.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b511db",
   "metadata": {},
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践**.\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ 在独特的环境中训练**智能体**.\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef533af",
   "metadata": {},
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第2单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit2/README.md)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第1部分**](https://huggingface.co/blog/deep-rl-q-part1)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第2部分**](https://huggingface.co/blog/deep-rl-q-part2)\n",
    "\n",
    "🔲 📢 注册我们的Discord服务器并在#introduce-yourself频道介绍自己 🥳\n",
    "\n",
    "🔲 🐕 你是Discord新手吗? 请查看我们的discord 101以获得最佳实践 👉 https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441b3cc",
   "metadata": {},
   "source": [
    "## 一个Q-Learning的小回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668d36e",
   "metadata": {},
   "source": [
    "* Q-Learning是一种**强化学习算法**\n",
    "    * 通过训练一个包含**动作(action)-价值(value)的Q-函数(Q-Function)**, Q-表(Q-Table)作为内部存储器, 它包含了**所有的状态-价值对.**\n",
    "    * 给定一个状态和动作, 我们的Q-函数将**搜索Q-表对应的值.**\n",
    "\n",
    "![Q-函数-2.jpg](./assets/Q-function-2.jpg)\n",
    "\n",
    "* 当训练完成时, **我们得到一个最优的Q-函数, 因此得到一个最优的Q-表.**\n",
    "* 如果我们有**最优的Q-函数**, 那我们就知道**对于每个状态最好的行动是什么**, 所以我们就有一个最优策略.\n",
    "\n",
    "![link-value-policy.jpg](./assets/link-value-policy.jpg)\n",
    "\n",
    "但是一开始, 我们的**Q-表是没用的, 因为它的每个状态-动作对都是任意值(大多数时候我们将Q-表初始化为0).** 当我们探索环境并更新我们的Q-表时, 它将给我们越来越好的近似值.\n",
    "\n",
    "![Q-learning-1.jpg](./assets/Q-learning-1.jpg)\n",
    "\n",
    "这是Q-Learning的伪代码:\n",
    "\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ed823d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
