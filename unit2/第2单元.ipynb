{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e129ae5b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第2单元: 使用FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 进行 Q-Learning\n",
    "在这份笔记中, **你将从头开始编写你的第一个强化学习智能体**使用Q-Learning来玩冰冻湖 ❄️ 并将其分享到社区, 请记得实验不同的配置\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit2频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境: \n",
    "* [FrozenLake-v1](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "* [Taxi-v3](https://www.gymlibrary.ml/environments/toy_text/taxi/)\n",
    "\n",
    "📚 强化学习库: Python和NumPy\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b785d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![envs.gif](./assets/envs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a9749",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用环境库**Gym**.\n",
    "* 能够从头开始编写一个Q-Learning智能体.\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cca1b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![深度强化学习课程.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b511db",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践**.\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ 在独特的环境中训练**智能体**.\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef533af",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第2单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit2/README.md)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第1部分**](https://huggingface.co/blog/deep-rl-q-part1)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第2部分**](https://huggingface.co/blog/deep-rl-q-part2)\n",
    "\n",
    "🔲 📢 注册我们的Discord服务器并在#introduce-yourself频道介绍自己 🥳\n",
    "\n",
    "🔲 🐕 你是Discord新手吗? 请查看我们的discord 101以获得最佳实践 👉 https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441b3cc",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一个Q-Learning的小回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668d36e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "* Q-Learning是一种**强化学习算法**\n",
    "    * 通过训练一个包含**动作(action)-价值(value)的Q-函数(Q-Function)**, Q-表(Q-Table)作为内部存储器, 它包含了**所有的状态-动作对.**\n",
    "    * 给定一个状态和动作, 我们的Q-函数将**搜索Q-表对应的值.**\n",
    "\n",
    "![Q-函数-2.jpg](./assets/Q-function-2.jpg)\n",
    "\n",
    "* 当训练完成时, **我们得到一个最优的Q-函数, 因此得到一个最优的Q-表.**\n",
    "* 如果我们有**最优的Q-函数**, 那我们就知道**对于每个状态最好的行动是什么**, 所以我们就有一个最优策略.\n",
    "\n",
    "![link-value-policy.jpg](./assets/link-value-policy.jpg)\n",
    "\n",
    "但是一开始, 我们的**Q-表是无效的, 因为它的每个状态-动作对都是任意值(大多数时候我们将Q-表初始化为0).** 当我们探索环境并更新我们的Q-表时, 它将给我们越来越好的近似值.\n",
    "\n",
    "![Q-learning-1.jpg](./assets/Q-learning-1.jpg)\n",
    "\n",
    "这是Q-Learning的伪代码:\n",
    "\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bef6",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第0步: 设置虚拟屏幕 🖥\n",
    "\n",
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境(记录视频帧).**\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fafa06",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg \n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1cad2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa149d3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "\n",
    "第一步是安装多个依赖项:\n",
    "\n",
    "* `gym`: 包含FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 环境.\n",
    "* `pygame`: 用于FrozenLake-v1和Taxi-v3的UI.\n",
    "* `numpy`: 用于处理我们的Q-表.\n",
    "\n",
    "Hugging Face Hub 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b047e48",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install gym==0.24.0 pygame\n",
    "!pip install huggingface_hub\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1550b30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 导入包 📦\n",
    "\n",
    "除了安装的库, 我们还使用:\n",
    "* `imageio`: 生成回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1b6ce",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61dbef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们现在已经准备好编写我们的Q-Learing算法了 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56942fdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 第1部分: 冰冻湖 ⛄️ (不打滑版)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea830f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第1步: 创建并理解[冰冻湖环境 ⛄️ ](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "💡 当你开始使用一个环境的好习惯是查看它的文档.\n",
    "\n",
    "👉 https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "我们将训练我们的Q-Learning智能体从起始状态(S)导航到目标状态(G), 方法是仅在冰面(F)上行走并避开冰窟窿(H).\n",
    "\n",
    "我们有两个尺寸的环境:\n",
    "* `map_name='4x4'`: 4x4地图版本.\n",
    "* `map_name='8x8'`: 8x8地图版本.\n",
    "\n",
    "环境有两种模式:\n",
    "* `is_slippery=False`: 由于冰冻湖不打滑的特性, 智能体总能沿预期方向移动.\n",
    "* `is_slippery=True`: 由于冰冻湖打滑(随机)的特性, 智能体可能不总沿预期方向移动."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e90d83",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "现在让我们使用4x4地图和不打滑的版本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73daa2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 创建FrozenLake-v1环境并使用4x4地图和不打滑的版本.\n",
    "env = gym.make()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703ea55",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0dc1a3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "env = gym.make(id='FrozenLake-v1',\n",
    "               map_name='4x4',\n",
    "               is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7e02c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "你也可以自定义地图来使用:\n",
    "```python\n",
    "desc=['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "但是我们现在将使用默认的环境."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01f278",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 让我们一起看看环境是什么样的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ce7cb",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 我们使用 gym.make('<环境的名称>') 创建环境.\n",
    "env.reset()\n",
    "print('_' * 5 + '可观察的环境' + '_' * 5)\n",
    "print()\n",
    "print('可观察的环境', env.observation_space)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8e0ef",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "我们通过`可观察的环境 Discrete(16)`看到, 可观察的环境是一个表示**智能体当前位置current_row * nrows + current_col(行和列均从0开始)的值.**\n",
    "\n",
    "例如, 在4x4的地图上目标位置可以被计算成: 3 * 4 + 3 = 15. 可能的环境值取决于地图的大小. **例如, 4x4的地图有16个可能的环境值.**\n",
    "\n",
    "对于当前, 初始值 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d48284",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9df15b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5)\n",
    "print()\n",
    "print('动作的总数', env.action_space.n)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f9e30",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "动作空间(智能体可用的动作集)是有4个可用动作的离散值 🎮:\n",
    "\n",
    "* 0: 向左走\n",
    "* 1: 向下走\n",
    "* 2: 向右走\n",
    "* 3: 向上走\n",
    "\n",
    "奖励函数 💰:\n",
    "\n",
    "* 到达目标: +1\n",
    "* 到达冰窟窿: 0\n",
    "* 到达冰面: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1231d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第2步: 创建并初始化Q-表 🗄\n",
    "(👀 伪代码的第1步)\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)\n",
    "是时候初始化我们的Q-表了! 要知道我们需要使用多少行(states)和列(actions), 我们需要知道可观察和动作空间. OpenAI Gym给我们提供了一个方法: `env.action_space.n`和`env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c4683",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_space = \n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = \n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feab293",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 让我们创建大小为(state_space, action_space)的Q-表, 并使用np.zeros将它们的值初始化为0.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = \n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8b4d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34ad24",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5ec6f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb424dd5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 让我们创建大小为(state_space, action_space)的Q-表, 并使用np.zeros将它们的值初始化为0.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros([state_space, action_space])\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca6b80",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d26a614",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第3步: 定义epsilon-greedy策略 🤖️\n",
    "epsilon-greedy是一个平衡探索/经验的训练策略.\n",
    "\n",
    "epsilon-greedy的思路是:\n",
    "* 当概率为$1-\\epsilon$: **我们使用经验**(也就是我们的智能体选择一个最高的状态-动作对值的动作).\n",
    "* 当概率为$\\epsilon$: **我们进行探索**(尝试随机动作).\n",
    "\n",
    "然后随着训练的进行, 我们逐渐**降低epsilon的值, 因为我们需要越来越少的探索和使用越来越多的经验.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a5b68d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![Q-learning-4.jpg](./assets/Q-learning-4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89fddf6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # 随机生成一个在0和1之前的数字.\n",
    "    random_num =\n",
    "    # 如果random_num > epsilon --> 使用经验.\n",
    "    if random_num > epsilon:\n",
    "        # 从给定状态中采取最大值的动作;\n",
    "        # 在这np.argmax()非常有用.\n",
    "        action =\n",
    "    # 否则 --> 探索\n",
    "    else:\n",
    "        action = # 进行随机动作.\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca932559",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd4ab83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def epsilon_greedy_policy(Qtable, state, epsilon):\n",
    "    # 随机生成一个在0和1之前的数字.\n",
    "    random_num = np.random.uniform(0, 1)\n",
    "    # 如果random_num > epsilon --> 使用经验.\n",
    "    if random_num > epsilon:\n",
    "        # 从给定状态中采取最大值的动作;\n",
    "        # 在这np.argmax()非常有用.\n",
    "        action = np.argmax(Qtable[state])\n",
    "    # 否则 --> 探索\n",
    "    else:\n",
    "        action = env.action_space.sample() # 进行随机动作.\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f710c0",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第4步: 定义贪心策略 🤖️\n",
    "请记住我们有两个策略, 因为Q-Learning是一种**off-policy**算法. 这意味着我们使用**不同的策略来执行动作和更新价值函数.**\n",
    "* epsilon-greedy策略(动作策略)\n",
    "* 贪心策略(更新策略)\n",
    "\n",
    "贪心策略也将是我们训练Q-learing智能体后最终的策略. 贪心策略用来从Q-表中选择一个动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e347f0f7",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "![off-on-4.jpg](./assets/off-on-4.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aaf2c5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # 经验: 从给定状态中采取最大值的动作.\n",
    "    action =\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9707be",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fa8a8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def greedy_policy(Qtable, state):\n",
    "    # 经验: 从给定状态中采取最大值的动作.\n",
    "    action = np.argmax(Qtable[state])\n",
    "\n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37db1bb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第5步: 定义超参数 ⚙️\n",
    "与探索相关的参数是一些最重要的超参数.\n",
    "* 我们需要确保我们的智能体**探索足够的状态空间**来学习一个好的值近似(value approximation), 为此我们需要逐渐降低epsilon.\n",
    "* 如果你epsilon减小得过快(太高的衰减率), **你将承担智能体卡住的风险**, 因为你的智能体没有探索足够的状态空间, 因此无法解决问题."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803d2a1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 训练参数.\n",
    "n_training_episodes = 10000  # 训练的总轮数.\n",
    "learning_rate = 0.7  # 学习率.\n",
    "\n",
    "# 评估参数.\n",
    "n_eval_episodes = 100  # 测试的总轮数.\n",
    "\n",
    "# 环境参数.\n",
    "env_id = 'FrozenLake-v1'  # 环境的名称.\n",
    "max_steps = 99  # 每轮的最大步数.\n",
    "gamma = 0.95  # 衰减系数.\n",
    "eval_seed = []  # 环境的评估种子.\n",
    "\n",
    "# 探索参数.\n",
    "epsilon = 1.0  # 探索率.\n",
    "max_epsilon = 1.0  # 初始探索率.\n",
    "min_epsilon = 0.05  # 最小探索率.\n",
    "decay_rate = 0.0005  # 指数探索衰减率."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62d589f",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第6步: 创建训练循环"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fc2cc59",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in range(n_training_episodes):\n",
    "        # 减小epsilon(因为我们需要越来越少的探索).\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # 重置环境.\n",
    "        state = env.reset()\n",
    "\n",
    "        # 循环.\n",
    "        for step in range(max_steps):\n",
    "            # 使用epsilon-greedy策略选择动作At.\n",
    "            action =\n",
    "\n",
    "            # 采取动作At并观察Rt+1和St+1,\n",
    "            # 采取动作(a)并观察结果状态(s')和奖罚(r).\n",
    "            new_state, reward, done, info =\n",
    "\n",
    "            # 更新Q(s, a) = Q(s, a) + lr[R(s, a) + gamma * max Q(s', a') - Q(s, a)]\n",
    "            Qtable[state][action] =\n",
    "\n",
    "            # 如果done=True, 退出循环.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # 更新状态.\n",
    "            state = new_state\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b60fb",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667c9cd3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable):\n",
    "    for episode in range(n_training_episodes):\n",
    "        # 减小epsilon(因为我们需要越来越少的探索).\n",
    "        epsilon = min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * episode)\n",
    "        # 重置环境.\n",
    "        state = env.reset()\n",
    "\n",
    "        # 循环.\n",
    "        for step in range(max_steps):\n",
    "            # 使用epsilon-greedy策略选择动作At.\n",
    "            action = epsilon_greedy_policy(Qtable, state, epsilon)\n",
    "\n",
    "            # 采取动作At并观察Rt+1和St+1,\n",
    "            # 采取动作(a)并观察结果状态(s')和奖罚(r).\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            # 更新Q(s, a) = Q(s, a) + lr[R(s, a) + gamma * max Q(s', a') - Q(s, a)]\n",
    "            Qtable[state][action] += learning_rate * (reward + gamma * np.max(Qtable[new_state]) - Qtable[state][action])\n",
    "\n",
    "            # 如果done=True, 退出循环.\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            # 更新状态.\n",
    "            state = new_state\n",
    "\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06172632",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第7步: 训练Q-Learning智能体 🏃"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2191ced",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake = train(n_training_episodes, min_epsilon, max_epsilon, decay_rate, env, max_steps, Qtable_frozenlake)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77aabf4",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### 第8步: 现在让我们看看Q-表是什么样子 👀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88e97a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Qtable_frozenlake"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeaf9a46",
   "metadata": {},
   "source": [
    "### 第9步: 定义评估函数 📝"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517015da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_agent(env, max_steps, n_eval_episodes, Q, seed):\n",
    "    \"\"\"用`n_eval_episodes`轮评估智能体, 并返回奖励的均值和标准差.\n",
    "\n",
    "    Args:\n",
    "        env: 评估环境.\n",
    "        max_steps: 每轮的最大步数.\n",
    "        n_eval_episodes: 测试的总轮数.\n",
    "        Q: Q-表.\n",
    "        seed: 评估随机种子数组(用于Taxi-v3).\n",
    "\n",
    "    Returns:\n",
    "        奖励的均值和标准差.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        if seed:\n",
    "            state = env.reset(seed=seed[episode])\n",
    "        else:\n",
    "            state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        for step in range(max_steps):\n",
    "            # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "            action = np.argmax(Q[state])\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b3f834b",
   "metadata": {},
   "source": [
    "### 第10步: 评估我们的Q-Learing智能体 📈\n",
    "* 正常情况你的奖励均值应该是1.0\n",
    "* 这相对容易, 因为状态空间确实非常小(16). 你可以尝试[使用打滑版替换.](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d84a269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 评估我们的智能体.\n",
    "mean_reward, std_reward = evaluate_agent(env, max_steps, n_eval_episodes, Qtable_frozenlake, eval_seed)\n",
    "print(f'平均奖励={mean_reward:.2f} +/- {std_reward:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "470c1d9f",
   "metadata": {},
   "source": [
    "### 第11步(不涉及核心内容, 可选): 发布我们训练好的模型到 Hub 上 🔥\n",
    "\n",
    "现在我们看到经过训练之后得到了很棒的结果, 我们可以通过一行代码发布我们训练的模型到hub🤗上.\n",
    "\n",
    "这有一个模型卡的例子:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b57b64b",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17605326",
   "metadata": {},
   "source": [
    "在底层, Hub使用基于git的仓库(即使你不知道什么是git也不用担心), 这意味着你可以在实验和提高你的智能体以后更新新版本的模型."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de30536",
   "metadata": {},
   "source": [
    "#### 请勿修改下面的代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4422442",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import datetime\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, HfFolder, Repository\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "457841e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_video(env, Qtable, out_directory, fps=1):\n",
    "    images = []  \n",
    "    done = False\n",
    "    state = env.reset(seed=random.randint(0, 500))\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "    \n",
    "    while not done:\n",
    "        # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "        action = np.argmax(Qtable[state])\n",
    "        state, reward, done, info = env.step(action) # 我们直接使用next_state = state来记录顺序(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "        \n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339969cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def push_to_hub(repo_id,\n",
    "                model,\n",
    "                env,\n",
    "                video_fps=1,\n",
    "                local_repo_path='hub',\n",
    "                commit_message='Push Q-Learning agent to Hub',\n",
    "                token=None):\n",
    "    _, repo_name = repo_id.split('/')\n",
    "\n",
    "    eval_env = env\n",
    "\n",
    "    # Step 1: Clone or create the repo\n",
    "    # Create the repo (or clone its content if it's nonempty)\n",
    "    api = HfApi()\n",
    "\n",
    "    repo_url = api.create_repo(repo_id=repo_id,\n",
    "                               token=token,\n",
    "                               private=False,\n",
    "                               exist_ok=True)\n",
    "\n",
    "    # Git pull\n",
    "    repo_local_path = Path(local_repo_path) / repo_name\n",
    "    repo = Repository(repo_local_path, clone_from=repo_url, use_auth_token=True)\n",
    "    repo.git_pull()\n",
    "\n",
    "    repo.lfs_track(['*.mp4'])\n",
    "\n",
    "    # Step 1: Save the model\n",
    "    if env.spec.kwargs.get('map_name'):\n",
    "        model['map_name'] = env.spec.kwargs.get('map_name')\n",
    "        if env.spec.kwargs.get('is_slippery', '') == False:\n",
    "            model['slippery'] = False\n",
    "\n",
    "    print(model)\n",
    "\n",
    "    # Pickle the model\n",
    "    with open(Path(repo_local_path) / 'q-learning.pkl', 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "\n",
    "    # Step 2: Evaluate the model and build JSON\n",
    "    mean_reward, std_reward = evaluate_agent(eval_env, model['max_steps'], model['n_eval_episodes'], model['qtable'],\n",
    "                                             model['eval_seed'])\n",
    "\n",
    "    # First get datetime\n",
    "    eval_datetime = datetime.datetime.now()\n",
    "    eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "    evaluate_data = {\n",
    "        'env_id': model['env_id'],\n",
    "        'mean_reward': mean_reward,\n",
    "        'n_eval_episodes': model['n_eval_episodes'],\n",
    "        'eval_datetime': eval_form_datetime,\n",
    "    }\n",
    "    # Write a JSON file\n",
    "    with open(Path(repo_local_path) / 'results.json', 'w') as outfile:\n",
    "        json.dump(evaluate_data, outfile)\n",
    "\n",
    "    # Step 3: Create the model card\n",
    "    # Env id\n",
    "    env_name = model['env_id']\n",
    "    if env.spec.kwargs.get('map_name'):\n",
    "        env_name += '-' + env.spec.kwargs.get('map_name')\n",
    "\n",
    "    if env.spec.kwargs.get('is_slippery', '') == False:\n",
    "        env_name += '-' + 'no_slippery'\n",
    "\n",
    "    metadata = {}\n",
    "    metadata['tags'] = [\n",
    "        env_name,\n",
    "        'q-learning',\n",
    "        'reinforcement-learning',\n",
    "        'custom-implementation'\n",
    "    ]\n",
    "\n",
    "    # Add metrics\n",
    "    eval = metadata_eval_result(model_pretty_name=repo_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_name,\n",
    "                                dataset_id=env_name)\n",
    "\n",
    "    # Merges both dictionaries\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    model_card = f'''\n",
    "      # **Q-Learning** Agent playing **{env_id}**\n",
    "      This is a trained model of a **Q-Learning** agent playing **{env_id}** .\n",
    "      '''\n",
    "\n",
    "    model_card += '''\n",
    "      ## Usage\n",
    "      ```python\n",
    "      '''\n",
    "\n",
    "    model_card += f'''model = load_from_hub(repo_id='{repo_id}', filename='q-learning.pkl')\n",
    "    \n",
    "      # Don't forget to check if you need to add additional attributes (is_slippery=False etc)\n",
    "      env = gym.make(model['env_id'])\n",
    "    \n",
    "      evaluate_agent(env, model['max_steps'], model['n_eval_episodes'], model['qtable'], model['eval_seed'])\n",
    "      '''\n",
    "\n",
    "    model_card += '''\n",
    "      ```\n",
    "      '''\n",
    "\n",
    "    readme_path = repo_local_path / 'README.md'\n",
    "    readme = ''\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # Save our metrics to Readme metadata\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "    # Step 4: Record a video\n",
    "    video_path = repo_local_path / 'replay.mp4'\n",
    "    record_video(env, model['qtable'], video_path, video_fps)\n",
    "\n",
    "    # Push everything to hub\n",
    "    print(f'Pushing repo {repo_name} to the Hugging Face Hub')\n",
    "    repo.push_to_hub(commit_message=commit_message)\n",
    "\n",
    "    print(f'Your model is pushed to the hub. You can view your model here: {repo_url}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
