{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e129ae5b",
   "metadata": {},
   "source": [
    "# 第2单元: 使用FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 进行 Q-Learning\n",
    "在这份笔记中, **你将从头开始编写你的第一个强化学习智能体**使用Q-Learning来玩冰冻湖 ❄️ 并将其分享到社区, 并记得实验不同的配置\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit2频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境: \n",
    "* [FrozenLake-v1](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "* [Taxi-v3](https://www.gymlibrary.ml/environments/toy_text/taxi/)\n",
    "\n",
    "📚 强化学习库: Python和NumPy\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例. ⬇️"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355b785d",
   "metadata": {},
   "source": [
    "![envs.gif](./assets/envs.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06a9749",
   "metadata": {},
   "source": [
    "## 这份笔记的目标🏆\n",
    "\n",
    "在这份笔记学习结束后, 你将:\n",
    "\n",
    "* 能够使用环境库**Gym**.\n",
    "* 能够从头开始编写一个Q-Learning智能体.\n",
    "* 能够通过精彩的回放和得分🔥**发布你训练的智能体到Hugging Face Hub**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1cca1b",
   "metadata": {},
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![深度强化学习课程.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b511db",
   "metadata": {},
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践**.\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ 在独特的环境中训练**智能体**.\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef533af",
   "metadata": {},
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第2单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit2/README.md)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第1部分**](https://huggingface.co/blog/deep-rl-q-part1)\n",
    "\n",
    "🔲 📚 [阅读**Q-Learning简介的第2部分**](https://huggingface.co/blog/deep-rl-q-part2)\n",
    "\n",
    "🔲 📢 注册我们的Discord服务器并在#introduce-yourself频道介绍自己 🥳\n",
    "\n",
    "🔲 🐕 你是Discord新手吗? 请查看我们的discord 101以获得最佳实践 👉 https://github.com/huggingface/deep-rl-class/blob/main/DISCORD.Md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d441b3cc",
   "metadata": {},
   "source": [
    "## 一个Q-Learning的小回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5668d36e",
   "metadata": {},
   "source": [
    "* Q-Learning是一种**强化学习算法**\n",
    "    * 通过训练一个包含**动作(action)-价值(value)的Q-函数(Q-Function)**, Q-表(Q-Table)作为内部存储器, 它包含了**所有的状态-价值对.**\n",
    "    * 给定一个状态和动作, 我们的Q-函数将**搜索Q-表对应的值.**\n",
    "\n",
    "![Q-函数-2.jpg](./assets/Q-function-2.jpg)\n",
    "\n",
    "* 当训练完成时, **我们得到一个最优的Q-函数, 因此得到一个最优的Q-表.**\n",
    "* 如果我们有**最优的Q-函数**, 那我们就知道**对于每个状态最好的行动是什么**, 所以我们就有一个最优策略.\n",
    "\n",
    "![link-value-policy.jpg](./assets/link-value-policy.jpg)\n",
    "\n",
    "但是一开始, 我们的**Q-表是无效的, 因为它的每个状态-动作对都是任意值(大多数时候我们将Q-表初始化为0).** 当我们探索环境并更新我们的Q-表时, 它将给我们越来越好的近似值.\n",
    "\n",
    "![Q-learning-1.jpg](./assets/Q-learning-1.jpg)\n",
    "\n",
    "这是Q-Learning的伪代码:\n",
    "\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df7bef6",
   "metadata": {},
   "source": [
    "### 第0步: 设置虚拟屏幕 🖥\n",
    "\n",
    "在笔记中, 我们需要生成一个回放视频. 因此在Colab(或你本地的jupyter)中, **我们需要一个虚拟屏幕能渲染环境(记录视频帧).**\n",
    "\n",
    "下面的单元格将安装虚拟屏幕库并创建和运行虚拟屏幕. 🖥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fafa06",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install gitlfs ffmpeg \n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f1cad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa149d3",
   "metadata": {},
   "source": [
    "### 第1步: 安装依赖项 🔽\n",
    "\n",
    "第一步是安装多个依赖项:\n",
    "\n",
    "* `gym`: 包含FrozenLake-v1 ⛄️ 和Taxi-v3 🚕 环境.\n",
    "* `pygame`: 用于FrozenLake-v1和Taxi-v3的UI.\n",
    "* `numpy`: 用于处理我们的Q-表.\n",
    "\n",
    "Hugging Face Hub 🤗 是一个任何人都可以分享和探索模型和数据集的地方. 它有版本控制, 评估, 可视化和其他功能, 可以允许你简单地与他人协作.\n",
    "\n",
    "你可以在这看到全部可用的深度强化学习模型. 👉 https://huggingface.co/models?other=q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b047e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.24.0 pygame\n",
    "!pip install huggingface_hub\n",
    "!pip install imageio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1550b30",
   "metadata": {},
   "source": [
    "### 第2步: 导入包 📦\n",
    "\n",
    "除了安装的库, 我们还使用:\n",
    "* `random`: 生成随机数(这对Epsilon-Greedy策略很有用).\n",
    "* `imageio`: 生成回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad1b6ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import pickle5 as pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db61dbef",
   "metadata": {},
   "source": [
    "我们现在已经准备好编写我们的Q-Learing算法了 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56942fdd",
   "metadata": {},
   "source": [
    "# 第1部分: 冰冻湖 ⛄️ (不打滑版)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea830f",
   "metadata": {},
   "source": [
    "### 第1步: 创建并理解[冰冻湖环境 ⛄️ ](https://www.gymlibrary.ml/environments/toy_text/frozen_lake/)\n",
    "---\n",
    "💡 当你开始使用一个环境的好习惯是查看它的文档.\n",
    "\n",
    "👉 https://www.gymlibrary.ml/environments/toy_text/frozen_lake/\n",
    "\n",
    "---\n",
    "我们将训练我们的Q-Learning智能体从起始状态(S)导航到目标状态(G), 方法是仅在冰面(F)上行走并避开冰窟窿(H).\n",
    "\n",
    "我们有两个尺寸的环境:\n",
    "* `map_name='4x4'`: 4x4地图版本.\n",
    "* `map_name='8x8'`: 8x8地图版本.\n",
    "\n",
    "环境有两种模式:\n",
    "* `is_slippery=False`: 由于冰冻湖不打滑的特性, 智能体总能沿预期方向移动.\n",
    "* `is_slippery=True`: 由于冰冻湖打滑(随机)的特性, 智能体可能不总沿预期方向移动."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51e90d83",
   "metadata": {},
   "source": [
    "现在让我们使用4x4地图和不打滑的版本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73daa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建FrozenLake-v1环境并使用4x4地图和不打滑的版本.\n",
    "env = gym.make()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1703ea55",
   "metadata": {},
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0dc1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(id='FrozenLake-v1',\n",
    "               map_name='4x4',\n",
    "               is_slippery=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f7e02c",
   "metadata": {},
   "source": [
    "你也可以自定义地图来使用:\n",
    "```python\n",
    "desc=['SFFF', 'FHFH', 'FFFH', 'HFFG']\n",
    "gym.make('FrozenLake-v1', desc=desc, is_slippery=True)\n",
    "```\n",
    "但是我们现在将使用默认的环境."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da01f278",
   "metadata": {},
   "source": [
    "### 让我们一起看看环境是什么样的:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c45ce7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 我们使用 gym.make('<环境的名称>') 创建环境.\n",
    "env.reset()\n",
    "print('_' * 5 + '可观察的环境' + '_' * 5)\n",
    "print()\n",
    "print('可观察的环境', env.observation_space)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe8e0ef",
   "metadata": {},
   "source": [
    "我们通过`可观察的环境 Discrete(16)`看到, 可观察的环境是一个表示**智能体当前位置current_row * nrows + current_col(行和列均从0开始)的值.**\n",
    "\n",
    "例如, 在4x4的地图上目标位置可以被计算成: 3 * 4 + 3 = 15. 可能的环境值取决于地图的大小. **例如, 4x4的地图有16个可能的环境值.**\n",
    "\n",
    "对于当前, 初始值 = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d48284",
   "metadata": {},
   "source": [
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9df15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5)\n",
    "print()\n",
    "print('动作的总数', env.action_space.n)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b58f9e30",
   "metadata": {},
   "source": [
    "动作空间(智能体可用的可能动作集)是有4个可用动作的离散值 🎮:\n",
    "\n",
    "* 0: 向左走\n",
    "* 1: 向下走\n",
    "* 2: 向右走\n",
    "* 3: 向上走\n",
    "\n",
    "奖励函数 💰:\n",
    "\n",
    "* 到达目标: +1\n",
    "* 到达冰窟窿: 0\n",
    "* 到达冰面: 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e1231d",
   "metadata": {},
   "source": [
    "### 第2步: 创建并初始化Q-表 🗄\n",
    "(👀 伪代码的第1步)\n",
    "![Q-learning-2.jpg](./assets/Q-learning-2.jpg)\n",
    "是时候初始化我们的Q-表了! 要知道我们需要使用多少行(states)和列(actions), 我们需要知道可观察和动作空间. OpenAI Gym给我们提供了一个方法: `env.action_space.n`和`env.observation_space.n`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c4683",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = \n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = \n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9feab293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们创建大小为(state_space, action_space)的Q-表, 并使用np.zeros将它们的值初始化为0.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = \n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aded8b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c34ad24",
   "metadata": {},
   "source": [
    "### 答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space = env.observation_space.n\n",
    "print('一共有', state_space, '个可能的状态')\n",
    "\n",
    "action_space = env.action_space.n\n",
    "print('一共有', action_space, '个可能的动作')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb424dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 让我们创建大小为(state_space, action_space)的Q-表, 并使用np.zeros将它们的值初始化为0.\n",
    "def initialize_q_table(state_space, action_space):\n",
    "    Qtable = np.zeros([state_space, action_space])\n",
    "    return Qtable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ca6b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "Qtable_frozenlake = initialize_q_table(state_space, action_space)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
