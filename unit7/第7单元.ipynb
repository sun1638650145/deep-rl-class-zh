{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e1b4fb1",
   "metadata": {},
   "source": [
    "# 第7单元: 通过PyBullet的机器人模拟Advantage Actor Critic(A2C) 🤖️\n",
    "\n",
    "在这份简短的笔记中, 你将学习如何将A2C和PyBullet结合使用. 并训练智能体行走. 更准确地说是训练蜘蛛(他们说是蚂蚁, 但我认为它是蜘蛛 😆) 🕸️\n",
    "\n",
    "❓如果你有任何问题, 请在discord的#study-group-unit7频道发帖 👉 https://discord.gg/aYka4Yhff9\n",
    "\n",
    "🎮 环境:\n",
    "\n",
    "* AntBulletEnv-v0 🕸️\n",
    "\n",
    "⬇️ 这是**你将在几分钟内实现的目标**的示例([原始视频下载链接](https://huggingface.co/ThomasSimonini/ppo-SpaceInvadersNoFrameskip-v4/resolve/main/replay.mp4)). ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d2326bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<video autoplay controls><source src='./assets/replay.mp4' type='video/mp4'></video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "<video autoplay controls><source src='./assets/replay.mp4' type='video/mp4'></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14283ef",
   "metadata": {},
   "source": [
    "💡 我们建议你使用Google Colab, 因为某些环境只适用于Ubuntu. Google Colab的免费版本很适合这个教程. 让我们开始吧! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2ea5e7",
   "metadata": {},
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![Deep Reinforcement Learning Course.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7da46458",
   "metadata": {},
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践.**\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ **在独特的环境中训练智能体.**\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f43bb81",
   "metadata": {},
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第7单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit7/README.md)\n",
    "\n",
    "🔲 📚 通过阅读章节**学习Advantage Actor Critic(A2C)** 👉 https://huggingface.co/blog/deep-rl-a2c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6334cc3f",
   "metadata": {},
   "source": [
    "### 第0步: 设置GPU 💪\n",
    "\n",
    "* 为了**更快的训练智能体, 我们将使用GPU,** 选择`修改 > 笔记本设置`\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a181139",
   "metadata": {},
   "source": [
    "* `硬件加速器 > GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d1ccfa",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb82093",
   "metadata": {},
   "source": [
    "### 安装依赖项 🔽\n",
    "第一步是安装多个依赖项:\n",
    "* `pybullet`: 包含AntBullet环境 🚶\n",
    "* `stable-baselines3`: 深度强化学习库.\n",
    "* `huggingface_sb3`: Stable-baselines3的插件, 用于从Hugging Face Hub 🤗 上下载或者上传模型.\n",
    "* `huggingface_hub`: 允许任何人使用Hugging Face Hub的仓库."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e681a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym==0.24.1\n",
    "!pip install pybullet\n",
    "!pip install stable-baselines3 tensorboard\n",
    "!pip install huggingface_sb3\n",
    "!pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69783f52",
   "metadata": {},
   "source": [
    "### 第2步: 导入包 📦"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be1cb46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import pybullet_envs\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f7b89",
   "metadata": {},
   "source": [
    "### 第3步: 创建AntBulletEnv-v0环境 🕸️\n",
    "#### 环境 🎮 \n",
    "在这个环境中, 智能体需要正确的使用它不同的关节才能正确的行走."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5181dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_id = 'AntBulletEnv-v0'\n",
    "# 创建环境.\n",
    "env = gym.make(env_id)\n",
    "\n",
    "# 获取状态空间和动作空间的大小.\n",
    "s_size = env.observation_space.shape[0]\n",
    "a_size = env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f04d3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '可观察的环境' + '_' * 5, end='\\n\\n')\n",
    "print('可观察的环境向量的形状', s_size)\n",
    "print('随机采样环境', env.observation_space.sample())  # 获得一个随机的可观察环境空间."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2a3fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('_' * 5 + '动作空间' + '_' * 5, end='\\n\\n')\n",
    "print('动作空间的形状', a_size)\n",
    "print('随机动作', env.action_space.sample())  # 获得一个随机的动作."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeab21c0",
   "metadata": {},
   "source": [
    "我们需要[对输入特征进行标准化](https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html). 为此, 存在一个修饰器(wrapper), 它将计算输入特征的运行平均值和标准差.\n",
    "\n",
    "我们还通过添加参数`norm_reward=True`来使用相同的修饰器(wrapper)标准化奖罚."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0414dfc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = make_vec_env(env_id, n_envs=4)\n",
    "\n",
    "# 添加这个装饰器来标准化可观察空间和奖罚.\n",
    "env = VecNormalize(env, norm_obs=True, norm_reward=False, clip_obs=10.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aaac63d",
   "metadata": {},
   "source": [
    "### 第4步: 创建A2C模型 🤖️\n",
    "在这种情况下, 因为我们有一个向量作为输入, 我们将使用一个多层感知机(Multi Layer Perception)作为策略.\n",
    "\n",
    "为了找到最优参数, 我查看了[Stable-Baselines3团队训练的官方智能体](https://huggingface.co/sb3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c3eb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C(policy='MlpPolicy',\n",
    "            env=env,\n",
    "            learning_rate=0.0096,\n",
    "            n_steps=8,\n",
    "            gamma=0.99,\n",
    "            gae_lambda=0.9,\n",
    "            ent_coef=0.0,\n",
    "            vf_coef=0.4,\n",
    "            max_grad_norm=0.5,\n",
    "            use_rms_prop=True,\n",
    "            use_sde=True,\n",
    "            normalize_advantage=False,\n",
    "            tensorboard_log='./tensorboard',\n",
    "            policy_kwargs=dict(log_std_init=2,\n",
    "                               ortho_init=False),\n",
    "            verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ed5d62",
   "metadata": {},
   "source": [
    "### 第5步: 训练A2C智能体 🏃\n",
    "* 让我们训练智能体 2,000,000 步, 不要忘记使用Colab的GPU. 这大概需要25~40分钟."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b286e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(2000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888dc146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 当保存智能体时同时保存模型和VecNormalize统计信息.\n",
    "model.save('a2c-AntBulletEnv-v0')\n",
    "env.save('vec_normalize.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2462a729",
   "metadata": {},
   "source": [
    "### 第6步: 评估智能体 📈\n",
    "* 现在我们的登月着陆器智能体已经训练好了, 我们需要**检查它的性能**.\n",
    "* Stable-Baselines3 提供了一个方法`evaluate_policy`来进行评估.\n",
    "* 现在这种情况下, 我们得到的平均奖励是`2371.90 +/- 16.50`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7f9ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "\n",
    "# 加载保存的统计信息.\n",
    "eval_env = DummyVecEnv([lambda: gym.make('AntBulletEnv-v0')])\n",
    "eval_env = VecNormalize.load('vec_normalize.pkl', eval_env)\n",
    "\n",
    "# 不要在测试时更新参数.\n",
    "eval_env.training = False\n",
    "# 测试时不需要对奖罚进行标准化.\n",
    "eval_env.norm_reward = False\n",
    "\n",
    "# 加载智能体.\n",
    "model = A2C.load('a2c-AntBulletEnv-v0')\n",
    "\n",
    "mean_reward, std_reward = evaluate_policy(model, env)\n",
    "\n",
    "print(f\"Mean reward = {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4d4bd87",
   "metadata": {},
   "source": [
    "### 第7步(不涉及核心内容, 可选): 发布我们训练好的模型到 Hub 上 🔥\n",
    "现在我们看到经过训练之后得到了很棒的结果, 我们可以通过一行代码发布我们训练的模型到hub🤗上.\n",
    "\n",
    "这有一个模型卡的例子:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbac5e",
   "metadata": {},
   "source": [
    "![ModelCard.png](./assets/ModelCard.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc573a56",
   "metadata": {},
   "source": [
    "在底层, Hub使用基于git的仓库(即使你不知道什么是git也不用担心), 这意味着你可以在实验和提高你的智能体以后更新新版本的模型."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae91986d",
   "metadata": {},
   "source": [
    "通过使用`package_to_hub`, **你可以评估, 记录回放视频, 生成智能体的模型卡并把它发布到hub.**\n",
    "\n",
    "看这边:\n",
    "\n",
    "* 你可以**展示我们的作品** 🔥\n",
    "* 你可以**可视化智能体的活动** 👀\n",
    "* 你可以**与社区分享其他人也可以使用的智能体** 💾\n",
    "* 你可以**访问排行榜🏆以查看你的智能体和你同学的智能体相比如何** 👉 https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82291129",
   "metadata": {},
   "source": [
    "为了能分享你的模型到社区, 有以下三个步骤需要做:\n",
    "\n",
    "1⃣️ (如果没有完成)创建一个Hugging Face账户 ➡ https://huggingface.co/join\n",
    "\n",
    "2⃣️ 登陆账户, 然后你需要保存一个Hugging Face的身份验证令牌(token).\n",
    "\n",
    "* 创建一个新的具有**写入规则**的令牌(https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f3978",
   "metadata": {},
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0aa0fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4436d1a0",
   "metadata": {},
   "source": [
    "如果你使用IDE, 也可在终端中使用以下命令:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501e73d",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310ef9bf",
   "metadata": {},
   "source": [
    "3⃣️ 我们现在准备好使用`package_to_hub()`发布我们训练的智能体到 🤗 Hub 🔥."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0308efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "package_to_hub(model=model,\n",
    "               model_name=f'a2c-{env_id}',\n",
    "               model_architecture='A2C',\n",
    "               env_id=env_id,\n",
    "               eval_env=eval_env,\n",
    "               repo_id=f'ThomasSimonini/a2c-{env_id}',  # 记得修改成你的用户名.\n",
    "               commit_message='Initial commit')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95e71e0",
   "metadata": {},
   "source": [
    "## 额外的挑战(可选) 🏆\n",
    "最好的学习方式就是**自己进行尝试**! 为什么不试试`HalfCheetahBulletEnv-v0`? \n",
    "\n",
    "在[排行榜](https://huggingface.co/spaces/ThomasSimonini/Lunar-Lander-Leaderboard)中, 你将找到你的智能体的位置. 你想要获得第一吗?\n",
    "\n",
    "以下是一些实现这个目标的想法:\n",
    "* 训练更多的时间步\n",
    "* 尝试不同的超参数. 你可以在 👉 https://huggingface.co/models?other=AntBulletEnv-v0 看到其他同学的超参数\n",
    "* **发布你训练的新模型**到Hub上 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bebba22",
   "metadata": {},
   "source": [
    "第8单元见! 🔥\n",
    "## 不断学习, 不断精彩 🤗 ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
