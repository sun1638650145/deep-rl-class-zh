{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd30585b",
   "metadata": {},
   "source": [
    "# ç¬¬8å•å…ƒ: ä½¿ç”¨PyTorchç¼–å†™Proximal Policy Optimization(PPO) ğŸ¤–ï¸\n",
    "\n",
    "åœ¨æœ¬å•å…ƒä¸­, ä½ å°†å­¦ä¹ **ä½¿ç”¨PyTorchä»å¤´å¼€å§‹ç¼–å†™ä½ çš„PPOæ™ºèƒ½ä½“.**\n",
    "\n",
    "ä¸ºäº†æµ‹è¯•é²æ£’æ€§, æˆ‘ä»¬å°†åœ¨2ä¸ªä¸åŒçš„ç»å…¸ç¯å¢ƒè¿›è¡Œè®­ç»ƒ:\n",
    "\n",
    "* [CartPole-v1](https://www.gymlibrary.ml/environments/classic_control/cart_pole/?highlight=cartpole)\n",
    "* [LunarLander-v2 ğŸš€](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)\n",
    "\n",
    "æˆ‘ä»¬é€šè¿‡æ·±å…¥äº†è§£PPOçš„å·¥ä½œåŸç†æ¥å®Œæˆè¯¾ç¨‹çš„åŸºç¡€éƒ¨åˆ†. åœ¨ç¬¬1å•å…ƒ, ä½ å­¦ä¹ äº†åœ¨LunarLander-v2ä¸Šè®­ç»ƒPPOæ™ºèƒ½ä½“. ä½†æ˜¯ç°åœ¨, ç¬¬8å•å…ƒ, ä½ å¯ä»¥ä»å¤´å¼€å§‹ç¼–å†™ä»£ç . è¿™çœŸæ˜¯å¤ªä¸å¯æ€è®®äº† ğŸ¤©.\n",
    "\n",
    "![cover.jpg](./assets/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085c67c",
   "metadata": {},
   "source": [
    "â¬‡ï¸ è¿™æ˜¯ä½ å°†åœ¨å‡ åˆ†é’Ÿå†…å®ç°çš„ç›®æ ‡çš„ç¤ºä¾‹([åŸå§‹è§†é¢‘1ä¸‹è½½é“¾æ¥](https://huggingface.co/sb3/ppo-CartPole-v1/resolve/main/replay.mp4), [åŸå§‹è§†é¢‘2ä¸‹è½½é“¾æ¥](https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4)). â¬‡ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<video autoplay controls><source src='./assets/replay1.mp4' type='video/mp4'></video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<video autoplay controls><source src='./assets/replay2.mp4' type='video/mp4'></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3924008",
   "metadata": {},
   "source": [
    "ğŸ’¡ æˆ‘ä»¬å»ºè®®ä½ ä½¿ç”¨Google Colab, å› ä¸ºæŸäº›ç¯å¢ƒåªé€‚ç”¨äºUbuntu. Google Colabçš„å…è´¹ç‰ˆæœ¬å¾ˆé€‚åˆè¿™ä¸ªæ•™ç¨‹. è®©æˆ‘ä»¬å¼€å§‹å§! ğŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f70cfd",
   "metadata": {},
   "source": [
    "## è¿™ä»½ç¬”è®°æ¥è‡ªæ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹\n",
    "![Deep Reinforcement Learning Course.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078086dd",
   "metadata": {},
   "source": [
    "åœ¨è¿™ä¸ªå…è´¹è¯¾ç¨‹ä¸­, ä½ å°†:\n",
    "\n",
    "* ğŸ“– ç ”ç©¶æ·±åº¦å¼ºåŒ–å­¦ä¹ çš„**ç†è®ºå’Œå®è·µ.**\n",
    "* ğŸ§‘â€ğŸ’» å­¦ä¹ **ä½¿ç”¨æµè¡Œçš„æ·±åº¦å¼ºåŒ–å­¦ä¹ åº“**, ä¾‹å¦‚Stable Baselines3, RL Baselines3 Zooå’ŒRLlib.\n",
    "* ğŸ¤–ï¸ **åœ¨ç‹¬ç‰¹çš„ç¯å¢ƒä¸­è®­ç»ƒæ™ºèƒ½ä½“.**\n",
    "\n",
    "è¿˜æœ‰æ›´å¤šçš„è¯¾ç¨‹ ğŸ“š å†…å®¹ ğŸ‘‰ https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "ä¿æŒè¿›åº¦çš„æœ€ä½³æ–¹å¼æ˜¯åŠ å…¥æˆ‘ä»¬çš„DiscordæœåŠ¡å™¨ä¸ç¤¾åŒºå’Œæˆ‘ä»¬è¿›è¡Œäº¤æµ. ğŸ‘‰ğŸ» https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04cee2",
   "metadata": {},
   "source": [
    "## å…ˆå†³æ¡ä»¶ ğŸ—\n",
    "\n",
    "åœ¨æ·±å…¥ç ”ç©¶ç¬”è®°ä¹‹å‰, ä½ éœ€è¦:\n",
    "\n",
    "ğŸ”² ğŸ“š [é˜…è¯»ç¬¬8å•å…ƒçš„README.](https://github.com/huggingface/deep-rl-class/blob/main/unit8/README.md)\n",
    "\n",
    "ğŸ”² ğŸ“š é€šè¿‡é˜…è¯»ç« èŠ‚**å­¦ä¹ Proximal Policy Optimization(PPO)** ğŸ‘‰ https://huggingface.co/blog/deep-rl-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb65e5",
   "metadata": {},
   "source": [
    "### ç¬¬0æ­¥: è®¾ç½®GPU ğŸ’ª\n",
    "\n",
    "* ä¸ºäº†**æ›´å¿«çš„è®­ç»ƒæ™ºèƒ½ä½“, æˆ‘ä»¬å°†ä½¿ç”¨GPU,** é€‰æ‹©`ä¿®æ”¹ > ç¬”è®°æœ¬è®¾ç½®`\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8488e0",
   "metadata": {},
   "source": [
    "* `ç¡¬ä»¶åŠ é€Ÿå™¨ > GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0664be3",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5d1cd",
   "metadata": {},
   "source": [
    "### ç¬¬1æ­¥: å®‰è£…ä¾èµ–é¡¹ ğŸ”½ å’Œ è™šæ‹Ÿå±å¹• ğŸ’»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a244064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install ffmpeg\n",
    "# å¦‚æœä½ ä½¿ç”¨IDE(ä¾‹å¦‚PyCharmæˆ–VS Code)å°†ä¸éœ€è¦è¿™äº›æ­¥éª¤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece15dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym box2d-py  # å¦‚æœä½¿ç”¨Apple M1 conda install box2d-py\n",
    "!pip install huggingface_hub\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºè™šæ‹Ÿå±å¹•.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce758ac0",
   "metadata": {},
   "source": [
    "### ç¬¬2æ­¥: è®©æˆ‘ä»¬ä½¿ç”¨Costa Huangçš„æ•™ç¨‹ä»å¤´å¼€å§‹ç¼–å†™PPO\n",
    "* å¯¹äºPPOçš„æ ¸å¿ƒå®ç°, æˆ‘ä»¬å°†ä½¿ç”¨ä¼˜ç§€çš„[Costa Huangçš„æ•™ç¨‹](https://costa.sh/).\n",
    "* é™¤æ­¤ä¹‹å¤–, æ›´æ·±å…¥çš„äº†è§£ä½ å¯ä»¥é˜…è¯»13ä¸ªæ ¸å¿ƒå®ç°ç»†èŠ‚: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "ğŸ‘‰ è§†é¢‘æ•™ç¨‹: https://youtu.be/MEt6rrxH8W4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" ' \n",
    "     + 'title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; '\n",
    "     + 'clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba4641",
   "metadata": {},
   "source": [
    "* æœ€å¥½çš„åŠæ³•æ˜¯å…ˆåœ¨ä¸‹é¢çš„å•å…ƒæ ¼ä¸­ç¼–å†™ä»£ç , è¿™æ ·å¦‚æœä½ çš„è¿›ç¨‹è¢«å…³é—­, ä¹Ÿä¸ä¼šä¸¢å¤±ä»£ç ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ä½ çš„ä»£ç :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480c4ff",
   "metadata": {},
   "source": [
    "### ç¬¬3æ­¥: æ·»åŠ Hugging Faceé›†æˆ ğŸ¤—\n",
    "* ä¸ºäº†å°†æˆ‘ä»¬çš„æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hub, æˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ª`package_to_hub`å‡½æ•°.\n",
    "* æ·»åŠ æˆ‘ä»¬çš„éœ€è¦å°†æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hubçš„ä¾èµ–é¡¹."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e446ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import imageio\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "from wasabi import Printer\n",
    "\n",
    "msg = Printer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f741",
   "metadata": {},
   "source": [
    "* åœ¨å‡½æ•°`parse_args()`ä¸­æ·»åŠ æ–°å‚æ•°æ¥å®šä¹‰æˆ‘ä»¬æƒ³è¦å‘å¸ƒæ¨¡å‹çš„`repo-id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de38bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ·»åŠ Hugging Face Hubå‚æ•°.\n",
    "parser.add_argument('--repo-id',\n",
    "                    type=str,\n",
    "                    default='ThomasSimonini/ppo-CartPole-v1',\n",
    "                    help='Hugging Face Hubä¸­æ¨¡å‹ä»“åº“çš„ID{ç”¨æˆ·å/ä»“åº“å}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8e7dc",
   "metadata": {},
   "source": [
    "* æ¥ä¸‹æ¥, æˆ‘ä»¬æ·»åŠ å°†æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hubæ‰€éœ€çš„æ–¹æ³•\n",
    "* è¿™äº›æ–¹æ³•æœ‰:\n",
    "    * `_evalutate_agent()`: è¯„ä¼°æ™ºèƒ½ä½“.\n",
    "    * `_generate_model_card()`: ä¸ºä½ çš„æ™ºèƒ½ä½“ç”Ÿæˆæ¨¡å‹å¡.\n",
    "    * `_record_video()`: å½•åˆ¶æ™ºèƒ½ä½“çš„å›æ”¾è§†é¢‘."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   hyperparameters,\n",
    "                   eval_env,\n",
    "                   video_fps=30,\n",
    "                   commit_message='å‘å¸ƒå¼ºåŒ–å­¦ä¹ æ¨¡å‹åˆ°Hugging Face Hub.',\n",
    "                   token=None,\n",
    "                   logs=None):\n",
    "    \"\"\"è¯„ä¼°, ç”Ÿæˆè§†é¢‘å¹¶å°†æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hub.\n",
    "\n",
    "    æ­¤å‡½æ•°å°†æ‰§è¡Œå®Œæ•´çš„æµæ°´çº¿:\n",
    "        * è¯„ä¼°æ¨¡å‹\n",
    "        * ç”Ÿæˆæ¨¡å‹å¡\n",
    "        * ç”Ÿæˆæ™ºèƒ½ä½“çš„å›æ”¾è§†é¢‘\n",
    "        * å°†å…¨éƒ¨å†…å®¹å‘å¸ƒåˆ°Hugging Face Hub\n",
    "\n",
    "    Args:\n",
    "        repo_id: Hugging Face Hubä¸­æ¨¡å‹ä»“åº“çš„ID\n",
    "        model: è®­ç»ƒçš„æ¨¡å‹.\n",
    "        hyperparameters: è®­ç»ƒæ¨¡å‹çš„è¶…å‚æ•°.\n",
    "        eval_env: ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„ç¯å¢ƒ.\n",
    "        video_fps: æ¸²æŸ“å›æ”¾è§†é¢‘çš„å¸§ç‡.\n",
    "        commit_message: æäº¤çš„ä¿¡æ¯.\n",
    "        token: å‘å¸ƒæ¨¡å‹çš„Hugging Faceä»¤ç‰Œ.\n",
    "        logs: ä½ è¦ä¸Šä¼ çš„TensorBoardæ—¥å¿—çš„æœ¬åœ°ç›®å½•.\n",
    "    \"\"\"\n",
    "    msg.info('è¿™ä¸ªå‡½æ•°å°†ä¿å­˜, è¯„ä¼°, ç”Ÿæˆæ™ºèƒ½ä½“å›æ”¾è§†é¢‘, åˆ›å»ºæ¨¡å‹å¡å¹¶å°†æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hub.'\n",
    "             'æœ€å¤šå¯èƒ½éœ€è¦1åˆ†é’Ÿ. è¿™æ˜¯ä¸€é¡¹æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ, å¦‚æœä½ é‡åˆ°BUG, è¯·æ‰“å¼€ä¸€ä¸ªissue.')\n",
    "\n",
    "    # ç¬¬1æ­¥: å…‹éš†æˆ–åˆ›å»ºä»“åº“.\n",
    "    repo_url = HfApi().create_repo(repo_id=repo_id,\n",
    "                                   token=token,\n",
    "                                   private=False,\n",
    "                                   exist_ok=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        tmp_dir = Path(tmp_dir)\n",
    "\n",
    "        # ç¬¬2æ­¥: ä¿å­˜æ¨¡å‹.\n",
    "        torch.save(model.state_dict(), tmp_dir / 'model.pt')\n",
    "\n",
    "        # ç¬¬3æ­¥: è¯„ä¼°æ¨¡å‹å¹¶æ„å»ºJSON.\n",
    "        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n",
    "\n",
    "        # é¦–å…ˆ, è·å–å½“å‰æ—¶é—´.\n",
    "        eval_datetime = datetime.datetime.now()\n",
    "        eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "        evaluate_data = {\n",
    "            'env_id': hyperparameters.env_id,\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'n_evaluation_episodes': 10,\n",
    "            'eval_datetime': eval_form_datetime\n",
    "        }\n",
    "\n",
    "        # å†™å…¥JSONæ–‡ä»¶.\n",
    "        with open(tmp_dir / 'hyperparameters.json', 'w') as outfile:\n",
    "            json.dump(evaluate_data, outfile)\n",
    "\n",
    "        # ç¬¬4æ­¥: å½•åˆ¶å›æ”¾è§†é¢‘.\n",
    "        video_path = tmp_dir / 'replay.mp4'\n",
    "        _record_video(eval_env, model, video_path, video_fps)\n",
    "\n",
    "        # ç¬¬5æ­¥: åˆ›å»ºæ¨¡å‹å¡.\n",
    "        generated_model_card, metadata = _generate_model_card('PPO',\n",
    "                                                              hyperparameters.env_id,\n",
    "                                                              mean_reward,\n",
    "                                                              std_reward,\n",
    "                                                              hyperparameters)\n",
    "        _save_model_card(tmp_dir, generated_model_card, metadata)\n",
    "\n",
    "        # ç¬¬6æ­¥: å¦‚æœéœ€è¦åˆ™æ·»åŠ æ—¥å¿—.\n",
    "        if logs:\n",
    "            _add_logdir(tmp_dir, Path(logs))\n",
    "\n",
    "        msg.info(f'æ­£åœ¨å°†ä»“åº“{repo_id}å‘å¸ƒåˆ°Hugging Face Hub...')\n",
    "\n",
    "        repo_url = upload_folder(repo_id=repo_id,\n",
    "                                 folder_path=tmp_dir,\n",
    "                                 path_in_repo='',\n",
    "                                 commit_message=commit_message,\n",
    "                                 token=token)\n",
    "\n",
    "        msg.info(f'ä½ çš„æ¨¡å‹å·²ç»å‘å¸ƒåˆ°Hugging Face Hub. ä½ å¯ä»¥ç‚¹å‡»é“¾æ¥æŸ¥çœ‹çš„ä½ çš„æ¨¡å‹: {repo_url}')\n",
    "\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def _evaluate_agent(env, n_eval_episodes, policy):\n",
    "    \"\"\"ç”¨`n_eval_episodes`è½®è¯„ä¼°æ™ºèƒ½ä½“, å¹¶è¿”å›å¥–åŠ±çš„å‡å€¼å’Œæ ‡å‡†å·®.\n",
    "\n",
    "    Args:\n",
    "        env: è¯„ä¼°ç¯å¢ƒ.\n",
    "        n_eval_episodes: æµ‹è¯•çš„æ€»è½®æ•°.\n",
    "        policy: å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“.\n",
    "\n",
    "    Returns:\n",
    "        å¥–åŠ±çš„å‡å€¼å’Œæ ‡å‡†å·®.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, done, info = env.step(action.numpy())\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "def _record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "\n",
    "    while not done:\n",
    "        state = torch.Tensor(state)\n",
    "        # åœ¨ç»™å®šçŠ¶æ€ä¸‹, é‡‡å–å…·æœ‰æœ€å¤§æœŸæœ›å¥–åŠ±çš„åŠ¨ä½œ(ç´¢å¼•).\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(action.numpy())  # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨next_state = stateæ¥è®°å½•é¡ºåº(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "\n",
    "\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "    \"\"\"ä¸ºHugging Face Hubç”Ÿæˆæ¨¡å‹å¡.\n",
    "\n",
    "    Args:\n",
    "        model_name: æ¨¡å‹çš„åç§°.\n",
    "        env_id: ç¯å¢ƒçš„åç§°.\n",
    "        mean_reward: å¥–ç½šçš„å‡å€¼.\n",
    "        std_reward: å¥–ç½šçš„æ ‡å‡†å·®.\n",
    "        hyperparameters: è®­ç»ƒæ¨¡å‹çš„è¶…å‚æ•°.\n",
    "    \"\"\"\n",
    "    # ç¬¬1æ­¥: é€‰æ‹©å…ƒæ•°æ®.\n",
    "    metadata = _generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "    # å°†è¶…å‚æ•°å‘½åç©ºé—´è½¬æ¢ä¸ºå­—ç¬¦ä¸².\n",
    "    converted_dict = vars(hyperparameters)\n",
    "    converted_str = str(converted_dict)\n",
    "    converted_str = converted_str.split(', ')\n",
    "    converted_str = '\\n'.join(converted_str)\n",
    "\n",
    "    # ç¬¬2æ­¥: ç”Ÿæˆæ¨¡å‹å¡.\n",
    "    model_card = f'''\n",
    "    # ä½¿ç”¨PPOæ™ºèƒ½ä½“æ¥ç© {env_id}\n",
    "    \n",
    "    è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨PPOè®­ç»ƒæœ‰ç´ çš„æ¨¡å‹ç© {env_id}.\n",
    "    è¦å­¦ä¹ ç¼–å†™ä½ è‡ªå·±çš„PPOæ™ºèƒ½ä½“å¹¶è®­ç»ƒå®ƒ, \n",
    "    è¯·æŸ¥é˜…æ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ç¬¬8å•å…ƒ: https://github.com/huggingface/deep-rl-class/tree/main/unit8\n",
    "    \n",
    "    # è¶…å‚æ•°\n",
    "    ```python\n",
    "    {converted_str}\n",
    "    ```\n",
    "    '''\n",
    "\n",
    "    return model_card, metadata\n",
    "\n",
    "\n",
    "def _generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "    \"\"\"å®šä¹‰æ¨¡å‹å¡çš„å…ƒæ•°æ®.\n",
    "\n",
    "    Args:\n",
    "        model_name: æ¨¡å‹çš„åç§°.\n",
    "        env_id: ç¯å¢ƒçš„åç§°.\n",
    "        mean_reward: å¥–ç½šçš„å‡å€¼.\n",
    "        std_reward: å¥–ç½šçš„æ ‡å‡†å·®.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'tag': [\n",
    "            env_id,\n",
    "            'ppo',\n",
    "            'deep-reinforcement-learning',\n",
    "            'reinforcement-learning',\n",
    "            'custom-implementation',\n",
    "            'deep-rl-class'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # æ·»åŠ è¯„ä¼°.\n",
    "    eval = metadata_eval_result(model_pretty_name=model_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_id,\n",
    "                                dataset_id=env_id)\n",
    "\n",
    "    # åˆå¹¶æ‰€æœ‰çš„å­—å…¸.\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"ä¿å­˜æ¨¡å‹å¡åˆ°ä»“åº“.\n",
    "\n",
    "    Args:\n",
    "        local_path: ä»“åº“çš„åœ°å€.\n",
    "        generated_model_card: é€šè¿‡`_generate_model_card()`ç”Ÿæˆçš„æ¨¡å‹å¡.\n",
    "        metadata: å…ƒæ•°æ®.\n",
    "    \"\"\"\n",
    "    readme_path = local_path / 'README.md'\n",
    "\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # ä¿å­˜æˆ‘ä»¬çš„è¯„ä¼°ä¿¡æ¯åˆ°READMEçš„å…ƒæ•°æ®.\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "\n",
    "def _add_logdir(local_path: Path,\n",
    "                logdir: Path):\n",
    "    \"\"\"æ·»åŠ æ—¥å¿—åˆ°ä»“åº“.\n",
    "\n",
    "    Args:\n",
    "        local_path: ä»“åº“çš„åœ°å€.\n",
    "        logdir: æ—¥å¿—çš„åœ°å€.\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # æ·»åŠ æ—¥å¿—åˆ°ä»“åº“ä¸‹, æ–°åœ°å€å«`logs`\n",
    "        repo_logdir = local_path / 'logs'\n",
    "\n",
    "        # å¦‚æœå½“å‰çš„æ—¥å¿—ç›®å½•å­˜åœ¨, å°±åˆ é™¤.\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # å¤åˆ¶æ—¥å¿—åˆ°ä»“åº“çš„æ—¥å¿—ä¸­.\n",
    "        shutil.copytree(logdir, repo_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018dca8d",
   "metadata": {},
   "source": [
    "* æœ€å, æˆ‘ä»¬åœ¨PPOè®­ç»ƒå®Œåè°ƒç”¨è¿™ä¸ªå‡½æ•°."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# åˆ›å»ºä¸€ä¸ªè¯„ä¼°ç¯å¢ƒ.\n",
    "eval_env = gym.make(args.env_id)\n",
    "\n",
    "package_to_hub(repo_id=args.repo_id,\n",
    "               model=agent,  # æˆ‘ä»¬æƒ³è¦ä¿å­˜çš„æ¨¡å‹.\n",
    "               hyperparameters=args,\n",
    "               eval_env=gym.make(args.env_id),\n",
    "               logs=f'runs/{runs_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39b3d4",
   "metadata": {},
   "source": [
    "* è¿™æ˜¯æœ€ç»ˆçš„`ppo.py`æ–‡ä»¶çš„æ ·å­."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"æ–‡æ¡£å’Œå®éªŒç»“æœå¯ä»¥åœ¨ https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy æ‰¾åˆ°.\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from huggingface_hub.hf_api import HfApi\n",
    "from huggingface_hub.hf_api import upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from wasabi import Printer\n",
    "\n",
    "msg = Printer()\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--exp-name', type=str, default=os.path.basename(__file__).rstrip('.py'), help='å®éªŒçš„åç§°')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='å®éªŒçš„éšæœºç§å­')\n",
    "    parser.add_argument('--torch-deterministic',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='å‡å°‘ç®—æ³•çš„éšæœºæ€§, å¦‚æœåˆ‡æ¢, `torch.backends.cudnn.deterministic=False`')\n",
    "    parser.add_argument('--cuda',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='é»˜è®¤æƒ…å†µä¸‹å°†å¯ç”¨CUDA')\n",
    "    parser.add_argument('--track',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=False,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='è¯¥å®éªŒå°†å¯¹æƒé‡å’Œåå·®è¿›è¡Œè¿½è¸ª')\n",
    "    parser.add_argument('--wandb-project-name', type=str, default='cleanRL', help='wanDbé¡¹ç›®çš„åç§°')\n",
    "    parser.add_argument('--wandb-entity', type=str, default=None, help='wanDbé¡¹ç›®çš„å®ä½“')\n",
    "    parser.add_argument('--capture-video',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=False,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='æ˜¯å¦ä¿å­˜æ™ºèƒ½ä½“çš„å›æ”¾è§†é¢‘(æŸ¥çœ‹`videos`æ–‡ä»¶å¤¹)')\n",
    "\n",
    "    # ç®—æ³•å‚æ•°.\n",
    "    parser.add_argument('--env-id', type=str, default='CartPole-v1', help='ç¯å¢ƒçš„åç§°')\n",
    "    parser.add_argument('--total-timesteps', type=int, default=50000, help='å®éªŒçš„æ€»æ—¶é—´æ­¥')\n",
    "    parser.add_argument('--learning-rate', type=float, default=2.5e-4, help='ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡')\n",
    "    parser.add_argument('--num-envs', type=int, default=4, help='å¹¶è¡Œçš„ç¯å¢ƒæ•°é‡')\n",
    "    parser.add_argument('--num-steps', type=int, default=128, help='æ¯ä¸ªç¯å¢ƒä¸­ç­–ç•¥çš„æ¯è½®æœ€å¤§æ­¥æ•°')\n",
    "    parser.add_argument('--anneal-lr',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='ç­–ç•¥å’Œä»·å€¼ç½‘ç»œçš„å­¦ä¹ ç‡é€€ç«')\n",
    "    parser.add_argument('--gae',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡å™¨è¿›è¡Œä¼˜åŠ¿è®¡ç®—')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='æŠ˜æ‰£ç³»æ•°')\n",
    "    parser.add_argument('--gae-lambda', type=float, default=0.95, help='å¹¿ä¹‰ä¼˜åŠ¿ä¼°è®¡å™¨çš„åå·®ä¸æ–¹å·®æƒè¡¡å› å­')\n",
    "    parser.add_argument('--num-minibatches', type=int, default=4, help='æ‰¹æ¬¡çš„å¤§å°')\n",
    "    parser.add_argument('--update-epochs', type=int, default=4, help='æ›´æ–°ç­–ç•¥çš„Kä¸ªå‘¨æœŸ')\n",
    "    parser.add_argument('--norm-adv',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='ä½¿ç”¨å¹¿ä¹‰ä¼˜åŠ¿')\n",
    "    parser.add_argument('--clip-coef', type=float, default=0.2, help='ä»£ç†è£åˆ‡ç³»æ•°')\n",
    "    parser.add_argument('--clip-vloss',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='æ ¹æ®è®ºæ–‡, æ˜¯å¦å¯¹ä»·å€¼å‡½æ•°ä½¿ç”¨è£åˆ‡æŸå¤±')\n",
    "    parser.add_argument('--ent-coef', type=float, default=0.01, help='ç†µç³»æ•°')\n",
    "    parser.add_argument('--vf-coef', type=float, default=0.5, help='ä»·å€¼ç³»æ•°')\n",
    "    parser.add_argument('--max-grad-norm', type=float, default=0.5, help='æ¢¯åº¦è£åˆ‡çš„æœ€å¤§èŒƒæ•°')\n",
    "    parser.add_argument('--target-kl', type=float, default=None, help='ç›®æ ‡KLæ•£åº¦é˜ˆå€¼')\n",
    "\n",
    "    # æ·»åŠ Hugging Faceå‚æ•°.\n",
    "    parser.add_argument('--repo-id',\n",
    "                        type=str,\n",
    "                        default='ThomasSimonini/ppo-CartPole-v1',\n",
    "                        help='Hugging Face Hubä¸­æ¨¡å‹ä»“åº“çš„ID{ç”¨æˆ·å/ä»“åº“å}')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   hyperparameters,\n",
    "                   eval_env,\n",
    "                   video_fps=30,\n",
    "                   commit_message='å‘å¸ƒå¼ºåŒ–å­¦ä¹ æ¨¡å‹åˆ°Hugging Face Hub.',\n",
    "                   token=None,\n",
    "                   logs=None):\n",
    "    \"\"\"è¯„ä¼°, ç”Ÿæˆè§†é¢‘å¹¶å°†æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hub.\n",
    "\n",
    "    æ­¤å‡½æ•°å°†æ‰§è¡Œå®Œæ•´çš„æµæ°´çº¿:\n",
    "        * è¯„ä¼°æ¨¡å‹\n",
    "        * ç”Ÿæˆæ¨¡å‹å¡\n",
    "        * ç”Ÿæˆæ™ºèƒ½ä½“çš„å›æ”¾è§†é¢‘\n",
    "        * å°†å…¨éƒ¨å†…å®¹å‘å¸ƒåˆ°Hugging Face Hub\n",
    "\n",
    "    Args:\n",
    "        repo_id: Hugging Face Hubä¸­æ¨¡å‹ä»“åº“çš„ID\n",
    "        model: è®­ç»ƒçš„æ¨¡å‹.\n",
    "        hyperparameters: è®­ç»ƒæ¨¡å‹çš„è¶…å‚æ•°.\n",
    "        eval_env: ç”¨äºè¯„ä¼°æ™ºèƒ½ä½“çš„ç¯å¢ƒ.\n",
    "        video_fps: æ¸²æŸ“å›æ”¾è§†é¢‘çš„å¸§ç‡.\n",
    "        commit_message: æäº¤çš„ä¿¡æ¯.\n",
    "        token: å‘å¸ƒæ¨¡å‹çš„Hugging Faceä»¤ç‰Œ.\n",
    "        logs: ä½ è¦ä¸Šä¼ çš„TensorBoardæ—¥å¿—çš„æœ¬åœ°ç›®å½•.\n",
    "    \"\"\"\n",
    "    msg.info('è¿™ä¸ªå‡½æ•°å°†ä¿å­˜, è¯„ä¼°, ç”Ÿæˆæ™ºèƒ½ä½“å›æ”¾è§†é¢‘, åˆ›å»ºæ¨¡å‹å¡å¹¶å°†æ¨¡å‹å‘å¸ƒåˆ°Hugging Face Hub.'\n",
    "             'æœ€å¤šå¯èƒ½éœ€è¦1åˆ†é’Ÿ. è¿™æ˜¯ä¸€é¡¹æ­£åœ¨è¿›è¡Œçš„å·¥ä½œ, å¦‚æœä½ é‡åˆ°BUG, è¯·æ‰“å¼€ä¸€ä¸ªissue.')\n",
    "\n",
    "    # ç¬¬1æ­¥: å…‹éš†æˆ–åˆ›å»ºä»“åº“.\n",
    "    repo_url = HfApi().create_repo(repo_id=repo_id,\n",
    "                                   token=token,\n",
    "                                   private=False,\n",
    "                                   exist_ok=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        tmp_dir = Path(tmp_dir)\n",
    "\n",
    "        # ç¬¬2æ­¥: ä¿å­˜æ¨¡å‹.\n",
    "        torch.save(model.state_dict(), tmp_dir / 'model.pt')\n",
    "\n",
    "        # ç¬¬3æ­¥: è¯„ä¼°æ¨¡å‹å¹¶æ„å»ºJSON.\n",
    "        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n",
    "\n",
    "        # é¦–å…ˆ, è·å–å½“å‰æ—¶é—´.\n",
    "        eval_datetime = datetime.datetime.now()\n",
    "        eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "        evaluate_data = {\n",
    "            'env_id': hyperparameters.env_id,\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'n_evaluation_episodes': 10,\n",
    "            'eval_datetime': eval_form_datetime\n",
    "        }\n",
    "\n",
    "        # å†™å…¥JSONæ–‡ä»¶.\n",
    "        with open(tmp_dir / 'hyperparameters.json', 'w') as outfile:\n",
    "            json.dump(evaluate_data, outfile)\n",
    "\n",
    "        # ç¬¬4æ­¥: å½•åˆ¶å›æ”¾è§†é¢‘.\n",
    "        video_path = tmp_dir / 'replay.mp4'\n",
    "        _record_video(eval_env, model, video_path, video_fps)\n",
    "\n",
    "        # ç¬¬5æ­¥: åˆ›å»ºæ¨¡å‹å¡.\n",
    "        generated_model_card, metadata = _generate_model_card('PPO',\n",
    "                                                              hyperparameters.env_id,\n",
    "                                                              mean_reward,\n",
    "                                                              std_reward,\n",
    "                                                              hyperparameters)\n",
    "        _save_model_card(tmp_dir, generated_model_card, metadata)\n",
    "\n",
    "        # ç¬¬6æ­¥: å¦‚æœéœ€è¦åˆ™æ·»åŠ æ—¥å¿—.\n",
    "        if logs:\n",
    "            _add_logdir(tmp_dir, Path(logs))\n",
    "\n",
    "        msg.info(f'æ­£åœ¨å°†ä»“åº“{repo_id}å‘å¸ƒåˆ°Hugging Face Hub...')\n",
    "\n",
    "        repo_url = upload_folder(repo_id=repo_id,\n",
    "                                 folder_path=tmp_dir,\n",
    "                                 path_in_repo='',\n",
    "                                 commit_message=commit_message,\n",
    "                                 token=token)\n",
    "\n",
    "        msg.info(f'ä½ çš„æ¨¡å‹å·²ç»å‘å¸ƒåˆ°Hugging Face Hub. ä½ å¯ä»¥ç‚¹å‡»é“¾æ¥æŸ¥çœ‹çš„ä½ çš„æ¨¡å‹: {repo_url}')\n",
    "\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def _evaluate_agent(env, n_eval_episodes, policy):\n",
    "    \"\"\"ç”¨`n_eval_episodes`è½®è¯„ä¼°æ™ºèƒ½ä½“, å¹¶è¿”å›å¥–åŠ±çš„å‡å€¼å’Œæ ‡å‡†å·®.\n",
    "\n",
    "    Args:\n",
    "        env: è¯„ä¼°ç¯å¢ƒ.\n",
    "        n_eval_episodes: æµ‹è¯•çš„æ€»è½®æ•°.\n",
    "        policy: å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“.\n",
    "\n",
    "    Returns:\n",
    "        å¥–åŠ±çš„å‡å€¼å’Œæ ‡å‡†å·®.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, done, info = env.step(action.numpy())\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "def _record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "\n",
    "    while not done:\n",
    "        state = torch.Tensor(state)\n",
    "        # åœ¨ç»™å®šçŠ¶æ€ä¸‹, é‡‡å–å…·æœ‰æœ€å¤§æœŸæœ›å¥–åŠ±çš„åŠ¨ä½œ(ç´¢å¼•).\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(action.numpy())  # æˆ‘ä»¬ç›´æ¥ä½¿ç”¨next_state = stateæ¥è®°å½•é¡ºåº(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "\n",
    "\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "    \"\"\"ä¸ºHugging Face Hubç”Ÿæˆæ¨¡å‹å¡.\n",
    "\n",
    "    Args:\n",
    "        model_name: æ¨¡å‹çš„åç§°.\n",
    "        env_id: ç¯å¢ƒçš„åç§°.\n",
    "        mean_reward: å¥–ç½šçš„å‡å€¼.\n",
    "        std_reward: å¥–ç½šçš„æ ‡å‡†å·®.\n",
    "        hyperparameters: è®­ç»ƒæ¨¡å‹çš„è¶…å‚æ•°.\n",
    "    \"\"\"\n",
    "    # ç¬¬1æ­¥: é€‰æ‹©å…ƒæ•°æ®.\n",
    "    metadata = _generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "    # å°†è¶…å‚æ•°å‘½åç©ºé—´è½¬æ¢ä¸ºå­—ç¬¦ä¸².\n",
    "    converted_dict = vars(hyperparameters)\n",
    "    converted_str = str(converted_dict)\n",
    "    converted_str = converted_str.split(', ')\n",
    "    converted_str = '\\n'.join(converted_str)\n",
    "\n",
    "    # ç¬¬2æ­¥: ç”Ÿæˆæ¨¡å‹å¡.\n",
    "    model_card = f'''\n",
    "    # ä½¿ç”¨PPOæ™ºèƒ½ä½“æ¥ç© {env_id}\n",
    "    \n",
    "    è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨PPOè®­ç»ƒæœ‰ç´ çš„æ¨¡å‹ç© {env_id}.\n",
    "    è¦å­¦ä¹ ç¼–å†™ä½ è‡ªå·±çš„PPOæ™ºèƒ½ä½“å¹¶è®­ç»ƒå®ƒ, \n",
    "    è¯·æŸ¥é˜…æ·±åº¦å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ç¬¬8å•å…ƒ: https://github.com/huggingface/deep-rl-class/tree/main/unit8\n",
    "    \n",
    "    # è¶…å‚æ•°\n",
    "    ```python\n",
    "    {converted_str}\n",
    "    ```\n",
    "    '''\n",
    "\n",
    "    return model_card, metadata\n",
    "\n",
    "\n",
    "def _generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "    \"\"\"å®šä¹‰æ¨¡å‹å¡çš„å…ƒæ•°æ®.\n",
    "\n",
    "    Args:\n",
    "        model_name: æ¨¡å‹çš„åç§°.\n",
    "        env_id: ç¯å¢ƒçš„åç§°.\n",
    "        mean_reward: å¥–ç½šçš„å‡å€¼.\n",
    "        std_reward: å¥–ç½šçš„æ ‡å‡†å·®.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'tag': [\n",
    "            env_id,\n",
    "            'ppo',\n",
    "            'deep-reinforcement-learning',\n",
    "            'reinforcement-learning',\n",
    "            'custom-implementation',\n",
    "            'deep-rl-class'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # æ·»åŠ è¯„ä¼°.\n",
    "    eval = metadata_eval_result(model_pretty_name=model_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_id,\n",
    "                                dataset_id=env_id)\n",
    "\n",
    "    # åˆå¹¶æ‰€æœ‰çš„å­—å…¸.\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"ä¿å­˜æ¨¡å‹å¡åˆ°ä»“åº“.\n",
    "\n",
    "    Args:\n",
    "        local_path: ä»“åº“çš„åœ°å€.\n",
    "        generated_model_card: é€šè¿‡`_generate_model_card()`ç”Ÿæˆçš„æ¨¡å‹å¡.\n",
    "        metadata: å…ƒæ•°æ®.\n",
    "    \"\"\"\n",
    "    readme_path = local_path / 'README.md'\n",
    "\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # ä¿å­˜æˆ‘ä»¬çš„è¯„ä¼°ä¿¡æ¯åˆ°READMEçš„å…ƒæ•°æ®.\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "\n",
    "def _add_logdir(local_path: Path,\n",
    "                logdir: Path):\n",
    "    \"\"\"æ·»åŠ æ—¥å¿—åˆ°ä»“åº“.\n",
    "\n",
    "    Args:\n",
    "        local_path: ä»“åº“çš„åœ°å€.\n",
    "        logdir: æ—¥å¿—çš„åœ°å€.\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # æ·»åŠ æ—¥å¿—åˆ°ä»“åº“ä¸‹, æ–°åœ°å€å«`logs`\n",
    "        repo_logdir = local_path / 'logs'\n",
    "\n",
    "        # å¦‚æœå½“å‰çš„æ—¥å¿—ç›®å½•å­˜åœ¨, å°±åˆ é™¤.\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # å¤åˆ¶æ—¥å¿—åˆ°ä»“åº“çš„æ—¥å¿—ä¸­.\n",
    "        shutil.copytree(logdir, repo_logdir)\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f'videos/{run_name}')\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.asarray(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.asarray(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01)\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    run_name = f'{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}'\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(config=vars(args),\n",
    "                   project=args.wandb_project_name,\n",
    "                   entity=args.wandb_entity,\n",
    "                   name=run_name,\n",
    "                   sync_tensorboard=True,\n",
    "                   monitor_gym=True,\n",
    "                   save_code=True)\n",
    "    writer = SummaryWriter(f'runs/{run_name}')\n",
    "    writer.add_text('hyperparameters',\n",
    "                    '|params|value|\\n|-|-|\\n%s' % ('\\n'.join([f'|{key}|{value}' for key, value in vars(args).items()])))\n",
    "\n",
    "    # å°½é‡ä¸è¦ä¿®æ”¹: éšæœºç§å­\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')\n",
    "\n",
    "    # ç¯å¢ƒè®¾ç½®.\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(env_id=args.env_id,\n",
    "                                              seed=args.seed + i,\n",
    "                                              idx=i,\n",
    "                                              capture_video=args.capture_video,\n",
    "                                              run_name=run_name) for i in range(args.num_envs)])\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), 'ä»…æ”¯æŒç¦»æ•£åŠ¨ä½œç©ºé—´'\n",
    "\n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    # ç®—æ³•é€»è¾‘: å­˜å‚¨è®¾ç½®.\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # è¯·ä¸è¦ä¿®æ”¹: å¼€å§‹æ¸¸æˆ.\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.Tensor(args.num_envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # æ˜¯å¦å¯¹å­¦ä¹ é€Ÿç‡è¿›è¡Œé€€ç«.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0]['lr'] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += 1 * args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # ç®—æ³•é€»è¾‘: åŠ¨ä½œé€»è¾‘.\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # è¯·ä¸è¦ä¿®æ”¹: æ‰§è¡Œæ¸¸æˆå¹¶ä¿å­˜æ—¥å¿—.\n",
    "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "            for item in info:\n",
    "                if 'episode' in item.keys():\n",
    "                    print(f\"æ€»æ—¶é—´æ­¥={global_step}, å½“å‰è¿”å›={item['episode']['r']}\")\n",
    "                    writer.add_scalar('charts/episodic_return', item['episode']['r'], global_step)\n",
    "                    writer.add_scalar('charts/episodic_return', item['episode']['r'], global_step)\n",
    "                    break\n",
    "\n",
    "        # å¦‚æœå½“å‰è½®æ²¡æœ‰ç»“æŸ, åˆ™å¼•å¯¼å€¼.\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            if args.gae:\n",
    "                advantages = torch.zeros_like(rewards).to(device)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(args.num_steps)):\n",
    "                    if t == args.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        nextvalues = values[t + 1]\n",
    "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "            else:\n",
    "                returns = torch.zeros_like(rewards).to(device)\n",
    "                for t in reversed(range(args.num_steps)):\n",
    "                    if t == args.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        next_return = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        next_return = values[t + 1]\n",
    "                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
    "                advantages = returns - values\n",
    "\n",
    "        # å±•å¹³æ‰¹æ¬¡.\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # ä¼˜åŒ–ç­–ç•¥å’Œä»·å€¼ç½‘ç»œ.\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # è®¡ç®—è¿‘ä¼¼KLæ•£åº¦ http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # ç­–ç•¥æŸå¤±.\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # ä»·å€¼æŸå¤±.\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None:\n",
    "                if approx_kl > args.target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # è¯·ä¸è¦ä¿®æ”¹: ä¸ºç»˜å›¾è®°å½•å¥–åŠ±.\n",
    "        writer.add_scalar('charts/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "        writer.add_scalar('losses/value_loss', v_loss.item(), global_step)\n",
    "        writer.add_scalar('losses/policy_loss', pg_loss.item(), global_step)\n",
    "        writer.add_scalar('losses/entropy', entropy_loss.item(), global_step)\n",
    "        writer.add_scalar('losses/old_approx_kl', old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar('losses/approx_kl', approx_kl.item(), global_step)\n",
    "        writer.add_scalar('losses/clipfrac', np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar('losses/explained_variance', explained_var, global_step)\n",
    "        print('SPS:', int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar('charts/SPS', int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "    # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ.\n",
    "    eval_env = gym.make(args.env_id)\n",
    "\n",
    "    package_to_hub(repo_id=args.repo_id,\n",
    "                   model=agent,  # æˆ‘ä»¬æƒ³ä¿å­˜çš„æ¨¡å‹.\n",
    "                   hyperparameters=args,\n",
    "                   eval_env=eval_env,\n",
    "                   logs=f'runs/{run_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340523fa",
   "metadata": {},
   "source": [
    "ä¸ºäº†èƒ½åˆ†äº«ä½ çš„æ¨¡å‹åˆ°ç¤¾åŒº, æœ‰ä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤éœ€è¦åš:\n",
    "\n",
    "1âƒ£ï¸ (å¦‚æœæ²¡æœ‰å®Œæˆ)åˆ›å»ºä¸€ä¸ªHugging Faceè´¦æˆ· â¡ https://huggingface.co/join\n",
    "\n",
    "2âƒ£ï¸ ç™»é™†è´¦æˆ·, ç„¶åä½ éœ€è¦ä¿å­˜ä¸€ä¸ªHugging Faceçš„èº«ä»½éªŒè¯ä»¤ç‰Œ(token).\n",
    "\n",
    "* åˆ›å»ºä¸€ä¸ªæ–°çš„å…·æœ‰**å†™å…¥è§„åˆ™**çš„ä»¤ç‰Œ(https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862bde5",
   "metadata": {},
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243415d",
   "metadata": {},
   "source": [
    "å¦‚æœä½ ä½¿ç”¨IDE, ä¹Ÿå¯åœ¨ç»ˆç«¯ä¸­ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e6c8f",
   "metadata": {},
   "source": [
    "### ç¬¬4æ­¥: è®©æˆ‘ä»¬å¼€å§‹è®­ç»ƒ ğŸ”¥\n",
    "* ç°åœ¨ä½ å·²ç»å®Œæˆä»å¤´å¼€å§‹ç¼–å†™PPOç®—æ³•å¹¶æ·»åŠ åˆ°Hugging Face Hub, æˆ‘ä»¬å¯ä»¥å¼€å§‹è®­ç»ƒäº† ğŸ”¥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823abf1",
   "metadata": {},
   "source": [
    "* é¦–å…ˆ, ä½ éœ€è¦å¤åˆ¶ä½ çš„ä»£ç åˆ°ä½ åˆ›å»ºçš„åä¸º`ppo.py`çš„æ–‡ä»¶ä¸­."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ef29c",
   "metadata": {},
   "source": [
    "![image.png](./assets/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50244092",
   "metadata": {},
   "source": [
    "![image.png](./assets/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb71827",
   "metadata": {},
   "source": [
    "* ç°åœ¨æˆ‘ä»¬åªéœ€è¦ä½¿ç”¨`python <pythonè„šæœ¬çš„åç§°>.py`å’Œæˆ‘ä»¬ç”¨`argparse`å®šä¹‰çš„å‚æ•°æ¥è¿è¡Œè¿™ä¸ªpythonè„šæœ¬."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ppo.py \\\n",
    "--env-id='LunarLander-v2' \\\n",
    "--repo-id='YOUR_REPO_ID' \\\n",
    "--total-timesteps=50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712bdd04",
   "metadata": {},
   "source": [
    "## é¢å¤–çš„æŒ‘æˆ˜(å¯é€‰) ğŸ†\n",
    "åœ¨[æ’è¡Œæ¦œ](https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard)ä¸­, ä½ å°†æ‰¾åˆ°ä½ çš„æ™ºèƒ½ä½“çš„ä½ç½®. ä½ æƒ³è¦è·å¾—ç¬¬ä¸€å—?\n",
    "\n",
    "ä»¥ä¸‹æ˜¯ä¸€äº›å®ç°è¿™ä¸ªç›®æ ‡çš„æƒ³æ³•:\n",
    "* è®­ç»ƒæ›´å¤šçš„æ—¶é—´æ­¥\n",
    "* é€šè¿‡æŸ¥çœ‹ä½ åŒå­¦æ‰€åšçš„æ¥å°è¯•ä¸åŒçš„è¶…å‚æ•°\n",
    "* **å‘å¸ƒä½ è®­ç»ƒçš„æ–°æ¨¡å‹**åˆ°Hubä¸Š ğŸ”¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c5124",
   "metadata": {},
   "source": [
    "ä¸‹ä¸ªå•å…ƒè§! ğŸ”¥\n",
    "## ä¸æ–­å­¦ä¹ , ä¸æ–­ç²¾å½© ğŸ¤— ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
