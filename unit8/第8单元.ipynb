{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd30585b",
   "metadata": {},
   "source": [
    "# 第8单元: 使用PyTorch编写Proximal Policy Optimization(PPO) 🤖️\n",
    "\n",
    "在本单元中, 你将学习**使用PyTorch从头开始编写你的PPO智能体.**\n",
    "\n",
    "为了测试鲁棒性, 我们将在2个不同的经典环境进行训练:\n",
    "\n",
    "* [CartPole-v1](https://www.gymlibrary.ml/environments/classic_control/cart_pole/?highlight=cartpole)\n",
    "* [LunarLander-v2 🚀](https://www.gymlibrary.ml/environments/box2d/lunar_lander/)\n",
    "\n",
    "我们通过深入了解PPO的工作原理来完成课程的基础部分. 在第1单元, 你学习了在LunarLander-v2上训练PPO智能体. 但是现在, 第8单元, 你可以从头开始编写代码. 这真是太不可思议了 🤩.\n",
    "\n",
    "![cover.jpg](./assets/cover.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c085c67c",
   "metadata": {},
   "source": [
    "⬇️ 这是你将在几分钟内实现的目标的示例([原始视频1下载链接](https://huggingface.co/sb3/ppo-CartPole-v1/resolve/main/replay.mp4), [原始视频2下载链接](https://huggingface.co/sb3/ppo-LunarLander-v2/resolve/main/replay.mp4)). ⬇️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d3e8b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<video autoplay controls><source src='./assets/replay1.mp4' type='video/mp4'></video>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d5948f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<video autoplay controls><source src='./assets/replay2.mp4' type='video/mp4'></video>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3924008",
   "metadata": {},
   "source": [
    "💡 我们建议你使用Google Colab, 因为某些环境只适用于Ubuntu. Google Colab的免费版本很适合这个教程. 让我们开始吧! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f70cfd",
   "metadata": {},
   "source": [
    "## 这份笔记来自深度强化学习课程\n",
    "![Deep Reinforcement Learning Course.jpg](./assets/DeepReinforcementLearningCourse.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078086dd",
   "metadata": {},
   "source": [
    "在这个免费课程中, 你将:\n",
    "\n",
    "* 📖 研究深度强化学习的**理论和实践.**\n",
    "* 🧑‍💻 学习**使用流行的深度强化学习库**, 例如Stable Baselines3, RL Baselines3 Zoo和RLlib.\n",
    "* 🤖️ **在独特的环境中训练智能体.**\n",
    "\n",
    "还有更多的课程 📚 内容 👉 https://github.com/huggingface/deep-rl-class\n",
    "\n",
    "保持进度的最佳方式是加入我们的Discord服务器与社区和我们进行交流. 👉🏻 https://discord.gg/aYka4Yhff9"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c04cee2",
   "metadata": {},
   "source": [
    "## 先决条件 🏗\n",
    "\n",
    "在深入研究笔记之前, 你需要:\n",
    "\n",
    "🔲 📚 [阅读第8单元的README.](https://github.com/huggingface/deep-rl-class/blob/main/unit8/README.md)\n",
    "\n",
    "🔲 📚 通过阅读章节**学习Proximal Policy Optimization(PPO)** 👉 https://huggingface.co/blog/deep-rl-ppo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65eb65e5",
   "metadata": {},
   "source": [
    "### 第0步: 设置GPU 💪\n",
    "\n",
    "* 为了**更快的训练智能体, 我们将使用GPU,** 选择`修改 > 笔记本设置`\n",
    "![image.png](./assets/image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8488e0",
   "metadata": {},
   "source": [
    "* `硬件加速器 > GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0664be3",
   "metadata": {},
   "source": [
    "![image.png](./assets/image1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc5d1cd",
   "metadata": {},
   "source": [
    "### 第1步: 安装依赖项 🔽 和 虚拟屏幕 💻"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a244064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt install ffmpeg\n",
    "# 如果你使用IDE(例如PyCharm或VS Code)将不需要这些步骤.\n",
    "!apt install python-opengl xvfb \n",
    "!pip install pyvirtualdisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece15dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym box2d-py  # 如果使用Apple M1 conda install box2d-py\n",
    "!pip install huggingface_hub\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e472eb4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建虚拟屏幕.\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce758ac0",
   "metadata": {},
   "source": [
    "### 第2步: 让我们使用Costa Huang的教程从头开始编写PPO\n",
    "* 对于PPO的核心实现, 我们将使用优秀的[Costa Huang的教程](https://costa.sh/).\n",
    "* 除此之外, 更深入的了解你可以阅读13个核心实现细节: https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/\n",
    "\n",
    "👉 视频教程: https://youtu.be/MEt6rrxH8W4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d085c2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/MEt6rrxH8W4\" ' \n",
    "     + 'title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; '\n",
    "     + 'clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ba4641",
   "metadata": {},
   "source": [
    "* 最好的办法是先在下面的单元格中编写代码, 这样如果你的进程被关闭, 也不会丢失代码."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0f7c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 你的代码:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b480c4ff",
   "metadata": {},
   "source": [
    "### 第3步: 添加Hugging Face集成 🤗\n",
    "* 为了将我们的模型发布到Hugging Face Hub, 我们需要定义一个`package_to_hub`函数.\n",
    "* 添加我们的需要将模型发布到Hugging Face Hub的依赖项."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e446ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import imageio\n",
    "import json\n",
    "import shutil\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "from huggingface_hub import HfApi, upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "from wasabi import Printer\n",
    "\n",
    "msg = Printer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a99f741",
   "metadata": {},
   "source": [
    "* 在函数`parse_args()`中添加新参数来定义我们想要发布模型的`repo-id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de38bc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 添加Hugging Face Hub参数.\n",
    "parser.add_argument('--repo-id',\n",
    "                    type=str,\n",
    "                    default='ThomasSimonini/ppo-CartPole-v1',\n",
    "                    help='Hugging Face Hub中模型仓库的ID{用户名/仓库名}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad8e7dc",
   "metadata": {},
   "source": [
    "* 接下来, 我们添加将模型发布到Hugging Face Hub所需的方法\n",
    "* 这些方法有:\n",
    "    * `_evalutate_agent()`: 评估智能体.\n",
    "    * `_generate_model_card()`: 为你的智能体生成模型卡.\n",
    "    * `_record_video()`: 录制智能体的回放视频."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b267850c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   hyperparameters,\n",
    "                   eval_env,\n",
    "                   video_fps=30,\n",
    "                   commit_message='发布强化学习模型到Hugging Face Hub.',\n",
    "                   token=None,\n",
    "                   logs=None):\n",
    "    \"\"\"评估, 生成视频并将模型发布到Hugging Face Hub.\n",
    "\n",
    "    此函数将执行完整的流水线:\n",
    "        * 评估模型\n",
    "        * 生成模型卡\n",
    "        * 生成智能体的回放视频\n",
    "        * 将全部内容发布到Hugging Face Hub\n",
    "\n",
    "    Args:\n",
    "        repo_id: Hugging Face Hub中模型仓库的ID\n",
    "        model: 训练的模型.\n",
    "        hyperparameters: 训练模型的超参数.\n",
    "        eval_env: 用于评估智能体的环境.\n",
    "        video_fps: 渲染回放视频的帧率.\n",
    "        commit_message: 提交的信息.\n",
    "        token: 发布模型的Hugging Face令牌.\n",
    "        logs: 你要上传的TensorBoard日志的本地目录.\n",
    "    \"\"\"\n",
    "    msg.info('这个函数将保存, 评估, 生成智能体回放视频, 创建模型卡并将模型发布到Hugging Face Hub.'\n",
    "             '最多可能需要1分钟. 这是一项正在进行的工作, 如果你遇到BUG, 请打开一个issue.')\n",
    "\n",
    "    # 第1步: 克隆或创建仓库.\n",
    "    repo_url = HfApi().create_repo(repo_id=repo_id,\n",
    "                                   token=token,\n",
    "                                   private=False,\n",
    "                                   exist_ok=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        tmp_dir = Path(tmp_dir)\n",
    "\n",
    "        # 第2步: 保存模型.\n",
    "        torch.save(model.state_dict(), tmp_dir / 'model.pt')\n",
    "\n",
    "        # 第3步: 评估模型并构建JSON.\n",
    "        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n",
    "\n",
    "        # 首先, 获取当前时间.\n",
    "        eval_datetime = datetime.datetime.now()\n",
    "        eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "        evaluate_data = {\n",
    "            'env_id': hyperparameters.env_id,\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'n_evaluation_episodes': 10,\n",
    "            'eval_datetime': eval_form_datetime\n",
    "        }\n",
    "\n",
    "        # 写入JSON文件.\n",
    "        with open(tmp_dir / 'hyperparameters.json', 'w') as outfile:\n",
    "            json.dump(evaluate_data, outfile)\n",
    "\n",
    "        # 第4步: 录制回放视频.\n",
    "        video_path = tmp_dir / 'replay.mp4'\n",
    "        _record_video(eval_env, model, video_path, video_fps)\n",
    "\n",
    "        # 第5步: 创建模型卡.\n",
    "        generated_model_card, metadata = _generate_model_card('PPO',\n",
    "                                                              hyperparameters.env_id,\n",
    "                                                              mean_reward,\n",
    "                                                              std_reward,\n",
    "                                                              hyperparameters)\n",
    "        _save_model_card(tmp_dir, generated_model_card, metadata)\n",
    "\n",
    "        # 第6步: 如果需要则添加日志.\n",
    "        if logs:\n",
    "            _add_logdir(tmp_dir, Path(logs))\n",
    "\n",
    "        msg.info(f'正在将仓库{repo_id}发布到Hugging Face Hub...')\n",
    "\n",
    "        repo_url = upload_folder(repo_id=repo_id,\n",
    "                                 folder_path=tmp_dir,\n",
    "                                 path_in_repo='',\n",
    "                                 commit_message=commit_message,\n",
    "                                 token=token)\n",
    "\n",
    "        msg.info(f'你的模型已经发布到Hugging Face Hub. 你可以点击链接查看的你的模型: {repo_url}')\n",
    "\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def _evaluate_agent(env, n_eval_episodes, policy):\n",
    "    \"\"\"用`n_eval_episodes`轮评估智能体, 并返回奖励的均值和标准差.\n",
    "\n",
    "    Args:\n",
    "        env: 评估环境.\n",
    "        n_eval_episodes: 测试的总轮数.\n",
    "        policy: 强化学习智能体.\n",
    "\n",
    "    Returns:\n",
    "        奖励的均值和标准差.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, done, info = env.step(action.numpy())\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "def _record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "\n",
    "    while not done:\n",
    "        state = torch.Tensor(state)\n",
    "        # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(action.numpy())  # 我们直接使用next_state = state来记录顺序(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "\n",
    "\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "    \"\"\"为Hugging Face Hub生成模型卡.\n",
    "\n",
    "    Args:\n",
    "        model_name: 模型的名称.\n",
    "        env_id: 环境的名称.\n",
    "        mean_reward: 奖罚的均值.\n",
    "        std_reward: 奖罚的标准差.\n",
    "        hyperparameters: 训练模型的超参数.\n",
    "    \"\"\"\n",
    "    # 第1步: 选择元数据.\n",
    "    metadata = _generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "    # 将超参数命名空间转换为字符串.\n",
    "    converted_dict = vars(hyperparameters)\n",
    "    converted_str = str(converted_dict)\n",
    "    converted_str = converted_str.split(', ')\n",
    "    converted_str = '\\n'.join(converted_str)\n",
    "\n",
    "    # 第2步: 生成模型卡.\n",
    "    model_card = f'''\n",
    "    # 使用PPO智能体来玩 {env_id}\n",
    "    \n",
    "    这是一个使用PPO训练有素的模型玩 {env_id}.\n",
    "    要学习编写你自己的PPO智能体并训练它, \n",
    "    请查阅深度强化学习课程第8单元: https://github.com/huggingface/deep-rl-class/tree/main/unit8\n",
    "    \n",
    "    # 超参数\n",
    "    ```python\n",
    "    {converted_str}\n",
    "    ```\n",
    "    '''\n",
    "\n",
    "    return model_card, metadata\n",
    "\n",
    "\n",
    "def _generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "    \"\"\"定义模型卡的元数据.\n",
    "\n",
    "    Args:\n",
    "        model_name: 模型的名称.\n",
    "        env_id: 环境的名称.\n",
    "        mean_reward: 奖罚的均值.\n",
    "        std_reward: 奖罚的标准差.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'tag': [\n",
    "            env_id,\n",
    "            'ppo',\n",
    "            'deep-reinforcement-learning',\n",
    "            'reinforcement-learning',\n",
    "            'custom-implementation',\n",
    "            'deep-rl-class'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 添加评估.\n",
    "    eval = metadata_eval_result(model_pretty_name=model_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_id,\n",
    "                                dataset_id=env_id)\n",
    "\n",
    "    # 合并所有的字典.\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"保存模型卡到仓库.\n",
    "\n",
    "    Args:\n",
    "        local_path: 仓库的地址.\n",
    "        generated_model_card: 通过`_generate_model_card()`生成的模型卡.\n",
    "        metadata: 元数据.\n",
    "    \"\"\"\n",
    "    readme_path = local_path / 'README.md'\n",
    "\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # 保存我们的评估信息到README的元数据.\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "\n",
    "def _add_logdir(local_path: Path,\n",
    "                logdir: Path):\n",
    "    \"\"\"添加日志到仓库.\n",
    "\n",
    "    Args:\n",
    "        local_path: 仓库的地址.\n",
    "        logdir: 日志的地址.\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # 添加日志到仓库下, 新地址叫`logs`\n",
    "        repo_logdir = local_path / 'logs'\n",
    "\n",
    "        # 如果当前的日志目录存在, 就删除.\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # 复制日志到仓库的日志中.\n",
    "        shutil.copytree(logdir, repo_logdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018dca8d",
   "metadata": {},
   "source": [
    "* 最后, 我们在PPO训练完后调用这个函数."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00af584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 创建一个评估环境.\n",
    "eval_env = gym.make(args.env_id)\n",
    "\n",
    "package_to_hub(repo_id=args.repo_id,\n",
    "               model=agent,  # 我们想要保存的模型.\n",
    "               hyperparameters=args,\n",
    "               eval_env=gym.make(args.env_id),\n",
    "               logs=f'runs/{runs_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca39b3d4",
   "metadata": {},
   "source": [
    "* 这是最终的`ppo.py`文件的样子."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1061cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"文档和实验结果可以在 https://docs.cleanrl.dev/rl-algorithms/ppo/#ppopy 找到.\"\"\"\n",
    "import argparse\n",
    "import datetime\n",
    "\n",
    "import json\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "from pathlib import Path\n",
    "\n",
    "import gym\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from huggingface_hub.hf_api import HfApi\n",
    "from huggingface_hub.hf_api import upload_folder\n",
    "from huggingface_hub.repocard import metadata_eval_result, metadata_save\n",
    "from torch import nn\n",
    "from torch.distributions import Categorical\n",
    "from torch.optim import Adam\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from wasabi import Printer\n",
    "\n",
    "msg = Printer()\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--exp-name', type=str, default=os.path.basename(__file__).rstrip('.py'), help='实验的名称')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='实验的随机种子')\n",
    "    parser.add_argument('--torch-deterministic',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='减少算法的随机性, 如果切换, `torch.backends.cudnn.deterministic=False`')\n",
    "    parser.add_argument('--cuda',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='默认情况下将启用CUDA')\n",
    "    parser.add_argument('--track',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=False,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='该实验将对权重和偏差进行追踪')\n",
    "    parser.add_argument('--wandb-project-name', type=str, default='cleanRL', help='wanDb项目的名称')\n",
    "    parser.add_argument('--wandb-entity', type=str, default=None, help='wanDb项目的实体')\n",
    "    parser.add_argument('--capture-video',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=False,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='是否保存智能体的回放视频(查看`videos`文件夹)')\n",
    "\n",
    "    # 算法参数.\n",
    "    parser.add_argument('--env-id', type=str, default='CartPole-v1', help='环境的名称')\n",
    "    parser.add_argument('--total-timesteps', type=int, default=50000, help='实验的总时间步')\n",
    "    parser.add_argument('--learning-rate', type=float, default=2.5e-4, help='优化器的学习率')\n",
    "    parser.add_argument('--num-envs', type=int, default=4, help='并行的环境数量')\n",
    "    parser.add_argument('--num-steps', type=int, default=128, help='每个环境中策略的每轮最大步数')\n",
    "    parser.add_argument('--anneal-lr',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='策略和价值网络的学习率退火')\n",
    "    parser.add_argument('--gae',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='使用广义优势估计器进行优势计算')\n",
    "    parser.add_argument('--gamma', type=float, default=0.99, help='折扣系数')\n",
    "    parser.add_argument('--gae-lambda', type=float, default=0.95, help='广义优势估计器的偏差与方差权衡因子')\n",
    "    parser.add_argument('--num-minibatches', type=int, default=4, help='批次的大小')\n",
    "    parser.add_argument('--update-epochs', type=int, default=4, help='更新策略的K个周期')\n",
    "    parser.add_argument('--norm-adv',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='使用广义优势')\n",
    "    parser.add_argument('--clip-coef', type=float, default=0.2, help='代理裁切系数')\n",
    "    parser.add_argument('--clip-vloss',\n",
    "                        type=lambda x: bool(strtobool(x)),\n",
    "                        default=True,\n",
    "                        nargs='?',\n",
    "                        const=True,\n",
    "                        help='根据论文, 是否对价值函数使用裁切损失')\n",
    "    parser.add_argument('--ent-coef', type=float, default=0.01, help='熵系数')\n",
    "    parser.add_argument('--vf-coef', type=float, default=0.5, help='价值系数')\n",
    "    parser.add_argument('--max-grad-norm', type=float, default=0.5, help='梯度裁切的最大范数')\n",
    "    parser.add_argument('--target-kl', type=float, default=None, help='目标KL散度阈值')\n",
    "\n",
    "    # 添加Hugging Face参数.\n",
    "    parser.add_argument('--repo-id',\n",
    "                        type=str,\n",
    "                        default='ThomasSimonini/ppo-CartPole-v1',\n",
    "                        help='Hugging Face Hub中模型仓库的ID{用户名/仓库名}')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    args.batch_size = int(args.num_envs * args.num_steps)\n",
    "    args.minibatch_size = int(args.batch_size // args.num_minibatches)\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "def package_to_hub(repo_id,\n",
    "                   model,\n",
    "                   hyperparameters,\n",
    "                   eval_env,\n",
    "                   video_fps=30,\n",
    "                   commit_message='发布强化学习模型到Hugging Face Hub.',\n",
    "                   token=None,\n",
    "                   logs=None):\n",
    "    \"\"\"评估, 生成视频并将模型发布到Hugging Face Hub.\n",
    "\n",
    "    此函数将执行完整的流水线:\n",
    "        * 评估模型\n",
    "        * 生成模型卡\n",
    "        * 生成智能体的回放视频\n",
    "        * 将全部内容发布到Hugging Face Hub\n",
    "\n",
    "    Args:\n",
    "        repo_id: Hugging Face Hub中模型仓库的ID\n",
    "        model: 训练的模型.\n",
    "        hyperparameters: 训练模型的超参数.\n",
    "        eval_env: 用于评估智能体的环境.\n",
    "        video_fps: 渲染回放视频的帧率.\n",
    "        commit_message: 提交的信息.\n",
    "        token: 发布模型的Hugging Face令牌.\n",
    "        logs: 你要上传的TensorBoard日志的本地目录.\n",
    "    \"\"\"\n",
    "    msg.info('这个函数将保存, 评估, 生成智能体回放视频, 创建模型卡并将模型发布到Hugging Face Hub.'\n",
    "             '最多可能需要1分钟. 这是一项正在进行的工作, 如果你遇到BUG, 请打开一个issue.')\n",
    "\n",
    "    # 第1步: 克隆或创建仓库.\n",
    "    repo_url = HfApi().create_repo(repo_id=repo_id,\n",
    "                                   token=token,\n",
    "                                   private=False,\n",
    "                                   exist_ok=True)\n",
    "\n",
    "    with tempfile.TemporaryDirectory() as tmp_dir:\n",
    "        tmp_dir = Path(tmp_dir)\n",
    "\n",
    "        # 第2步: 保存模型.\n",
    "        torch.save(model.state_dict(), tmp_dir / 'model.pt')\n",
    "\n",
    "        # 第3步: 评估模型并构建JSON.\n",
    "        mean_reward, std_reward = _evaluate_agent(eval_env, 10, model)\n",
    "\n",
    "        # 首先, 获取当前时间.\n",
    "        eval_datetime = datetime.datetime.now()\n",
    "        eval_form_datetime = eval_datetime.isoformat()\n",
    "\n",
    "        evaluate_data = {\n",
    "            'env_id': hyperparameters.env_id,\n",
    "            'mean_reward': mean_reward,\n",
    "            'std_reward': std_reward,\n",
    "            'n_evaluation_episodes': 10,\n",
    "            'eval_datetime': eval_form_datetime\n",
    "        }\n",
    "\n",
    "        # 写入JSON文件.\n",
    "        with open(tmp_dir / 'hyperparameters.json', 'w') as outfile:\n",
    "            json.dump(evaluate_data, outfile)\n",
    "\n",
    "        # 第4步: 录制回放视频.\n",
    "        video_path = tmp_dir / 'replay.mp4'\n",
    "        _record_video(eval_env, model, video_path, video_fps)\n",
    "\n",
    "        # 第5步: 创建模型卡.\n",
    "        generated_model_card, metadata = _generate_model_card('PPO',\n",
    "                                                              hyperparameters.env_id,\n",
    "                                                              mean_reward,\n",
    "                                                              std_reward,\n",
    "                                                              hyperparameters)\n",
    "        _save_model_card(tmp_dir, generated_model_card, metadata)\n",
    "\n",
    "        # 第6步: 如果需要则添加日志.\n",
    "        if logs:\n",
    "            _add_logdir(tmp_dir, Path(logs))\n",
    "\n",
    "        msg.info(f'正在将仓库{repo_id}发布到Hugging Face Hub...')\n",
    "\n",
    "        repo_url = upload_folder(repo_id=repo_id,\n",
    "                                 folder_path=tmp_dir,\n",
    "                                 path_in_repo='',\n",
    "                                 commit_message=commit_message,\n",
    "                                 token=token)\n",
    "\n",
    "        msg.info(f'你的模型已经发布到Hugging Face Hub. 你可以点击链接查看的你的模型: {repo_url}')\n",
    "\n",
    "    return repo_url\n",
    "\n",
    "\n",
    "def _evaluate_agent(env, n_eval_episodes, policy):\n",
    "    \"\"\"用`n_eval_episodes`轮评估智能体, 并返回奖励的均值和标准差.\n",
    "\n",
    "    Args:\n",
    "        env: 评估环境.\n",
    "        n_eval_episodes: 测试的总轮数.\n",
    "        policy: 强化学习智能体.\n",
    "\n",
    "    Returns:\n",
    "        奖励的均值和标准差.\n",
    "    \"\"\"\n",
    "    episode_rewards = []\n",
    "\n",
    "    for episode in range(n_eval_episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_rewards_ep = 0\n",
    "\n",
    "        while done is False:\n",
    "            state = torch.Tensor(state)\n",
    "            action, _, _, _ = policy.get_action_and_value(state)\n",
    "            new_state, reward, done, info = env.step(action.numpy())\n",
    "            total_rewards_ep += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            state = new_state\n",
    "        episode_rewards.append(total_rewards_ep)\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "\n",
    "    return mean_reward, std_reward\n",
    "\n",
    "\n",
    "def _record_video(env, policy, out_directory, fps=30):\n",
    "    images = []\n",
    "    done = False\n",
    "    state = env.reset()\n",
    "    img = env.render(mode='rgb_array')\n",
    "    images.append(img)\n",
    "\n",
    "    while not done:\n",
    "        state = torch.Tensor(state)\n",
    "        # 在给定状态下, 采取具有最大期望奖励的动作(索引).\n",
    "        action, _, _, _ = policy.get_action_and_value(state)\n",
    "        state, reward, done, info = env.step(action.numpy())  # 我们直接使用next_state = state来记录顺序(recording logic).\n",
    "        img = env.render(mode='rgb_array')\n",
    "        images.append(img)\n",
    "\n",
    "    imageio.mimsave(out_directory, [np.array(img) for i, img in enumerate(images)], fps=fps)\n",
    "\n",
    "\n",
    "def _generate_model_card(model_name, env_id, mean_reward, std_reward, hyperparameters):\n",
    "    \"\"\"为Hugging Face Hub生成模型卡.\n",
    "\n",
    "    Args:\n",
    "        model_name: 模型的名称.\n",
    "        env_id: 环境的名称.\n",
    "        mean_reward: 奖罚的均值.\n",
    "        std_reward: 奖罚的标准差.\n",
    "        hyperparameters: 训练模型的超参数.\n",
    "    \"\"\"\n",
    "    # 第1步: 选择元数据.\n",
    "    metadata = _generate_metadata(model_name, env_id, mean_reward, std_reward)\n",
    "\n",
    "    # 将超参数命名空间转换为字符串.\n",
    "    converted_dict = vars(hyperparameters)\n",
    "    converted_str = str(converted_dict)\n",
    "    converted_str = converted_str.split(', ')\n",
    "    converted_str = '\\n'.join(converted_str)\n",
    "\n",
    "    # 第2步: 生成模型卡.\n",
    "    model_card = f'''\n",
    "    # 使用PPO智能体来玩 {env_id}\n",
    "    \n",
    "    这是一个使用PPO训练有素的模型玩 {env_id}.\n",
    "    要学习编写你自己的PPO智能体并训练它, \n",
    "    请查阅深度强化学习课程第8单元: https://github.com/huggingface/deep-rl-class/tree/main/unit8\n",
    "    \n",
    "    # 超参数\n",
    "    ```python\n",
    "    {converted_str}\n",
    "    ```\n",
    "    '''\n",
    "\n",
    "    return model_card, metadata\n",
    "\n",
    "\n",
    "def _generate_metadata(model_name, env_id, mean_reward, std_reward):\n",
    "    \"\"\"定义模型卡的元数据.\n",
    "\n",
    "    Args:\n",
    "        model_name: 模型的名称.\n",
    "        env_id: 环境的名称.\n",
    "        mean_reward: 奖罚的均值.\n",
    "        std_reward: 奖罚的标准差.\n",
    "    \"\"\"\n",
    "    metadata = {\n",
    "        'tag': [\n",
    "            env_id,\n",
    "            'ppo',\n",
    "            'deep-reinforcement-learning',\n",
    "            'reinforcement-learning',\n",
    "            'custom-implementation',\n",
    "            'deep-rl-class'\n",
    "        ]\n",
    "    }\n",
    "\n",
    "    # 添加评估.\n",
    "    eval = metadata_eval_result(model_pretty_name=model_name,\n",
    "                                task_pretty_name='reinforcement-learning',\n",
    "                                task_id='reinforcement-learning',\n",
    "                                metrics_pretty_name='mean_reward',\n",
    "                                metrics_id='mean_reward',\n",
    "                                metrics_value=f'{mean_reward:.2f} +/- {std_reward:.2f}',\n",
    "                                dataset_pretty_name=env_id,\n",
    "                                dataset_id=env_id)\n",
    "\n",
    "    # 合并所有的字典.\n",
    "    metadata = {**metadata, **eval}\n",
    "\n",
    "    return metadata\n",
    "\n",
    "\n",
    "def _save_model_card(local_path, generated_model_card, metadata):\n",
    "    \"\"\"保存模型卡到仓库.\n",
    "\n",
    "    Args:\n",
    "        local_path: 仓库的地址.\n",
    "        generated_model_card: 通过`_generate_model_card()`生成的模型卡.\n",
    "        metadata: 元数据.\n",
    "    \"\"\"\n",
    "    readme_path = local_path / 'README.md'\n",
    "\n",
    "    if readme_path.exists():\n",
    "        with readme_path.open('r', encoding='utf8') as f:\n",
    "            readme = f.read()\n",
    "    else:\n",
    "        readme = generated_model_card\n",
    "\n",
    "    with readme_path.open('w', encoding='utf-8') as f:\n",
    "        f.write(readme)\n",
    "\n",
    "    # 保存我们的评估信息到README的元数据.\n",
    "    metadata_save(readme_path, metadata)\n",
    "\n",
    "\n",
    "def _add_logdir(local_path: Path,\n",
    "                logdir: Path):\n",
    "    \"\"\"添加日志到仓库.\n",
    "\n",
    "    Args:\n",
    "        local_path: 仓库的地址.\n",
    "        logdir: 日志的地址.\n",
    "    \"\"\"\n",
    "    if logdir.exists() and logdir.is_dir():\n",
    "        # 添加日志到仓库下, 新地址叫`logs`\n",
    "        repo_logdir = local_path / 'logs'\n",
    "\n",
    "        # 如果当前的日志目录存在, 就删除.\n",
    "        if repo_logdir.exists():\n",
    "            shutil.rmtree(repo_logdir)\n",
    "\n",
    "        # 复制日志到仓库的日志中.\n",
    "        shutil.copytree(logdir, repo_logdir)\n",
    "\n",
    "\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f'videos/{run_name}')\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "\n",
    "        return env\n",
    "\n",
    "    return thunk\n",
    "\n",
    "\n",
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    torch.nn.init.orthogonal_(layer.weight, std)\n",
    "    torch.nn.init.constant_(layer.bias, bias_const)\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    def __init__(self, envs):\n",
    "        super(Agent, self).__init__()\n",
    "        self.critic = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.asarray(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 1), std=1.0)\n",
    "        )\n",
    "\n",
    "        self.actor = nn.Sequential(\n",
    "            layer_init(nn.Linear(np.asarray(envs.single_observation_space.shape).prod(), 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, 64)),\n",
    "            nn.Tanh(),\n",
    "            layer_init(nn.Linear(64, envs.single_action_space.n), std=0.01)\n",
    "        )\n",
    "\n",
    "    def get_value(self, x):\n",
    "        return self.critic(x)\n",
    "\n",
    "    def get_action_and_value(self, x, action=None):\n",
    "        logits = self.actor(x)\n",
    "        probs = Categorical(logits=logits)\n",
    "\n",
    "        if action is None:\n",
    "            action = probs.sample()\n",
    "\n",
    "        return action, probs.log_prob(action), probs.entropy(), self.critic(x)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    args = parse_args()\n",
    "    run_name = f'{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}'\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(config=vars(args),\n",
    "                   project=args.wandb_project_name,\n",
    "                   entity=args.wandb_entity,\n",
    "                   name=run_name,\n",
    "                   sync_tensorboard=True,\n",
    "                   monitor_gym=True,\n",
    "                   save_code=True)\n",
    "    writer = SummaryWriter(f'runs/{run_name}')\n",
    "    writer.add_text('hyperparameters',\n",
    "                    '|params|value|\\n|-|-|\\n%s' % ('\\n'.join([f'|{key}|{value}' for key, value in vars(args).items()])))\n",
    "\n",
    "    # 尽量不要修改: 随机种子\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() and args.cuda else 'cpu')\n",
    "\n",
    "    # 环境设置.\n",
    "    envs = gym.vector.SyncVectorEnv([make_env(env_id=args.env_id,\n",
    "                                              seed=args.seed + i,\n",
    "                                              idx=i,\n",
    "                                              capture_video=args.capture_video,\n",
    "                                              run_name=run_name) for i in range(args.num_envs)])\n",
    "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), '仅支持离散动作空间'\n",
    "\n",
    "    agent = Agent(envs).to(device)\n",
    "    optimizer = Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n",
    "\n",
    "    # 算法逻辑: 存储设置.\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "\n",
    "    # 请不要修改: 开始游戏.\n",
    "    global_step = 0\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.Tensor(args.num_envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "\n",
    "    for update in range(1, num_updates + 1):\n",
    "        # 是否对学习速率进行退火.\n",
    "        if args.anneal_lr:\n",
    "            frac = 1.0 - (update - 1.0) / num_updates\n",
    "            lrnow = frac * args.learning_rate\n",
    "            optimizer.param_groups[0]['lr'] = lrnow\n",
    "\n",
    "        for step in range(0, args.num_steps):\n",
    "            global_step += 1 * args.num_envs\n",
    "            obs[step] = next_obs\n",
    "            dones[step] = next_done\n",
    "\n",
    "            # 算法逻辑: 动作逻辑.\n",
    "            with torch.no_grad():\n",
    "                action, logprob, _, value = agent.get_action_and_value(next_obs)\n",
    "                values[step] = value.flatten()\n",
    "\n",
    "            actions[step] = action\n",
    "            logprobs[step] = logprob\n",
    "\n",
    "            # 请不要修改: 执行游戏并保存日志.\n",
    "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            rewards[step] = torch.tensor(reward).to(device).view(-1)\n",
    "            next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(done).to(device)\n",
    "\n",
    "            for item in info:\n",
    "                if 'episode' in item.keys():\n",
    "                    print(f\"总时间步={global_step}, 当前返回={item['episode']['r']}\")\n",
    "                    writer.add_scalar('charts/episodic_return', item['episode']['r'], global_step)\n",
    "                    writer.add_scalar('charts/episodic_return', item['episode']['r'], global_step)\n",
    "                    break\n",
    "\n",
    "        # 如果当前轮没有结束, 则引导值.\n",
    "        with torch.no_grad():\n",
    "            next_value = agent.get_value(next_obs).reshape(1, -1)\n",
    "            if args.gae:\n",
    "                advantages = torch.zeros_like(rewards).to(device)\n",
    "                lastgaelam = 0\n",
    "                for t in reversed(range(args.num_steps)):\n",
    "                    if t == args.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        nextvalues = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        nextvalues = values[t + 1]\n",
    "                    delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n",
    "                    advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n",
    "                returns = advantages + values\n",
    "            else:\n",
    "                returns = torch.zeros_like(rewards).to(device)\n",
    "                for t in reversed(range(args.num_steps)):\n",
    "                    if t == args.num_steps - 1:\n",
    "                        nextnonterminal = 1.0 - next_done\n",
    "                        next_return = next_value\n",
    "                    else:\n",
    "                        nextnonterminal = 1.0 - dones[t + 1]\n",
    "                        next_return = values[t + 1]\n",
    "                    returns[t] = rewards[t] + args.gamma * nextnonterminal * next_return\n",
    "                advantages = returns - values\n",
    "\n",
    "        # 展平批次.\n",
    "        b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n",
    "        b_logprobs = logprobs.reshape(-1)\n",
    "        b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n",
    "        b_advantages = advantages.reshape(-1)\n",
    "        b_returns = returns.reshape(-1)\n",
    "        b_values = values.reshape(-1)\n",
    "\n",
    "        # 优化策略和价值网络.\n",
    "        b_inds = np.arange(args.batch_size)\n",
    "        clipfracs = []\n",
    "        for epoch in range(args.update_epochs):\n",
    "            np.random.shuffle(b_inds)\n",
    "            for start in range(0, args.batch_size, args.minibatch_size):\n",
    "                end = start + args.minibatch_size\n",
    "                mb_inds = b_inds[start:end]\n",
    "\n",
    "                _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n",
    "                logratio = newlogprob - b_logprobs[mb_inds]\n",
    "                ratio = logratio.exp()\n",
    "\n",
    "                with torch.no_grad():\n",
    "                    # 计算近似KL散度 http://joschu.net/blog/kl-approx.html\n",
    "                    old_approx_kl = (-logratio).mean()\n",
    "                    approx_kl = ((ratio - 1) - logratio).mean()\n",
    "                    clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "\n",
    "                mb_advantages = b_advantages[mb_inds]\n",
    "                if args.norm_adv:\n",
    "                    mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n",
    "\n",
    "                # 策略损失.\n",
    "                pg_loss1 = -mb_advantages * ratio\n",
    "                pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n",
    "                pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n",
    "\n",
    "                # 价值损失.\n",
    "                newvalue = newvalue.view(-1)\n",
    "                if args.clip_vloss:\n",
    "                    v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n",
    "                    v_clipped = b_values[mb_inds] + torch.clamp(\n",
    "                        newvalue - b_values[mb_inds],\n",
    "                        -args.clip_coef,\n",
    "                        args.clip_coef\n",
    "                    )\n",
    "                    v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n",
    "                    v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n",
    "                    v_loss = 0.5 * v_loss_max.mean()\n",
    "                else:\n",
    "                    v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n",
    "\n",
    "                entropy_loss = entropy.mean()\n",
    "                loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "\n",
    "            if args.target_kl is not None:\n",
    "                if approx_kl > args.target_kl:\n",
    "                    break\n",
    "\n",
    "        y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "\n",
    "        # 请不要修改: 为绘图记录奖励.\n",
    "        writer.add_scalar('charts/learning_rate', optimizer.param_groups[0]['lr'], global_step)\n",
    "        writer.add_scalar('losses/value_loss', v_loss.item(), global_step)\n",
    "        writer.add_scalar('losses/policy_loss', pg_loss.item(), global_step)\n",
    "        writer.add_scalar('losses/entropy', entropy_loss.item(), global_step)\n",
    "        writer.add_scalar('losses/old_approx_kl', old_approx_kl.item(), global_step)\n",
    "        writer.add_scalar('losses/approx_kl', approx_kl.item(), global_step)\n",
    "        writer.add_scalar('losses/clipfrac', np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar('losses/explained_variance', explained_var, global_step)\n",
    "        print('SPS:', int(global_step / (time.time() - start_time)))\n",
    "        writer.add_scalar('charts/SPS', int(global_step / (time.time() - start_time)), global_step)\n",
    "\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "    # 创建评估环境.\n",
    "    eval_env = gym.make(args.env_id)\n",
    "\n",
    "    package_to_hub(repo_id=args.repo_id,\n",
    "                   model=agent,  # 我们想保存的模型.\n",
    "                   hyperparameters=args,\n",
    "                   eval_env=eval_env,\n",
    "                   logs=f'runs/{run_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340523fa",
   "metadata": {},
   "source": [
    "为了能分享你的模型到社区, 有以下两个步骤需要做:\n",
    "\n",
    "1⃣️ (如果没有完成)创建一个Hugging Face账户 ➡ https://huggingface.co/join\n",
    "\n",
    "2⃣️ 登陆账户, 然后你需要保存一个Hugging Face的身份验证令牌(token).\n",
    "\n",
    "* 创建一个新的具有**写入规则**的令牌(https://huggingface.co/settings/tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4862bde5",
   "metadata": {},
   "source": [
    "![image.png](./assets/image2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991d844c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3243415d",
   "metadata": {},
   "source": [
    "如果你使用IDE, 也可在终端中使用以下命令:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b9a9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli login"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b7e6c8f",
   "metadata": {},
   "source": [
    "### 第4步: 让我们开始训练 🔥\n",
    "* 现在你已经完成从头开始编写PPO算法并添加到Hugging Face Hub, 我们可以开始训练了 🔥."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4823abf1",
   "metadata": {},
   "source": [
    "* 首先, 你需要复制你的代码到你创建的名为`ppo.py`的文件中."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "793ef29c",
   "metadata": {},
   "source": [
    "![image.png](./assets/image3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50244092",
   "metadata": {},
   "source": [
    "![image.png](./assets/image4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb71827",
   "metadata": {},
   "source": [
    "* 现在我们只需要使用`python <python脚本的名称>.py`和我们用`argparse`定义的参数来运行这个python脚本."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fbd352",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ppo.py \\\n",
    "--env-id='LunarLander-v2' \\\n",
    "--repo-id='YOUR_REPO_ID' \\\n",
    "--total-timesteps=50000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712bdd04",
   "metadata": {},
   "source": [
    "## 额外的挑战(可选) 🏆\n",
    "在[排行榜](https://huggingface.co/spaces/chrisjay/Deep-Reinforcement-Learning-Leaderboard)中, 你将找到你的智能体的位置. 你想要获得第一吗?\n",
    "\n",
    "以下是一些实现这个目标的想法:\n",
    "* 训练更多的时间步\n",
    "* 通过查看你同学所做的来尝试不同的超参数\n",
    "* **发布你训练的新模型**到Hub上 🔥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025c5124",
   "metadata": {},
   "source": [
    "下个单元见! 🔥\n",
    "## 不断学习, 不断精彩 🤗 ! "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
